{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script aims to develop a prompt chain structure to send large amounts of text/content to LLM APIs including through multiple calls\n",
    "\n",
    "#### Note: still in experimentation mode/progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import base64\n",
    "import json\n",
    "from typing import List, Dict, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import os\n",
    "#import google.generativeai as gemini\n",
    "#from openai import OpenAI\n",
    "import io, threading, time, re\n",
    "import pandas as pd\n",
    "from json_repair import repair_json\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling in files of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'full-ig/site'\n",
    "destination_folder = 'full-ig/json_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = []\n",
    "for file_name in os.listdir(source_folder):\n",
    "    # Check if the file ends with .html but not with compound extensions\n",
    "    if file_name.endswith('.json'):\n",
    "                                    # and not (file_name.endswith('.ttl.html') or \n",
    "                                            #  file_name.endswith('.json.html') or \n",
    "                                            #  file_name.endswith('.xml.html') or \n",
    "                                            #  file_name.endswith('.change.history.html')):\n",
    "        json_files.append(file_name)\n",
    "        # Move the file to the destination folder\n",
    "        shutil.copy(os.path.join(source_folder, file_name), destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_files_by_base_name(directory_path, delimiter='-'):\n",
    "    \"\"\"\n",
    "    Group files in the directory by their base name (portion before a delimiter).\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    delimiter (str): The delimiter to split the file name on (default is '-').\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are base names and values are lists of files that share the same base name.\n",
    "    \"\"\"\n",
    "    grouped_files = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):  # Only process .txt files\n",
    "            if delimiter in filename:  # Only consider files with the delimiter\n",
    "                # Get the base name (before the first delimiter)\n",
    "                base_name = filename.split(delimiter)[0]\n",
    "                \n",
    "                # Append the file to the group corresponding to its base name\n",
    "                grouped_files[base_name].append(filename)\n",
    "    \n",
    "    return grouped_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_files = group_files_by_base_name(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_name, files in grouped_files.items():\n",
    "    print(f\"Base name: {base_name} (Total files: {len(files)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_to_folders(directory_path, grouped_files):\n",
    "    \"\"\"\n",
    "    Copy files to folders if the base name group has more than 1 file, and remove them from the original directory.\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    grouped_files (dict): Dictionary of grouped files by base name.\n",
    "    \"\"\"\n",
    "    for base_name, files in grouped_files.items():\n",
    "        if len(files) >= 1:  # Only process groups with more than 1 file\n",
    "            # Create a folder for the base name in the same directory\n",
    "            base_folder = os.path.join(directory_path, base_name)\n",
    "            if not os.path.exists(base_folder):\n",
    "                os.makedirs(base_folder)  # Create the folder if it doesn't exist\n",
    "            print(f\"Created folder: {base_folder}\")\n",
    "            \n",
    "            # Copy each file in the group to the new folder\n",
    "            for file in files:\n",
    "                source_file = os.path.join(directory_path, file)\n",
    "                destination_file = os.path.join(base_folder, file)\n",
    "                shutil.copy(source_file, destination_file)  # Copy the file\n",
    "                # print(f\"Copied {file} to {base_folder}\")\n",
    "                \n",
    "                # Remove the file from the original directory\n",
    "                # os.remove(source_file)\n",
    "                # print(f\"Removed {file} from original directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_files_to_folders(directory_path, grouped_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidating JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_json_files(folder_path):\n",
    "    \"\"\"\n",
    "    Combines all JSON files in a folder into a single array of JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of JSON objects from all files\n",
    "    \"\"\"\n",
    "    combined_json = []\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    json_content = json.load(file)\n",
    "                    combined_json.append(json_content)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                \n",
    "    return combined_json\n",
    "\n",
    "def create_consolidated_jsons(base_directory='package/json_only'):\n",
    "    \"\"\"\n",
    "    Creates consolidated JSON files for each subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        base_directory (str): Base directory containing the categorized folders\n",
    "    \"\"\"\n",
    "    # Get all subdirectories\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "              if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    # Process each subdirectory\n",
    "    for subdir in subdirs:\n",
    "        folder_path = os.path.join(base_directory, subdir)\n",
    "        print(f\"Processing {subdir}...\")\n",
    "        \n",
    "        # Combine all JSON files in this folder\n",
    "        combined_data = combine_json_files(folder_path)\n",
    "        \n",
    "        if combined_data:\n",
    "            # Create output filename\n",
    "            output_filename = f\"{subdir}_combined.json\"\n",
    "            output_path = os.path.join(base_directory, output_filename)\n",
    "            \n",
    "            # Write the combined JSON to a file\n",
    "            try:\n",
    "                with open(output_path, 'w') as outfile:\n",
    "                    json.dump({\n",
    "                        \"resourceType\": subdir,\n",
    "                        \"total\": len(combined_data),\n",
    "                        \"entry\": combined_data\n",
    "                    }, outfile, indent=2)\n",
    "                print(f\"Created {output_filename} with {len(combined_data)} entries\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing {output_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the consolidated JSON files\n",
    "create_consolidated_jsons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating mixed processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Claude Processor Module\n",
    "# ----------------------\n",
    "# This module provides functionality to process large JSON documents and images through the Claude API\n",
    "# by breaking them into manageable chunks while maintaining context and structure.\n",
    "\n",
    "# Key Components:\n",
    "# 1. ChunkMetadata - Tracks information about JSON chunks\n",
    "# 2. ClaudeProcessor - Main class handling API interaction and data processing\n",
    "# 3. Helper functions for progress tracking and response management\n",
    "# \"\"\"\n",
    "\n",
    "# @dataclass\n",
    "# class ChunkMetadata:\n",
    "#     \"\"\"\n",
    "#     Metadata container for tracking JSON chunk information.\n",
    "    \n",
    "#     Attributes:\n",
    "#         chunk_number (int): Sequential number of this chunk\n",
    "#         total_chunks (int): Total number of chunks in the complete dataset\n",
    "#         resource_type (str): FHIR resource type or 'Collection' for multiple types\n",
    "#         chunk_size (int): Size of the chunk in bytes\n",
    "#     \"\"\"\n",
    "#     chunk_number: int\n",
    "#     total_chunks: int\n",
    "#     resource_type: str\n",
    "#     chunk_size: int\n",
    "\n",
    "\n",
    "\n",
    "# def heartbeat(stop_event: threading.Event, start_time: float) -> None:\n",
    "#     \"\"\"\n",
    "#     Provides visual feedback about processing progress.\n",
    "    \n",
    "#     Args:\n",
    "#         stop_event (threading.Event): Event to signal when processing is complete\n",
    "#         start_time (float): Timestamp when processing started\n",
    "    \n",
    "#     Prints elapsed time every 5 seconds until the stop event is set.\n",
    "#     \"\"\"\n",
    "#     while not stop_event.is_set():\n",
    "#         elapsed = time.time() - start_time\n",
    "#         print(f\"... still processing ({elapsed:.1f}s elapsed)\")\n",
    "#         time.sleep(5)\n",
    "\n",
    "# class ClaudeProcessor:\n",
    "#     \"\"\"\n",
    "#     Main class for processing large JSON documents and images through the Claude API.\n",
    "    \n",
    "#     Handles:\n",
    "#     - Breaking large JSONs into manageable chunks\n",
    "#     - Processing images alongside JSON data\n",
    "#     - Managing API interactions\n",
    "#     - Combining responses into coherent analysis\n",
    "    \n",
    "#     Attributes:\n",
    "#         client (Anthropic): Authenticated Claude API client\n",
    "#         model_name (str): Claude model to use for processing\n",
    "#         max_chunk_size (int): Maximum size in bytes for each chunk\n",
    "#         max_tokens (int): Maximum tokens in Claude's response\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         api_key: str,\n",
    "#         model_name: str = \"claude-3-5-sonnet-20240620\",\n",
    "#         max_chunk_size: int = 100000,\n",
    "#         max_tokens: int = 4096\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initialize the Claude Processor.\n",
    "        \n",
    "#         Args:\n",
    "#             api_key (str): Claude API authentication key\n",
    "#             model_name (str): Claude model version to use\n",
    "#             max_chunk_size (int): Maximum bytes per chunk\n",
    "#             max_tokens (int): Maximum tokens in response\n",
    "#         \"\"\"\n",
    "#         self.client = Anthropic(api_key=api_key)\n",
    "#         self.model_name = model_name\n",
    "#         self.max_chunk_size = max_chunk_size\n",
    "#         self.max_tokens = max_tokens\n",
    "        \n",
    "#     def _split_json(self, json_data: Union[Dict, List]) -> List[Dict]:\n",
    "#         \"\"\"\n",
    "#         Splits large JSON objects into processable chunks.\n",
    "        \n",
    "#         Args:\n",
    "#             json_data (Union[Dict, List]): Input JSON data\n",
    "            \n",
    "#         Returns:\n",
    "#             List[Dict]: List of chunks with metadata\n",
    "            \n",
    "#         Handles both single resources and collections while preserving structure.\n",
    "#         Each chunk includes metadata for context preservation.\n",
    "#         \"\"\"\n",
    "#         if isinstance(json_data, dict):\n",
    "#             # Handle single resource\n",
    "#             resource_type = json_data.get('resourceType', 'Unknown')\n",
    "#             entries = json_data.get('entry', [json_data])\n",
    "#         else:\n",
    "#             # Handle array of resources\n",
    "#             resource_type = 'Collection'\n",
    "#             entries = json_data\n",
    "            \n",
    "#         chunks = []\n",
    "#         current_chunk = []\n",
    "#         current_size = 0\n",
    "        \n",
    "#         for entry in entries:\n",
    "#             entry_size = len(json.dumps(entry))\n",
    "            \n",
    "#             if current_size + entry_size > self.max_chunk_size and current_chunk:\n",
    "#                 chunks.append(current_chunk)\n",
    "#                 current_chunk = []\n",
    "#                 current_size = 0\n",
    "                \n",
    "#             current_chunk.append(entry)\n",
    "#             current_size += entry_size\n",
    "            \n",
    "#         if current_chunk:\n",
    "#             chunks.append(current_chunk)\n",
    "            \n",
    "#         return [{\n",
    "#             'resourceType': resource_type,\n",
    "#             'total': len(chunk),\n",
    "#             'entry': chunk,\n",
    "#             'metadata': ChunkMetadata(\n",
    "#                 chunk_number=i+1,\n",
    "#                 total_chunks=len(chunks),\n",
    "#                 resource_type=resource_type,\n",
    "#                 chunk_size=len(json.dumps(chunk))\n",
    "#             )\n",
    "#         } for i, chunk in enumerate(chunks)]\n",
    "\n",
    "#     def _prepare_message(\n",
    "#         self,\n",
    "#         chunk: Dict,\n",
    "#         prompt: str,\n",
    "#         image_paths: Optional[List[str]] = None\n",
    "#     ) -> List[Dict]:\n",
    "#         \"\"\"\n",
    "#         Prepares messages for Claude API including images and JSON content.\n",
    "        \n",
    "#         Args:\n",
    "#             chunk (Dict): JSON chunk to process\n",
    "#             prompt (str): Analysis instructions for Claude\n",
    "#             image_paths (Optional[List[str]]): Paths to images to include\n",
    "            \n",
    "#         Returns:\n",
    "#             List[Dict]: Formatted message for Claude API\n",
    "            \n",
    "#         Handles both text and image content, maintaining chunk context.\n",
    "#         \"\"\"\n",
    "#         content = []\n",
    "        \n",
    "#         # Add any images first\n",
    "#         if image_paths:\n",
    "#             for img_path in image_paths:\n",
    "#                 with open(img_path, \"rb\") as img_file:\n",
    "#                     base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "#                     content.append({\n",
    "#                         \"type\": \"image\",\n",
    "#                         \"source\": {\n",
    "#                             \"type\": \"base64\",\n",
    "#                             \"media_type\": f\"image/{os.path.splitext(img_path)[1][1:]}\",\n",
    "#                             \"data\": base64_image\n",
    "#                         }\n",
    "#                     })\n",
    "\n",
    "#         # Add the JSON chunk and prompt\n",
    "#         metadata = chunk.pop('metadata')\n",
    "#         chunk_context = f\"\"\"This is chunk {metadata.chunk_number} of {metadata.total_chunks} \n",
    "#         from a {metadata.resource_type} resource.\\n\\n\"\"\"\n",
    "        \n",
    "#         content.append({\n",
    "#             \"type\": \"text\",\n",
    "#             \"text\": f\"{chunk_context}JSON Content:\\n{json.dumps(chunk, indent=2)}\\n\\n{prompt}\"\n",
    "#         })\n",
    "\n",
    "#         return [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "#     def process_json(\n",
    "#         self,\n",
    "#         json_data: Union[Dict, List],\n",
    "#         prompt: str,\n",
    "#         image_paths: Optional[List[str]] = None\n",
    "#     ) -> List[str]:\n",
    "#         \"\"\"\n",
    "#         Main method for processing JSON data with optional images.\n",
    "        \n",
    "#         Args:\n",
    "#             json_data (Union[Dict, List]): JSON content to analyze\n",
    "#             prompt (str): Instructions for Claude's analysis\n",
    "#             image_paths (Optional[List[str]]): Images to include\n",
    "            \n",
    "#         Returns:\n",
    "#             List[str]: List of Claude's responses for each chunk\n",
    "            \n",
    "#         Orchestrates the entire processing workflow including:\n",
    "#         - Chunking large JSONs\n",
    "#         - Adding images\n",
    "#         - Managing API calls\n",
    "#         - Progress tracking\n",
    "#         \"\"\"\n",
    "#         chunks = self._split_json(json_data)\n",
    "#         responses = []\n",
    "        \n",
    "#         for chunk in chunks:\n",
    "#             start_time = time.time()\n",
    "#             stop_event = threading.Event()\n",
    "#             heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "#             heartbeat_thread.start()\n",
    "\n",
    "#             try:\n",
    "#                 messages = self._prepare_message(chunk, prompt, image_paths)\n",
    "#                 response = self.client.messages.create(\n",
    "#                     model=self.model_name,\n",
    "#                     max_tokens=self.max_tokens,\n",
    "#                     messages=messages\n",
    "#                 )\n",
    "#                 responses.append(response.content[0].text)\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing chunk {chunk['metadata'].chunk_number}: {str(e)}\")\n",
    "#                 raise\n",
    "#             finally:\n",
    "#                 stop_event.set()\n",
    "#                 heartbeat_thread.join()\n",
    "\n",
    "#         return responses\n",
    "\n",
    "#     def combine_responses(self, responses: List[str]) -> str:\n",
    "#         \"\"\"\n",
    "#         Combines multiple chunk responses into a unified analysis.\n",
    "        \n",
    "#         Args:\n",
    "#             responses (List[str]): Individual chunk responses\n",
    "            \n",
    "#         Returns:\n",
    "#             str: Combined analysis with clear section separation\n",
    "            \n",
    "#         Creates a coherent final document while preserving chunk boundaries\n",
    "#         for reference and context.\n",
    "#         \"\"\"\n",
    "#         combined = \"=== Combined Analysis ===\\n\\n\"\n",
    "        \n",
    "#         for i, response in enumerate(responses, 1):\n",
    "#             combined += f\"=== Chunk {i} Analysis ===\\n{response}\\n\\n\"\n",
    "            \n",
    "#         return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a module designed to handle the processing of large JSON documents and images through the Claude AI API. It solves the common problem of processing JSON files that exceed Claude's input size limits by chunking the data while maintaining context and structure.\n",
    "    Processes large JSON files by automatically splitting them into manageable chunks\n",
    "    Handles both single resources and collections of resources\n",
    "    Integrates image processing capabilities\n",
    "    Maintains context across chunked data\n",
    "    Provides progress tracking during processing\n",
    "    Combines multiple responses into coherent analysis\n",
    "\n",
    "Key Methods\n",
    "\n",
    "1) JSON Splitting (_split_json):\n",
    "- Breaks large JSON files into processable chunks\n",
    "- Preserves resource structure and relationships\n",
    "- Adds metadata for context preservation\n",
    "\n",
    "\n",
    "2) Message Preparation (_prepare_message):\n",
    "- Formats data for Claude API\n",
    "- Handles both JSON and image content\n",
    "- Maintains chunk context\n",
    "- Manages base64 encoding for images\n",
    "\n",
    "\n",
    "3) JSON Processing (process_json):\n",
    "- Main processing workflow\n",
    "- Manages chunking and API calls\n",
    "- Handles image integration\n",
    "- Provides progress tracking\n",
    "- Implements error handling\n",
    "\n",
    "\n",
    "4) Response Combination (combine_responses):\n",
    "- Merges chunk responses\n",
    "- Creates coherent final analysis\n",
    "- Maintains section separation\n",
    "- Preserves context markers\n",
    "\n",
    "The heartbeat function provides real-time feedback during processing:\n",
    "- Shows elapsed time\n",
    "- Indicates active processing\n",
    "- Helps monitor long-running operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "from pathlib import Path\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "@dataclass\n",
    "class BatchMetadata:\n",
    "    \"\"\"\n",
    "    Enhanced metadata container for tracking mixed content batches.\n",
    "    \"\"\"\n",
    "    batch_number: int\n",
    "    total_batches: int\n",
    "    content_types: List[str]\n",
    "    batch_size: int\n",
    "    source_files: List[str]\n",
    "\n",
    "class ContentBatch:\n",
    "    \"\"\"\n",
    "    Container for managing mixed content types within a batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size: int = 100000):\n",
    "        self.json_content: List[Dict] = []\n",
    "        self.markdown_content: List[str] = []\n",
    "        self.image_paths: List[str] = []\n",
    "        self.current_size: int = 0\n",
    "        self.max_size: int = max_size\n",
    "        \n",
    "    def can_add(self, content_size: int) -> bool:\n",
    "        return self.current_size + content_size <= self.max_size\n",
    "    \n",
    "    def add_content(self, content_type: str, content: Union[Dict, str, Path]):\n",
    "        if content_type == 'json':\n",
    "            self.json_content.append(content)\n",
    "            self.current_size += len(json.dumps(content))\n",
    "        elif content_type == 'markdown':\n",
    "            self.markdown_content.append(content)\n",
    "            self.current_size += len(content.encode('utf-8'))\n",
    "        elif content_type == 'image':\n",
    "            self.image_paths.append(str(content))\n",
    "            # Approximate image contribution to context\n",
    "            self.current_size += 1000  # Conservative estimate\n",
    "\n",
    "class EnhancedClaudeProcessor(ClaudeProcessor):\n",
    "    \"\"\"\n",
    "    Enhanced processor supporting mixed content types and intelligent batching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.markdown_converter = markdown.Markdown()\n",
    "        \n",
    "    def _process_markdown(self, markdown_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert markdown to plain text while preserving structure.\n",
    "        \"\"\"\n",
    "        html = self.markdown_converter.convert(markdown_text)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup.get_text(separator='\\n\\n')\n",
    "    \n",
    "    def _create_batches(\n",
    "        self,\n",
    "        json_data: List[Dict],\n",
    "        markdown_files: List[Path],\n",
    "        image_paths: List[Path]\n",
    "    ) -> List[Tuple[ContentBatch, BatchMetadata]]:\n",
    "        \"\"\"\n",
    "        Create optimized batches of mixed content.\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        current_batch = ContentBatch(self.max_chunk_size)\n",
    "        \n",
    "        # Process markdown files first to establish context\n",
    "        for md_file in markdown_files:\n",
    "            with open(md_file, 'r', encoding='utf-8') as f:\n",
    "                content = self._process_markdown(f.read())\n",
    "                if not current_batch.can_add(len(content.encode('utf-8'))):\n",
    "                    batches.append(current_batch)\n",
    "                    current_batch = ContentBatch(self.max_chunk_size)\n",
    "                current_batch.add_content('markdown', content)\n",
    "        \n",
    "        # Add JSON content\n",
    "        for item in json_data:\n",
    "            content_size = len(json.dumps(item))\n",
    "            if not current_batch.can_add(content_size):\n",
    "                batches.append(current_batch)\n",
    "                current_batch = ContentBatch(self.max_chunk_size)\n",
    "            current_batch.add_content('json', item)\n",
    "        \n",
    "        # Add images strategically across batches\n",
    "        images_per_batch = max(1, len(image_paths) // (len(batches) + 1))\n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            batch_idx = min(i // images_per_batch, len(batches))\n",
    "            if batch_idx == len(batches):\n",
    "                current_batch.add_content('image', img_path)\n",
    "            else:\n",
    "                batches[batch_idx].add_content('image', img_path)\n",
    "        \n",
    "        if current_batch.json_content or current_batch.markdown_content or current_batch.image_paths:\n",
    "            batches.append(current_batch)\n",
    "            \n",
    "        # Create metadata for each batch\n",
    "        return [(batch, BatchMetadata(\n",
    "            batch_number=i+1,\n",
    "            total_batches=len(batches),\n",
    "            content_types=self._get_content_types(batch),\n",
    "            batch_size=batch.current_size,\n",
    "            source_files=self._get_source_files(batch)\n",
    "        )) for i, batch in enumerate(batches)]\n",
    "    \n",
    "    def _prepare_mixed_message(\n",
    "        self,\n",
    "        batch: ContentBatch,\n",
    "        metadata: BatchMetadata,\n",
    "        prompt: str\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prepare a message containing mixed content types.\n",
    "        \"\"\"\n",
    "        content = []\n",
    "        \n",
    "        # Add context about the batch\n",
    "        batch_context = f\"\"\"Processing batch {metadata.batch_number} of {metadata.total_batches}\n",
    "        Content types present: {', '.join(metadata.content_types)}\n",
    "        Source files: {', '.join(metadata.source_files)}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add images first\n",
    "        for img_path in batch.image_paths:\n",
    "            with open(img_path, \"rb\") as img_file:\n",
    "                base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "                content.append({\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": f\"image/{Path(img_path).suffix[1:]}\",\n",
    "                        \"data\": base64_image\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # Combine markdown and JSON content\n",
    "        text_content = batch_context + \"\\n\\n\"\n",
    "        \n",
    "        if batch.markdown_content:\n",
    "            text_content += \"=== Markdown Content ===\\n\\n\"\n",
    "            text_content += \"\\n\\n\".join(batch.markdown_content)\n",
    "            \n",
    "        if batch.json_content:\n",
    "            text_content += \"\\n\\n=== JSON Content ===\\n\\n\"\n",
    "            text_content += json.dumps(batch.json_content, indent=2)\n",
    "            \n",
    "        text_content += f\"\\n\\n{prompt}\"\n",
    "        \n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": text_content\n",
    "        })\n",
    "        \n",
    "        return [{\"role\": \"user\", \"content\": content}]\n",
    "    \n",
    "    def process_mixed_content(\n",
    "        self,\n",
    "        json_data: List[Dict],\n",
    "        markdown_files: List[Path],\n",
    "        image_paths: List[Path],\n",
    "        prompt: str\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Process mixed content types in optimized batches.\n",
    "        \"\"\"\n",
    "        batches = self._create_batches(json_data, markdown_files, image_paths)\n",
    "        responses = []\n",
    "        \n",
    "        for batch, metadata in batches:\n",
    "            start_time = time.time()\n",
    "            stop_event = threading.Event()\n",
    "            heartbeat_thread = threading.Thread(\n",
    "                target=heartbeat, \n",
    "                args=(stop_event, start_time)\n",
    "            )\n",
    "            heartbeat_thread.start()\n",
    "            \n",
    "            try:\n",
    "                messages = self._prepare_mixed_message(batch, metadata, prompt)\n",
    "                response = self.client.messages.create(\n",
    "                    model=self.model_name,\n",
    "                    max_tokens=self.max_tokens,\n",
    "                    messages=messages\n",
    "                )\n",
    "                responses.append(response.content[0].text)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {metadata.batch_number}: {str(e)}\")\n",
    "                raise\n",
    "            finally:\n",
    "                stop_event.set()\n",
    "                heartbeat_thread.join()\n",
    "        \n",
    "        return self.combine_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the processor\n",
    "# processor = ClaudeProcessor(\n",
    "#     api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "#     model_name=\"claude-3-5-sonnet-20240620\"\n",
    "# )\n",
    "\n",
    "# # Load your JSON data\n",
    "# with open('path_to_your_json.json', 'r') as f:\n",
    "#     json_data = json.load(f)\n",
    "\n",
    "# # List of image paths to include\n",
    "# image_paths = ['path_to_image1.jpg', 'path_to_image2.png']\n",
    "\n",
    "# # Define your prompt\n",
    "# prompt = \"\"\"Please analyze this portion of the Implementation Guide and provide:\n",
    "# 1. Key information and requirements\n",
    "# 2. Notable patterns or constraints\n",
    "# 3. How this section relates to the overall IG\n",
    "\n",
    "# Focus on new information not covered in previous chunks.\"\"\"\n",
    "\n",
    "# # Process the JSON and get responses\n",
    "# responses = processor.process_json(json_data, prompt, image_paths)\n",
    "\n",
    "# # Combine all responses into a final analysis\n",
    "# final_analysis = processor.combine_responses(responses)\n",
    "# print(final_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the enhanced processor\n",
    "processor = EnhancedClaudeProcessor(\n",
    "    api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "    model_name=\"claude-3-5-sonnet-20240620\"\n",
    ")\n",
    "\n",
    "# Prepare your content paths\n",
    "json_folder = Path('full-ig/json_only')\n",
    "markdown_folder = Path('full-ig/markdown')\n",
    "image_folder = Path('full-ig/images')\n",
    "\n",
    "# Load JSON data\n",
    "json_files = list(json_folder.glob('*_combined.json'))\n",
    "json_data = []\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as f:\n",
    "        json_data.extend(json.load(f)['entry'])\n",
    "\n",
    "# Get markdown and image files\n",
    "markdown_files = list(markdown_folder.glob('*.md'))\n",
    "image_paths = list(image_folder.glob('*.{jpg,png,jpeg}'))\n",
    "\n",
    "# Define your analysis prompt\n",
    "prompt = \"\"\"Please analyze this content batch and provide:\n",
    "1. Key information and requirements from both the markdown documentation and JSON structures\n",
    "2. Relationships between the documentation and implementation details\n",
    "3. Notable patterns or constraints\n",
    "4. How this section relates to the overall Implementation Guide\n",
    "\n",
    "Focus on new information not covered in previous batches.\"\"\"\n",
    "\n",
    "# Process all content\n",
    "final_analysis = processor.process_mixed_content(\n",
    "    json_data=json_data,\n",
    "    markdown_files=markdown_files,\n",
    "    image_paths=image_paths,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Save the analysis\n",
    "with open('analysis_output.md', 'w') as f:\n",
    "    f.write(final_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
