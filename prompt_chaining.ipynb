{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script aims to develop a prompt chain structure to send large amounts of text/content to LLM APIs including through multiple calls\n",
    "\n",
    "#### Note: still in experimentation mode/progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import base64\n",
    "import json\n",
    "from typing import List, Dict, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a module designed to handle the processing of large JSON documents and images through the Claude AI API. It solves the common problem of processing JSON files that exceed Claude's input size limits by chunking the data while maintaining context and structure.\n",
    "    Processes large JSON files by automatically splitting them into manageable chunks\n",
    "    Handles both single resources and collections of resources\n",
    "    Integrates image processing capabilities\n",
    "    Maintains context across chunked data\n",
    "    Provides progress tracking during processing\n",
    "    Combines multiple responses into coherent analysis\n",
    "\n",
    "Key Methods\n",
    "\n",
    "1) JSON Splitting (_split_json):\n",
    "- Breaks large JSON files into processable chunks\n",
    "- Preserves resource structure and relationships\n",
    "- Adds metadata for context preservation\n",
    "\n",
    "\n",
    "2) Message Preparation (_prepare_message):\n",
    "- Formats data for Claude API\n",
    "- Handles both JSON and image content\n",
    "- Maintains chunk context\n",
    "- Manages base64 encoding for images\n",
    "\n",
    "\n",
    "3) JSON Processing (process_json):\n",
    "- Main processing workflow\n",
    "- Manages chunking and API calls\n",
    "- Handles image integration\n",
    "- Provides progress tracking\n",
    "- Implements error handling\n",
    "\n",
    "\n",
    "4) Response Combination (combine_responses):\n",
    "- Merges chunk responses\n",
    "- Creates coherent final analysis\n",
    "- Maintains section separation\n",
    "- Preserves context markers\n",
    "\n",
    "The heartbeat function provides real-time feedback during processing:\n",
    "- Shows elapsed time\n",
    "- Indicates active processing\n",
    "- Helps monitor long-running operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Claude Processor Module\n",
    "----------------------\n",
    "This module provides functionality to process large JSON documents and images through the Claude API\n",
    "by breaking them into manageable chunks while maintaining context and structure.\n",
    "\n",
    "Key Components:\n",
    "1. ChunkMetadata - Tracks information about JSON chunks\n",
    "2. ClaudeProcessor - Main class handling API interaction and data processing\n",
    "3. Helper functions for progress tracking and response management\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"\n",
    "    Metadata container for tracking JSON chunk information.\n",
    "    \n",
    "    Attributes:\n",
    "        chunk_number (int): Sequential number of this chunk\n",
    "        total_chunks (int): Total number of chunks in the complete dataset\n",
    "        resource_type (str): FHIR resource type or 'Collection' for multiple types\n",
    "        chunk_size (int): Size of the chunk in bytes\n",
    "    \"\"\"\n",
    "    chunk_number: int\n",
    "    total_chunks: int\n",
    "    resource_type: str\n",
    "    chunk_size: int\n",
    "\n",
    "\n",
    "\n",
    "def heartbeat(stop_event: threading.Event, start_time: float) -> None:\n",
    "    \"\"\"\n",
    "    Provides visual feedback about processing progress.\n",
    "    \n",
    "    Args:\n",
    "        stop_event (threading.Event): Event to signal when processing is complete\n",
    "        start_time (float): Timestamp when processing started\n",
    "    \n",
    "    Prints elapsed time every 5 seconds until the stop event is set.\n",
    "    \"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"... still processing ({elapsed:.1f}s elapsed)\")\n",
    "        time.sleep(5)\n",
    "\n",
    "class ClaudeProcessor:\n",
    "    \"\"\"\n",
    "    Main class for processing large JSON documents and images through the Claude API.\n",
    "    \n",
    "    Handles:\n",
    "    - Breaking large JSONs into manageable chunks\n",
    "    - Processing images alongside JSON data\n",
    "    - Managing API interactions\n",
    "    - Combining responses into coherent analysis\n",
    "    \n",
    "    Attributes:\n",
    "        client (Anthropic): Authenticated Claude API client\n",
    "        model_name (str): Claude model to use for processing\n",
    "        max_chunk_size (int): Maximum size in bytes for each chunk\n",
    "        max_tokens (int): Maximum tokens in Claude's response\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str,\n",
    "        model_name: str = \"claude-3-5-sonnet-20240620\",\n",
    "        max_chunk_size: int = 100000,\n",
    "        max_tokens: int = 4096\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Claude Processor.\n",
    "        \n",
    "        Args:\n",
    "            api_key (str): Claude API authentication key\n",
    "            model_name (str): Claude model version to use\n",
    "            max_chunk_size (int): Maximum bytes per chunk\n",
    "            max_tokens (int): Maximum tokens in response\n",
    "        \"\"\"\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    def _split_json(self, json_data: Union[Dict, List]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Splits large JSON objects into processable chunks.\n",
    "        \n",
    "        Args:\n",
    "            json_data (Union[Dict, List]): Input JSON data\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of chunks with metadata\n",
    "            \n",
    "        Handles both single resources and collections while preserving structure.\n",
    "        Each chunk includes metadata for context preservation.\n",
    "        \"\"\"\n",
    "        if isinstance(json_data, dict):\n",
    "            # Handle single resource\n",
    "            resource_type = json_data.get('resourceType', 'Unknown')\n",
    "            entries = json_data.get('entry', [json_data])\n",
    "        else:\n",
    "            # Handle array of resources\n",
    "            resource_type = 'Collection'\n",
    "            entries = json_data\n",
    "            \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for entry in entries:\n",
    "            entry_size = len(json.dumps(entry))\n",
    "            \n",
    "            if current_size + entry_size > self.max_chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "                \n",
    "            current_chunk.append(entry)\n",
    "            current_size += entry_size\n",
    "            \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            \n",
    "        return [{\n",
    "            'resourceType': resource_type,\n",
    "            'total': len(chunk),\n",
    "            'entry': chunk,\n",
    "            'metadata': ChunkMetadata(\n",
    "                chunk_number=i+1,\n",
    "                total_chunks=len(chunks),\n",
    "                resource_type=resource_type,\n",
    "                chunk_size=len(json.dumps(chunk))\n",
    "            )\n",
    "        } for i, chunk in enumerate(chunks)]\n",
    "\n",
    "    def _prepare_message(\n",
    "        self,\n",
    "        chunk: Dict,\n",
    "        prompt: str,\n",
    "        image_paths: Optional[List[str]] = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prepares messages for Claude API including images and JSON content.\n",
    "        \n",
    "        Args:\n",
    "            chunk (Dict): JSON chunk to process\n",
    "            prompt (str): Analysis instructions for Claude\n",
    "            image_paths (Optional[List[str]]): Paths to images to include\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Formatted message for Claude API\n",
    "            \n",
    "        Handles both text and image content, maintaining chunk context.\n",
    "        \"\"\"\n",
    "        content = []\n",
    "        \n",
    "        # Add any images first\n",
    "        if image_paths:\n",
    "            for img_path in image_paths:\n",
    "                with open(img_path, \"rb\") as img_file:\n",
    "                    base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "                    content.append({\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": f\"image/{os.path.splitext(img_path)[1][1:]}\",\n",
    "                            \"data\": base64_image\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "        # Add the JSON chunk and prompt\n",
    "        metadata = chunk.pop('metadata')\n",
    "        chunk_context = f\"\"\"This is chunk {metadata.chunk_number} of {metadata.total_chunks} \n",
    "        from a {metadata.resource_type} resource.\\n\\n\"\"\"\n",
    "        \n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{chunk_context}JSON Content:\\n{json.dumps(chunk, indent=2)}\\n\\n{prompt}\"\n",
    "        })\n",
    "\n",
    "        return [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    def process_json(\n",
    "        self,\n",
    "        json_data: Union[Dict, List],\n",
    "        prompt: str,\n",
    "        image_paths: Optional[List[str]] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Main method for processing JSON data with optional images.\n",
    "        \n",
    "        Args:\n",
    "            json_data (Union[Dict, List]): JSON content to analyze\n",
    "            prompt (str): Instructions for Claude's analysis\n",
    "            image_paths (Optional[List[str]]): Images to include\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of Claude's responses for each chunk\n",
    "            \n",
    "        Orchestrates the entire processing workflow including:\n",
    "        - Chunking large JSONs\n",
    "        - Adding images\n",
    "        - Managing API calls\n",
    "        - Error handling\n",
    "        - Progress tracking\n",
    "        \"\"\"\n",
    "        chunks = self._split_json(json_data)\n",
    "        responses = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            start_time = time.time()\n",
    "            stop_event = threading.Event()\n",
    "            heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "            heartbeat_thread.start()\n",
    "\n",
    "            try:\n",
    "                messages = self._prepare_message(chunk, prompt, image_paths)\n",
    "                response = self.client.messages.create(\n",
    "                    model=self.model_name,\n",
    "                    max_tokens=self.max_tokens,\n",
    "                    messages=messages\n",
    "                )\n",
    "                responses.append(response.content[0].text)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {chunk['metadata'].chunk_number}: {str(e)}\")\n",
    "                raise\n",
    "            finally:\n",
    "                stop_event.set()\n",
    "                heartbeat_thread.join()\n",
    "\n",
    "        return responses\n",
    "\n",
    "    def combine_responses(self, responses: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Combines multiple chunk responses into a unified analysis.\n",
    "        \n",
    "        Args:\n",
    "            responses (List[str]): Individual chunk responses\n",
    "            \n",
    "        Returns:\n",
    "            str: Combined analysis with clear section separation\n",
    "            \n",
    "        Creates a coherent final document while preserving chunk boundaries\n",
    "        for reference and context.\n",
    "        \"\"\"\n",
    "        combined = \"=== Combined Analysis ===\\n\\n\"\n",
    "        \n",
    "        for i, response in enumerate(responses, 1):\n",
    "            combined += f\"=== Chunk {i} Analysis ===\\n{response}\\n\\n\"\n",
    "            \n",
    "        return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "processor = ClaudeProcessor(\n",
    "    api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "    model_name=\"claude-3-5-sonnet-20240620\"\n",
    ")\n",
    "\n",
    "# Load your JSON data\n",
    "with open('path_to_your_json.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# List of image paths to include\n",
    "image_paths = ['path_to_image1.jpg', 'path_to_image2.png']\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"\"\"Please analyze this portion of the Implementation Guide and provide:\n",
    "1. Key information and requirements\n",
    "2. Notable patterns or constraints\n",
    "3. How this section relates to the overall IG\n",
    "\n",
    "Focus on new information not covered in previous chunks.\"\"\"\n",
    "\n",
    "# Process the JSON and get responses\n",
    "responses = processor.process_json(json_data, prompt, image_paths)\n",
    "\n",
    "# Combine all responses into a final analysis\n",
    "final_analysis = processor.combine_responses(responses)\n",
    "print(final_analysis)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
