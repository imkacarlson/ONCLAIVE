{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ceadams/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import httpx\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "from openai import OpenAI\n",
    "import google.generativeai as gemini\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Basic setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "CERT_PATH = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "OUTPUT_DIR = 'processed_outputs'\n",
    "\n",
    "QUESTIONS = [\n",
    "    \"What is the purpose of the FHIR DaVinci PlanNet Implementation Guide?\",\n",
    "    \"Who are the intended users and actors of the FHIR DaVinci PlanNet Implementation Guide?\",\n",
    "    \"Are there one or more workflows defined in the FHIR DaVinci PDex Plan Net Implementation Guide? Please use all the information you know.\",\n",
    "    \"What data is being exchanged in the FHIR DaVinci PDex Plan Net Implementation Guide and why?\",\n",
    "    \"How is that data represented by the resources and profiles in the FHIR DaVinci PDex Plan Net Implementation Guide?\",\n",
    "    \"What actions (REST/CRUD) or operations can be used in the FHIR DaVinci PDex Plan Net Implementation Guide?\",\n",
    "    \"What are all the mandatory requirements and rules from the DaVinci PDex Plan Net Implementation Guide for compliant implementations?\",\n",
    "    \"What are all the optional requirements and rules from the DaVinci PDex Plan Net Implementation Guide for compliant implementations?\",\n",
    "    \"How would you create a test plan for the FHIR DaVinci PDex Plan Net Implementation Guide?\"\n",
    "]\n",
    "\n",
    "# Configuration for different LLMs\n",
    "CLAUDE_CONFIG = {\n",
    "    \"model_name\": \"claude-3-5-sonnet-20240620\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"temperature\": 0.7,\n",
    "    \"system_prompt\": \"\"\"You are a seasoned Healthcare Integration Test Engineer analyzing a FHIR Implementation Guide.\n",
    "                    Provide detailed, technically precise answers focused on implementation details.\"\"\"\n",
    "}\n",
    "\n",
    "GPT_CONFIG = {\n",
    "    \"model_name\": \"gpt-4\",\n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.7,\n",
    "    \"system_prompt\": \"\"\"You are a seasoned Healthcare Integration Test Engineer analyzing a FHIR Implementation Guide.\n",
    "                    Provide detailed, technically precise answers focused on implementation details.\"\"\"\n",
    "}\n",
    "\n",
    "GEMINI_CONFIG = {\n",
    "    \"model_name\": \"models/gemini-1.5-pro-001\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Rate limiting configurations\n",
    "RATE_LIMITS = {\n",
    "    \"claude\": {\n",
    "        \"requests_per_minute\": 25,\n",
    "        \"max_requests_per_day\": 5000,\n",
    "        \"delay_between_requests\": 2\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"requests_per_minute\": 200,\n",
    "        \"max_requests_per_day\": 10000,\n",
    "        \"delay_between_requests\": 0.5\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"requests_per_minute\": 60,\n",
    "        \"max_requests_per_day\": 60000,\n",
    "        \"delay_between_requests\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in RATE_LIMITS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    limits = RATE_LIMITS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= limits['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= limits['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:]\n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < limits['delay_between_requests']:\n",
    "        time.sleep(limits['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1\n",
    "\n",
    "def heartbeat(stop_event, start_time):\n",
    "    \"\"\"Print elapsed time periodically until stopped\"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"... still processing ({elapsed:.1f}s elapsed)\")\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        verify_path = CERT_PATH if os.path.exists(CERT_PATH) else True\n",
    "        http_client = httpx.Client(verify=verify_path, timeout=60.0)\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=GEMINI_CONFIG[\"model_name\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": GEMINI_CONFIG[\"max_tokens\"],\n",
    "                \"temperature\": GEMINI_CONFIG[\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_client = OpenAI(\n",
    "            api_key=os.getenv('OPENAI_API_KEY'),\n",
    "            timeout=60.0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gemini\": gemini_client,\n",
    "            \"gpt\": openai_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def ask_llm(client: Any, question: str, llm_type: str, rate_limiter: dict) -> str:\n",
    "    \"\"\"Ask question to specific LLM with rate limiting and retries\"\"\"\n",
    "    stop_event = threading.Event()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    heartbeat_thread = threading.Thread(\n",
    "        target=heartbeat,\n",
    "        args=(stop_event, start_time)\n",
    "    )\n",
    "    heartbeat_thread.start()\n",
    "    \n",
    "    try:\n",
    "        check_rate_limits(rate_limiter, llm_type)\n",
    "        \n",
    "        if llm_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=CLAUDE_CONFIG[\"model_name\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                system=CLAUDE_CONFIG[\"system_prompt\"],\n",
    "                max_tokens=CLAUDE_CONFIG[\"max_tokens\"]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif llm_type == \"gemini\":\n",
    "            response = client.generate_content(question)\n",
    "            return response.text\n",
    "            \n",
    "        elif llm_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=GPT_CONFIG[\"model_name\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": GPT_CONFIG[\"system_prompt\"]},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                max_tokens=GPT_CONFIG[\"max_tokens\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {llm_type} request: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        stop_event.set()\n",
    "        heartbeat_thread.join()\n",
    "\n",
    "def save_results(results: Dict[str, Dict[str, str]], llm_type: str):\n",
    "    \"\"\"Save analysis results with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{llm_type}_plannet_analysis_{timestamp}.json\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": timestamp,\n",
    "                \"llm_type\": llm_type,\n",
    "                \"generation_date\": datetime.now().isoformat()\n",
    "            },\n",
    "            \"results\": results\n",
    "        }, f, indent=2)\n",
    "\n",
    "def analyze_with_llm(llm_type: str, client: Any, rate_limiter: dict) -> Dict[str, str]:\n",
    "    \"\"\"Run analysis for all questions using specified LLM\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for question in QUESTIONS:\n",
    "        print(f\"\\nAnalyzing question with {llm_type}: {question}\")\n",
    "        response = ask_llm(client, question, llm_type, rate_limiter)\n",
    "        results[question] = response\n",
    "        time.sleep(2)  # Small delay between questions\n",
    "        \n",
    "    save_results(results, llm_type)\n",
    "    return results\n",
    "\n",
    "def run_analysis():\n",
    "    \"\"\"Run analysis using all LLMs\"\"\"\n",
    "    try:\n",
    "        # Setup\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        clients = setup_clients()\n",
    "        rate_limiter = create_rate_limiter()\n",
    "        \n",
    "        # Process with each LLM\n",
    "        results = {}\n",
    "        for llm_type, client in clients.items():\n",
    "            print(f\"\\nProcessing with {llm_type}...\")\n",
    "            results[llm_type] = analyze_with_llm(llm_type, client, rate_limiter)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in analysis: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with claude...\n",
      "\n",
      "Analyzing question with claude: What is the purpose of the FHIR DaVinci PlanNet Implementation Guide?\n",
      "... still processing (0.0s elapsed)\n",
      "... still processing (5.0s elapsed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing question with claude: Who are the intended users and actors of the FHIR DaVinci PlanNet Implementation Guide?\n",
      "... still processing (0.0s elapsed)\n"
     ]
    }
   ],
   "source": [
    "results = run_analysis()\n",
    "print(f\"\\nAnalysis complete! Results saved in: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
