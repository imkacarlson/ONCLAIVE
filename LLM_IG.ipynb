{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring FHIR Implementation Guides (IGs) + LLMs\n",
    "\n",
    "In this notebook, we aim to explore how much LLMs understand about FHIR Implementation Guides (IGs) and investigate ways to upload IG content for deeper analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have langchain-connunity and beautifulsoup4 installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ceadams/Documents/onclaive/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as gemini\n",
    "from anthropic import Anthropic\n",
    "from openai import OpenAI\n",
    "import io, threading, time, re, json\n",
    "import pandas as pd\n",
    "from json_repair import repair_json\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import shutil\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in US Core IG HTML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Be sure that you have downloaded the US Core IG HTML files and placed them in your current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'full-ig/site'\n",
    "destination_folder = 'full-ig/html_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store only .html files\n",
    "html_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(source_folder):\n",
    "    # Check if the file ends with .html but not with compound extensions\n",
    "    if file_name.endswith('.html') and not (file_name.endswith('.ttl.html') or \n",
    "                                             file_name.endswith('.json.html') or \n",
    "                                             file_name.endswith('.xml.html') or \n",
    "                                             file_name.endswith('.change.history.html')):\n",
    "        html_files.append(file_name)\n",
    "        # Move the file to the destination folder\n",
    "        shutil.copy(os.path.join(source_folder, file_name), destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading HTML with BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_only_folder = 'full-ig/html_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new folder named \"plain_text\" inside the current directory\n",
    "processed_files_path = os.path.join(html_only_folder, 'plain_txt')\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(processed_files_path):\n",
    "    os.makedirs(processed_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the files processed\n",
    "processed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the files in the HTML folder\n",
    "for file_name in os.listdir(html_only_folder):\n",
    "    # Full path to the .html file\n",
    "    html_file_path = os.path.join(html_only_folder, file_name)\n",
    "    \n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(html_file_path):\n",
    "        # Use BSHTMLLoader to load the HTML content\n",
    "        loader = BSHTMLLoader(html_file_path, bs_kwargs={'features': 'html.parser'})\n",
    "        data = loader.load()\n",
    "        # Extract the plain text from the loaded data\n",
    "        plain_text = '\\n'.join([doc.page_content for doc in data])\n",
    "        \n",
    "        # Create the output file path with .txt extension\n",
    "        txt_file_name = file_name.replace('.html', '.txt')\n",
    "        txt_file_path = os.path.join(processed_files_path, txt_file_name)\n",
    "        \n",
    "        # Write the extracted plain text to the new .txt file\n",
    "        with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(plain_text)\n",
    "        \n",
    "        # Append to processed files list\n",
    "        processed_files.append(txt_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in API keys for Claude, Gemini, and GPT from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "claude_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "OpenAI.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: need to better clean combined text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to combine all text files\n",
    "\n",
    "def combine_txt_files(directory_path):\n",
    "    \"\"\"Combines all .txt files in the specified directory into a single string.\"\"\"\n",
    "    combined_text = []\n",
    "    #iterate through txt files in directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    #append text from txt file to combined_text\n",
    "                    combined_text.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    return \"\\n\".join(combined_text)\n",
    "\n",
    "# Directory where txt files are located\n",
    "txt_directory = 'full-ig/html_only/plain_txt'\n",
    "# create combined text object\n",
    "combined_content = combine_txt_files(txt_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending IG through LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = Anthropic(api_key = claude_api_key)\n",
    "claude_version = \"claude-3-5-sonnet-20240620\"  # \"claude-3-opus-20240229\"   \"claude-3-5-sonnet-20240620\" \"claude-3-sonnet-20240229\" \"claude-3-haiku-20240307\"\n",
    "claude_max_output_tokens = 8192  # claude 3 opus is only 4096 tokens, sonnet is 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to send IG content Claude and request analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add sleep pauses\n",
    "def heartbeat(stop_event, start_time):\n",
    "    \"\"\"Prints elapsed time periodically until stopped.\"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"... still processing ({elapsed:.1f}s elapsed)\")\n",
    "        time.sleep(5)\n",
    "\n",
    "#send message request to claude letting it know an IG is being shared and providing it the action prompt\n",
    "def message_claude(claude_client, user_prompt, content_text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Sends a message to Claude API with the provided prompt and content.\n",
    "    \"\"\"\n",
    "    # Construct the full prompt\n",
    "    full_prompt = f\"\"\"Here is the content of an HL7 FHIR Implementation Guide:\n",
    "\n",
    "{content_text}\n",
    "\n",
    "{user_prompt}\"\"\"\n",
    "    \n",
    "    # Set up heartbeat\n",
    "    start_time = time.time()\n",
    "    stop_event = threading.Event()\n",
    "    heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "    heartbeat_thread.start()\n",
    "    \n",
    "    retry_count = 0\n",
    "    last_error = None\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            print(f\"Sending request to Claude API (attempt {retry_count + 1}/{max_retries})...\")\n",
    "            \n",
    "            response = claude_client.messages.create(\n",
    "                model=claude_version,\n",
    "                max_tokens=claude_max_output_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            print(\"Successfully received response from Claude API\")\n",
    "            response_text = response.content[0].text\n",
    "            return response, response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2 ** retry_count\n",
    "                print(f\"Error occurred: {str(e)}\")\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {max_retries} attempts. Last error: {str(e)}\")\n",
    "                raise\n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            heartbeat_thread.join()\n",
    "\n",
    "#analyze content of IG\n",
    "def analyze_ig(combined_content, prompt):\n",
    "    \"\"\"\n",
    "    Main function to process IG files and get Claude's analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Claude client\n",
    "        claude_client = create_anthropic_client()\n",
    "        \n",
    "        #confirm combined text object has been created\n",
    "        if not combined_content:\n",
    "            raise ValueError(\"No content found in text files\")\n",
    "        \n",
    "        #print characters of combined text object\n",
    "        print(f\"Combined content length: {len(combined_content)} characters\")\n",
    "        \n",
    "        print(\"Sending to Claude API...\")\n",
    "        response, response_text = message_claude(claude_client, prompt, combined_content)\n",
    "        \n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_ig: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined content length: 14999 characters\n",
      "Sending to Claude API...\n",
      "... still processing (0.0s elapsed)\n",
      "Sending request to Claude API (attempt 1/3)...\n",
      "... still processing (5.0s elapsed)\n",
      "... still processing (10.0s elapsed)\n",
      "... still processing (15.0s elapsed)\n",
      "Successfully received response from Claude API\n",
      "Here's a high-level analysis of the HL7 FHIR US Core Implementation Guide based on the provided content:\n",
      "\n",
      "1. High-level summary:\n",
      "This is the US Core Implementation Guide, version 8.0.0-ballot. It defines a set of FHIR profiles, extensions, and other artifacts to represent core health data for use in the United States healthcare system. The guide aims to establish a consistent foundation for FHIR implementations in the US.\n",
      "\n",
      "2. Key profiles and extensions:\n",
      "While not explicitly listed in the provided content, the IG likely includes profiles for common resources like Patient, Observation, Condition, etc. The content shows:\n",
      "- A ValueSet for Clinical Result Observation Categories\n",
      "- A SearchParameter for QuestionnaireResponse authored date\n",
      "- A sample Questionnaire for PHQ-9 (Patient Health Questionnaire)\n",
      "\n",
      "3. Main requirements and constraints:\n",
      "- The IG emphasizes conformance to US Core profiles and capabilities\n",
      "- It references USCDI (United States Core Data for Interoperability) requirements\n",
      "- \"Must Support\" flags are used to indicate required elements\n",
      "- SMART on FHIR capabilities are included\n",
      "\n",
      "4. Notable usage patterns or guidance:\n",
      "- Guidance is provided on various topics including clinical notes, medication lists, basic provenance, and screening/assessments\n",
      "- The IG addresses general FHIR implementation guidance for US healthcare contexts\n",
      "- Security considerations are included\n",
      "\n",
      "5. Significant changes or updates:\n",
      "- This is version 8.0.0-ballot, indicating it's a draft for ballot\n",
      "- The change log mentions \"Changes Between Versions\" suggesting ongoing updates\n",
      "- For the Clinical Result Observation Category ValueSet, it notes \"No changes\" since version 7.0.0\n",
      "\n",
      "Key points:\n",
      "- The IG is focused on standardizing core health data exchange in the US\n",
      "- It provides a foundation for FHIR implementations to ensure consistency and interoperability\n",
      "- The guide covers a wide range of topics from specific profiles to general implementation guidance\n",
      "- It's aligned with US-specific requirements like USCDI and SMART on FHIR\n",
      "- The IG is actively maintained and updated, with this version being a ballot draft\n",
      "\n",
      "This analysis is based on the limited content provided. A full review of the complete IG would offer more comprehensive insights into all profiles, extensions, and requirements defined.\n"
     ]
    }
   ],
   "source": [
    "# draft prompt to ask LLM to summarize IG text\n",
    "prompt = \"\"\"Please analyze this Implementation Guide and provide:\n",
    "1. A high-level summary of what this IG is about\n",
    "2. Key profiles and extensions defined\n",
    "3. Main requirements and constraints\n",
    "4. Notable usage patterns or guidance\n",
    "\n",
    "Please organize the information clearly and highlight particularly important aspects.\"\"\"\n",
    "\n",
    "# analyze partial combined text (text currently too large to all be ingested)\n",
    "result = analyze_ig(combined_content[1:15000], prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with sending the IG in smaller chunks\n",
    "Note: incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will no longer properly run due to changes made to how the text files are combined earlier in the script\n",
    "\n",
    "#defining function to split IG content into chunks\n",
    "def split_content(text, max_bytes=8000000):  # Leave some room for the prompt\n",
    "    \"\"\"\n",
    "    Splits content into chunks that won't exceed Claude's byte limit.\n",
    "    Tries to split at file boundaries marked by === Content from\n",
    "    \"\"\"\n",
    "    \n",
    "    # First split by file markers\n",
    "    file_sections = text.split(\"=== Content from\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_bytes = 0\n",
    "    \n",
    "    for section in file_sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "            \n",
    "        # Add the marker back except for the first section\n",
    "        if current_chunk:\n",
    "            section = \"=== Content from\" + section\n",
    "            \n",
    "        # Calculate bytes of this section\n",
    "        section_bytes = len(section.encode('utf-8'))\n",
    "        \n",
    "        # If adding this section would exceed limit, start new chunk\n",
    "        if current_bytes + section_bytes > max_bytes:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = section\n",
    "            current_bytes = section_bytes\n",
    "        else:\n",
    "            current_chunk += section\n",
    "            current_bytes += section_bytes\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def message_claude(claude_client, user_prompt, content_text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Sends a message to Claude API with the provided prompt and content.\n",
    "    \"\"\"\n",
    "    # Construct the full prompt\n",
    "    full_prompt = f\"\"\"Here is the content of an HL7 FHIR Implementation Guide:\n",
    "\n",
    "{content_text}\n",
    "\n",
    "{user_prompt}\"\"\"\n",
    "    # Check content length in bytes\n",
    "    prompt_bytes = len(full_prompt.encode('utf-8'))\n",
    "    if prompt_bytes > 9000000:  # Claude's limit\n",
    "        raise ValueError(f\"Content too large: {prompt_bytes} bytes\")\n",
    "    \n",
    "    # Set up heartbeat\n",
    "    start_time = time.time()\n",
    "    stop_event = threading.Event()\n",
    "    heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "    heartbeat_thread.start()\n",
    "    \n",
    "    retry_count = 0\n",
    "    last_error = None\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            print(f\"Sending request to Claude API (attempt {retry_count + 1}/{max_retries})...\")\n",
    "            \n",
    "            response = claude_client.messages.create(\n",
    "                model=claude_version,\n",
    "                max_tokens=claude_max_output_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            print(\"Successfully received response from Claude API\")\n",
    "            response_text = response.content[0].text\n",
    "            return response, response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2 ** retry_count\n",
    "                print(f\"Error occurred: {str(e)}\")\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {max_retries} attempts. Last error: {str(e)}\")\n",
    "                raise\n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            heartbeat_thread.join()\n",
    "\n",
    "def analyze_ig_in_chunks(txt_directory, prompt):\n",
    "    \"\"\"\n",
    "    Analyzes the IG content in chunks and combines the results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Claude client\n",
    "        claude_client = create_anthropic_client()\n",
    "        \n",
    "        # Combine all txt files\n",
    "        print(\"Combining text files...\")\n",
    "        combined_content = combine_txt_files(txt_directory)\n",
    "        \n",
    "        if not combined_content:\n",
    "            raise ValueError(\"No content found in text files\")\n",
    "        \n",
    "        print(f\"Total combined content length: {len(combined_content)} characters\")\n",
    "        \n",
    "        # Split content into chunks\n",
    "        chunks = split_content(combined_content)\n",
    "        print(f\"Split content into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Process each chunk\n",
    "        all_responses = []\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\nProcessing chunk {i} of {len(chunks)}...\")\n",
    "            \n",
    "            # Modify prompt for chunks after the first one\n",
    "            if i > 1:\n",
    "                chunk_prompt = f\"\"\"This is chunk {i} of {len(chunks)} from the same Implementation Guide. \n",
    "                Please continue the analysis, focusing on any new information in this chunk. \n",
    "                Do not repeat information you've already covered, only add new findings.\n",
    "                \n",
    "                {prompt}\"\"\"\n",
    "            else:\n",
    "                chunk_prompt = prompt\n",
    "            \n",
    "            response, response_text = message_claude(claude_client, chunk_prompt, chunk)\n",
    "            all_responses.append(response_text)\n",
    "            \n",
    "            print(f\"Completed chunk {i}\")\n",
    "        \n",
    "        # Combine all responses\n",
    "        final_response = \"\\n\\n=== Combined Analysis ===\\n\\n\" + \"\\n\\n=== Additional Findings ===\\n\\n\".join(all_responses)\n",
    "        \n",
    "        return final_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_ig_in_chunks: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining text files...\n",
      "Total combined content length: 9993422 characters\n",
      "Split content into 2 chunks\n",
      "\n",
      "Processing chunk 1 of 2...\n",
      "... still processing (0.0s elapsed)\n",
      "Sending request to Claude API (attempt 1/3)...\n",
      "Error occurred: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 203298 tokens > 199999 maximum'}}\n",
      "Retrying in 2 seconds...\n",
      "... still processing (5.0s elapsed)\n",
      "Sending request to Claude API (attempt 2/3)...\n",
      "Error occurred: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 203298 tokens > 199999 maximum'}}\n",
      "Retrying in 4 seconds...\n",
      "Sending request to Claude API (attempt 3/3)...\n",
      "Failed after 3 attempts. Last error: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 203298 tokens > 199999 maximum'}}\n",
      "Error in analyze_ig_in_chunks: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 203298 tokens > 199999 maximum'}}\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 203298 tokens > 199999 maximum'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m txt_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull-ig/html_only/plain_txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Get analysis\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_ig_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[48], line 181\u001b[0m, in \u001b[0;36manalyze_ig_in_chunks\u001b[0;34m(txt_directory, prompt)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     chunk_prompt \u001b[38;5;241m=\u001b[39m prompt\n\u001b[0;32m--> 181\u001b[0m response, response_text \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_claude\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclaude_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m all_responses\u001b[38;5;241m.\u001b[39mappend(response_text)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 119\u001b[0m, in \u001b[0;36mmessage_claude\u001b[0;34m(claude_client, user_prompt, content_text, max_retries)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request to Claude API (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_count\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclaude_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclaude_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclaude_max_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully received response from Claude API\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/Documents/onclaive/.venv/lib/python3.12/site-packages/anthropic/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/.venv/lib/python3.12/site-packages/anthropic/resources/messages.py:885\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    879\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    880\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    883\u001b[0m     )\n\u001b[0;32m--> 885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 203298 tokens > 199999 maximum'}}"
     ]
    }
   ],
   "source": [
    "result = analyze_ig_in_chunks(txt_directory, prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Read in relevant context files \n",
    "- IG_golden_rules\n",
    "- IG_example\n",
    "- IG_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
