{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring FHIR Implementation Guides (IGs) + LLMs\n",
    "\n",
    "In this notebook, we aim to explore how much LLMs understand about FHIR Implementation Guides (IGs) and investigate ways to upload IG content for deeper analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have langchain-community and beautifulsoup4 installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U json_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as gemini\n",
    "from anthropic import Anthropic\n",
    "from openai import OpenAI\n",
    "import io, threading, time, re, json\n",
    "import pandas as pd\n",
    "from json_repair import repair_json\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in US Core IG JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Be sure that you have downloaded the US Core IG files from https://www.hl7.org/fhir/us/core/package.tgz and placed them in your current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'package'\n",
    "# 'full-ig/site'\n",
    "destination_folder = 'package/json_only'\n",
    "# 'full-ig/html_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store only .json files\n",
    "# html_files = []\n",
    "json_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(source_folder):\n",
    "    # Check if the file ends with .html but not with compound extensions\n",
    "    if file_name.endswith('.json'):\n",
    "                                    # and not (file_name.endswith('.ttl.html') or \n",
    "                                            #  file_name.endswith('.json.html') or \n",
    "                                            #  file_name.endswith('.xml.html') or \n",
    "                                            #  file_name.endswith('.change.history.html')):\n",
    "        json_files.append(file_name)\n",
    "        # Move the file to the destination folder\n",
    "        shutil.copy(os.path.join(source_folder, file_name), destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading HTML with BeautifulSoup4\n",
    "\n",
    "This is archived code to load HTML files with BeautifulSoup4. We are now using JSON files so this step is skipped. \n",
    "\n",
    "\n",
    "TODO: Rewrite HTML loader as a function for potential future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_only_folder = 'full-ig/html_only'\n",
    "json_only_folder = 'package/json_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new folder named \"plain_text\" inside the current directory\n",
    "# processed_files_path = os.path.join(html_only_folder, 'plain_txt')\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "# if not os.path.exists(processed_files_path):\n",
    "#     os.makedirs(processed_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the files processed\n",
    "# processed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through the files in the HTML folder\n",
    "# for file_name in os.listdir(html_only_folder):\n",
    "#     # Full path to the .html file\n",
    "#     html_file_path = os.path.join(html_only_folder, file_name)\n",
    "    \n",
    "#     # Check if it's a file (not a directory)\n",
    "#     if os.path.isfile(html_file_path):\n",
    "#         # Use BSHTMLLoader to load the HTML content\n",
    "#         loader = BSHTMLLoader(html_file_path, bs_kwargs={'features': 'html.parser'})\n",
    "#         data = loader.load()\n",
    "#         # Extract the plain text from the loaded data\n",
    "#         plain_text = '\\n'.join([doc.page_content for doc in data])\n",
    "        \n",
    "#         # Create the output file path with .txt extension\n",
    "#         txt_file_name = file_name.replace('.html', '.txt')\n",
    "#         txt_file_path = os.path.join(processed_files_path, txt_file_name)\n",
    "        \n",
    "#         # Write the extracted plain text to the new .txt file\n",
    "#         with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "#             txt_file.write(plain_text)\n",
    "        \n",
    "#         # Append to processed files list\n",
    "#         processed_files.append(txt_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_files_by_base_name(directory_path, delimiter='-'):\n",
    "    \"\"\"\n",
    "    Group files in the directory by their base name (portion before a delimiter).\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    delimiter (str): The delimiter to split the file name on (default is '-').\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are base names and values are lists of files that share the same base name.\n",
    "    \"\"\"\n",
    "    grouped_files = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):  # Only process .txt files\n",
    "            if delimiter in filename:  # Only consider files with the delimiter\n",
    "                # Get the base name (before the first delimiter)\n",
    "                base_name = filename.split(delimiter)[0]\n",
    "                \n",
    "                # Append the file to the group corresponding to its base name\n",
    "                grouped_files[base_name].append(filename)\n",
    "    \n",
    "    return grouped_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'package/json_only'\n",
    "# 'full-ig/html_only/plain_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_files = group_files_by_base_name(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base name: StructureDefinition (Total files: 63)\n",
      "Base name: ValueSet (Total files: 29)\n",
      "Base name: SearchParameter (Total files: 110)\n",
      "Base name: CapabilityStatement (Total files: 2)\n",
      "Base name: CodeSystem (Total files: 5)\n",
      "Base name: OperationDefinition (Total files: 1)\n",
      "Base name: ImplementationGuide (Total files: 1)\n"
     ]
    }
   ],
   "source": [
    "for base_name, files in grouped_files.items():\n",
    "    print(f\"Base name: {base_name} (Total files: {len(files)})\")\n",
    "    # for file in files:\n",
    "    #     print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_to_folders(directory_path, grouped_files):\n",
    "    \"\"\"\n",
    "    Copy files to folders if the base name group has more than 1 file, and remove them from the original directory.\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    grouped_files (dict): Dictionary of grouped files by base name.\n",
    "    \"\"\"\n",
    "    for base_name, files in grouped_files.items():\n",
    "        if len(files) >= 1:  # Only process groups with more than 1 file\n",
    "            # Create a folder for the base name in the same directory\n",
    "            base_folder = os.path.join(directory_path, base_name)\n",
    "            if not os.path.exists(base_folder):\n",
    "                os.makedirs(base_folder)  # Create the folder if it doesn't exist\n",
    "            print(f\"Created folder: {base_folder}\")\n",
    "            \n",
    "            # Copy each file in the group to the new folder\n",
    "            for file in files:\n",
    "                source_file = os.path.join(directory_path, file)\n",
    "                destination_file = os.path.join(base_folder, file)\n",
    "                shutil.copy(source_file, destination_file)  # Copy the file\n",
    "                # print(f\"Copied {file} to {base_folder}\")\n",
    "                \n",
    "                # Remove the file from the original directory\n",
    "                # os.remove(source_file)\n",
    "                # print(f\"Removed {file} from original directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder: package/json_only/StructureDefinition\n",
      "Created folder: package/json_only/ValueSet\n",
      "Created folder: package/json_only/SearchParameter\n",
      "Created folder: package/json_only/CapabilityStatement\n",
      "Created folder: package/json_only/CodeSystem\n",
      "Created folder: package/json_only/OperationDefinition\n",
      "Created folder: package/json_only/ImplementationGuide\n"
     ]
    }
   ],
   "source": [
    "copy_files_to_folders(directory_path, grouped_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files categories match the files that we had identified to keep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing files\n",
    "\n",
    "In this step, we're preparing our loaded files by combining JSONs in each directory into a singular JSON, structured with top level items: \"resource_Type\", \"total\", and \"entry\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_json_files(folder_path):\n",
    "    \"\"\"\n",
    "    Combines all JSON files in a folder into a single array of JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of JSON objects from all files\n",
    "    \"\"\"\n",
    "    combined_json = []\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    json_content = json.load(file)\n",
    "                    combined_json.append(json_content)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                \n",
    "    return combined_json\n",
    "\n",
    "def create_consolidated_jsons(base_directory='package/json_only'):\n",
    "    \"\"\"\n",
    "    Creates consolidated JSON files for each subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        base_directory (str): Base directory containing the categorized folders\n",
    "    \"\"\"\n",
    "    # Get all subdirectories\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "              if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    # Process each subdirectory\n",
    "    for subdir in subdirs:\n",
    "        folder_path = os.path.join(base_directory, subdir)\n",
    "        print(f\"Processing {subdir}...\")\n",
    "        \n",
    "        # Combine all JSON files in this folder\n",
    "        combined_data = combine_json_files(folder_path)\n",
    "        \n",
    "        if combined_data:\n",
    "            # Create output filename\n",
    "            output_filename = f\"{subdir}_combined.json\"\n",
    "            output_path = os.path.join(base_directory, output_filename)\n",
    "            \n",
    "            # Write the combined JSON to a file\n",
    "            try:\n",
    "                with open(output_path, 'w') as outfile:\n",
    "                    json.dump({\n",
    "                        \"resourceType\": subdir,\n",
    "                        \"total\": len(combined_data),\n",
    "                        \"entry\": combined_data\n",
    "                    }, outfile, indent=2)\n",
    "                print(f\"Created {output_filename} with {len(combined_data)} entries\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing {output_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing StructureDefinition...\n",
      "Created StructureDefinition_combined.json with 63 entries\n",
      "Processing CapabilityStatement...\n",
      "Created CapabilityStatement_combined.json with 2 entries\n",
      "Processing CodeSystem...\n",
      "Created CodeSystem_combined.json with 5 entries\n",
      "Processing ValueSet...\n",
      "Created ValueSet_combined.json with 29 entries\n",
      "Processing SearchParameter...\n",
      "Created SearchParameter_combined.json with 110 entries\n",
      "Processing ImplementationGuide...\n",
      "Created ImplementationGuide_combined.json with 1 entries\n",
      "Processing OperationDefinition...\n",
      "Created OperationDefinition_combined.json with 1 entries\n"
     ]
    }
   ],
   "source": [
    "# Create the consolidated JSON files\n",
    "create_consolidated_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to identify narrative files and use that content to provide context to the LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archived functions/code to combine txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def combine_txt_files(directory_path):\n",
    "#     \"\"\"Combines all .txt files in the specified directory into a single string.\"\"\"\n",
    "#     combined_text = []\n",
    "#     #iterate through txt files in directory\n",
    "#     for filename in os.listdir(directory_path):\n",
    "#         if filename.endswith('.txt'):\n",
    "#             file_path = os.path.join(directory_path, filename)\n",
    "#             try:\n",
    "#                 with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#                     content = file.read()\n",
    "#                     #append text from txt file to combined_text\n",
    "#                     combined_text.append(content)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "#     return \"\\n\".join(combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where txt files are located\n",
    "# txt_directory = 'full-ig/html_only/plain_txt/SearchParameter'\n",
    "# create combined text object\n",
    "# combined_content = combine_txt_files(txt_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combined_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file\n",
    "with open('/Users/amathur/Documents/ONCLAIVE/onclaive-aanchalwip/package/json_only/ImplementationGuide/ImplementationGuide-hl7.fhir.us.core.json', 'r') as file:\n",
    "    implementation_guide = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending IG through LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in API keys for Claude, Gemini, and GPT from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "claude_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "OpenAI.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = Anthropic(api_key = claude_api_key)\n",
    "claude_version = \"claude-3-5-sonnet-20240620\"  # \"claude-3-opus-20240229\"   \"claude-3-5-sonnet-20240620\" \"claude-3-sonnet-20240229\" \"claude-3-haiku-20240307\"\n",
    "claude_max_output_tokens = 8192  # claude 3 opus is only 4096 tokens, sonnet is 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to send IG content Claude and request analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CERT_PATH = '/Users/amathur/ca-certificates.crt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anthropic_client():\n",
    "    \"\"\"Create Anthropic client with proper certificate verification\"\"\"\n",
    "    verify_path = CERT_PATH if os.path.exists(CERT_PATH) else True\n",
    "    http_client = httpx.Client(\n",
    "        verify=verify_path,\n",
    "        timeout=30.0\n",
    "    )\n",
    "    return Anthropic(\n",
    "        api_key=claude_api_key,\n",
    "        http_client=http_client\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add sleep pauses\n",
    "def heartbeat(stop_event, start_time):\n",
    "    \"\"\"Prints elapsed time periodically until stopped.\"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"... still processing ({elapsed:.1f}s elapsed)\")\n",
    "        time.sleep(5)\n",
    "\n",
    "#send message request to claude letting it know an IG is being shared and providing it the action prompt\n",
    "def message_claude(claude_client, user_prompt, content_text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Sends a message to Claude API with the provided prompt and content.\n",
    "    \"\"\"\n",
    "    # Construct the full prompt\n",
    "    full_prompt = f\"\"\"Here is the content of an HL7 FHIR Implementation Guide:\n",
    "\n",
    "{content_text}\n",
    "\n",
    "{user_prompt}\"\"\"\n",
    "    \n",
    "    # Set up heartbeat\n",
    "    start_time = time.time()\n",
    "    stop_event = threading.Event()\n",
    "    heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "    heartbeat_thread.start()\n",
    "    \n",
    "    retry_count = 0\n",
    "    last_error = None\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            print(f\"Sending request to Claude API (attempt {retry_count + 1}/{max_retries})...\")\n",
    "            \n",
    "            response = claude_client.messages.create(\n",
    "                model=claude_version,\n",
    "                max_tokens=claude_max_output_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            print(\"Successfully received response from Claude API\")\n",
    "            response_text = response.content[0].text\n",
    "            return response, response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2 ** retry_count\n",
    "                print(f\"Error occurred: {str(e)}\")\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {max_retries} attempts. Last error: {str(e)}\")\n",
    "                raise\n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            heartbeat_thread.join()\n",
    "\n",
    "#analyze content of IG\n",
    "def analyze_ig(content, prompt):\n",
    "    \"\"\"\n",
    "    Main function to process IG files and get Claude's analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Claude client\n",
    "        claude_client = create_anthropic_client()\n",
    "        \n",
    "        #confirm combined text object has been created\n",
    "        if not content:\n",
    "            raise ValueError(\"No content found in text files\")\n",
    "        \n",
    "        #print characters of combined text object\n",
    "        print(f\"Combined content length: {len(content)} characters\")\n",
    "        \n",
    "        print(\"Sending to Claude API...\")\n",
    "        response, response_text = message_claude(claude_client, prompt, content)\n",
    "        \n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_ig: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the implementation guide\n",
    "with open(\"/Users/amathur/Documents/ONCLAIVE/onclaive-aanchalwip/package/json_only/ImplementationGuide/ImplementationGuide-hl7.fhir.us.core.json\", \"r\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined content length: 292372 characters\n",
      "Sending to Claude API...\n",
      "... still processing (0.0s elapsed)\n",
      "Sending request to Claude API (attempt 1/3)...\n",
      "... still processing (5.0s elapsed)\n",
      "... still processing (10.0s elapsed)\n",
      "... still processing (15.0s elapsed)\n",
      "... still processing (20.0s elapsed)\n",
      "... still processing (25.0s elapsed)\n",
      "Successfully received response from Claude API\n",
      "This JSON file represents an Implementation Guide (IG) for the US Core FHIR profiles. Here are the key points:\n",
      "\n",
      "1. Purpose: It defines minimum conformance requirements for accessing patient data based on FHIR Version R4.\n",
      "\n",
      "2. Origin: It's based on requirements from Argonaut pilot implementations, ONC 2015 Edition Common Clinical Data Set (CCDS), and ONC U.S. Core Data for Interoperability (USCDI) v1.\n",
      "\n",
      "3. Scope: It covers various FHIR resources and profiles, including AllergyIntolerance, CarePlan, CareTeam, Condition, Device, DiagnosticReport, DocumentReference, Encounter, Goal, Immunization, Location, Medication, Observation, Organization, Patient, Practitioner, Procedure, and more.\n",
      "\n",
      "4. Content: The IG includes:\n",
      "   - Profiles and extensions for various FHIR resources\n",
      "   - Capability statements for servers and clients\n",
      "   - Search parameters and operations\n",
      "   - Terminology (value sets and code systems)\n",
      "   - Examples of FHIR resources\n",
      "\n",
      "5. Structure: The IG is organized into sections like Conformance, Guidance, Profiles and Extensions, Capability Statements, Search Parameters and Operations, Terminology, Security, and Examples.\n",
      "\n",
      "6. Version: This is version 7.0.0 of the US Core Implementation Guide.\n",
      "\n",
      "7. Publisher: HL7 International / Cross-Group Projects\n",
      "\n",
      "8. Intended Use: This guide serves as a foundation for future US Realm FHIR implementation guides and is used by various projects like DAF-Research, QI-Core, and CIMI.\n",
      "\n",
      "The overall purpose of this Implementation Guide is to standardize and facilitate the exchange of patient data in the US healthcare system using FHIR, ensuring interoperability and consistent implementation across different systems and use cases.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "The the content below is a JSON file in <JSON_CONTENT></JSON_CONTENT> tags. \n",
    "Can you summarize the key information and purpose of this data? \n",
    "\n",
    "<JSON_CONTENT>\n",
    "{content}\n",
    "</JSON_CONTENT>\n",
    "\"\"\"\n",
    "result = analyze_ig(content, prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_version = \"models/gemini-1.5-pro-001\" \n",
    "gemini_max_output_tokens = 8192\n",
    "temp = 0.75  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini maintains history in a chat session object. Messages are sent to a specific chat session.\n",
    "def message_gemini(prompt, chat_session=None):\n",
    "    global gemini_version, temp, gemini_max_output_tokens\n",
    "    if chat_session is None:\n",
    "        model = gemini.GenerativeModel(model_name=gemini_version, generation_config={\"max_output_tokens\": gemini_max_output_tokens, \"response_mime_type\": \"application/json\"})\n",
    "        chat_session = model.start_chat()\n",
    "    start_time = time.time()\n",
    "    stop_event = threading.Event()\n",
    "    heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "    heartbeat_thread.start()\n",
    "    try:\n",
    "        resoponse = chat_session.send_message(prompt, stream=False)\n",
    "    finally:\n",
    "        stop_event.set()  # Signal the heartbeat to stop\n",
    "        heartbeat_thread.join()  # Wait for the heartbeat thread to finish\n",
    "    return resoponse.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "The the content below is a JSON file in <JSON_CONTENT></JSON_CONTENT> tags. \n",
    "Can you summarize the key information and purpose of this data? \n",
    "\n",
    "<JSON_CONTENT>\n",
    "{content}\n",
    "</JSON_CONTENT>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gemini.GenerativeModel(model_name='gemini-1.5-flash-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 8192)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = gemini.get_model('models/gemini-1.5-flash-latest')\n",
    "(model_info.input_token_limit, model_info.output_token_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_tokens: 72174"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_tokens(prompt[0:model_info.input_token_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This JSON file represents an FHIR Implementation Guide (IG) named \"US Core Implementation Guide\". It defines the minimum conformance requirements for accessing patient data in the US healthcare system. \n",
      "\n",
      "Here's a breakdown of the key information:\n",
      "\n",
      "* **Purpose:** To establish a standard for interoperability between healthcare systems in the US by defining a set of FHIR profiles, extensions, search parameters, and operations that all systems must adhere to.\n",
      "* **Version:**  7.0.0\n",
      "* **FHIR Version:** 4.0.1\n",
      "* **Based On:** The Argonaut pilot implementations, ONC 2015 Edition Common Clinical Data Set (CCDS), and ONC U.S. Core Data for Interoperability (USCDI) v1.\n",
      "* **Scope:**  Defines minimum expectations for accessing patient data for various resources including Patient, Encounter, Condition, Observation, Medication, Immunization, and many more.\n",
      "* **Content:**\n",
      "    * **Profiles:** Specifies the structure and content of different FHIR resources. \n",
      "    * **Extensions:** Defines custom elements to extend the standard FHIR resources.\n",
      "    * **Search Parameters:** Defines standardized search criteria for retrieving specific data.\n",
      "    * **Operations:** Defines additional operations beyond the standard RESTful interactions.\n",
      "    * **Examples:** Provides sample data to illustrate the usage of profiles and extensions.\n",
      "    * **Terminology:**  Specifies the use of value sets and code systems for defining the data values. \n",
      "    * **Guidance:**  Provides instructions and recommendations for implementing the IG.\n",
      "    * **Security:** Includes information about security requirements for data exchange.\n",
      "\n",
      "This IG is crucial for fostering interoperability in US healthcare, enabling systems to exchange patient information securely and efficiently. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(prompt[0:model_info.input_token_limit])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This JSON file represents a FHIR Implementation Guide (IG) called \"US Core Implementation Guide\". It's designed to define the minimum conformance requirements for accessing patient data within the US healthcare system. \n",
    "\n",
    "Here's a breakdown of the key information and purpose:\n",
    "\n",
    "**Key Information:**\n",
    "\n",
    "* **ID:** hl7.fhir.us.core\n",
    "* **URL:** http://hl7.org/fhir/us/core/ImplementationGuide/hl7.fhir.us.core\n",
    "* **Version:** 7.0.0\n",
    "* **Name:** USCore\n",
    "* **Title:** US Core Implementation Guide\n",
    "* **Status:** active\n",
    "* **Publisher:** HL7 International / Cross-Group Projects\n",
    "* **Description:**  The IG builds upon FHIR R4 and incorporates requirements from past initiatives like Argonaut, ONC CCDS, and USCDI. It aims to facilitate interoperability by establishing minimum standards for accessing patient data. \n",
    "* **Jurisdiction:** US\n",
    "* **FHIR Version:** 4.0.1\n",
    "* **Dependencies:**  Relies on several other FHIR IGs and packages, such as HL7 Terminology, Smart App Launch, VSAC, and others.\n",
    "* **Profiles and Extensions:** Defines numerous FHIR profiles and extensions for various resource types (e.g., Patient, Encounter, Observation, MedicationRequest). These profiles detail specific requirements for data elements, codes, and value sets.\n",
    "* **Search Parameters and Operations:** Includes search parameters and operations, such as \"$docref\", for retrieving patient data in a standardized way. \n",
    "* **Terminology:**  Uses standard terminologies like LOINC and SNOMED CT for data elements.\n",
    "* **Examples:** Provides numerous example resources that demonstrate the use of the profiles and extensions.\n",
    "* **Guidance:** Offers general guidance, clinical notes guidance, medication list guidance, and specific guidance on USCDI requirements. \n",
    "* **Future Directions:**  Outlines plans for future expansion and updates to the US Core IG.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "The main purpose of the US Core Implementation Guide is to:\n",
    "\n",
    "* **Promote interoperability:** By defining common standards for accessing patient data, it allows different healthcare systems to exchange information more effectively.\n",
    "* **Support data exchange:**  It provides a framework for implementing FHIR within the US healthcare landscape, ensuring that systems can share essential patient data.\n",
    "* **Enable data access:**  It establishes the minimum requirements for accessing and using patient data, promoting patient-centered care.\n",
    "* **Facilitate certification:**  It serves as a foundation for ONC Health IT certification, ensuring that systems meet the necessary standards for interoperability. \n",
    "\n",
    "In essence, the US Core Implementation Guide acts as a roadmap for implementing FHIR in the US, promoting interoperability and enhancing the use of electronic health information. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft prompt to ask LLM to summarize IG text\n",
    "prompt = \"\"\"Please analyze this Implementation Guide and provide:\n",
    "1. A high-level summary of what this IG is about\n",
    "2. Key profiles and extensions defined\n",
    "3. Main requirements and constraints\n",
    "4. Notable usage patterns or guidance\n",
    "\n",
    "In as much detail as possible, please organize the information clearly and highlight particularly important aspects.\"\"\"\n",
    "\n",
    "# analyze partial combined text (text currently too large to all be ingested)\n",
    "# result = analyze_ig(combined_content[1:15000], prompt)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with sending the IG in smaller chunks\n",
    "Note: incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will no longer properly run due to changes made to how the text files are combined earlier in the script\n",
    "\n",
    "#defining function to split IG content into chunks\n",
    "def split_content(text, max_bytes=8000000):  # Leave some room for the prompt\n",
    "    \"\"\"\n",
    "    Splits content into chunks that won't exceed Claude's byte limit.\n",
    "    Tries to split at file boundaries marked by === Content from\n",
    "    \"\"\"\n",
    "    \n",
    "    # First split by file markers\n",
    "    file_sections = text.split(\"=== Content from\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_bytes = 0\n",
    "    \n",
    "    for section in file_sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "            \n",
    "        # Add the marker back except for the first section\n",
    "        if current_chunk:\n",
    "            section = \"=== Content from\" + section\n",
    "            \n",
    "        # Calculate bytes of this section\n",
    "        section_bytes = len(section.encode('utf-8'))\n",
    "        \n",
    "        # If adding this section would exceed limit, start new chunk\n",
    "        if current_bytes + section_bytes > max_bytes:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = section\n",
    "            current_bytes = section_bytes\n",
    "        else:\n",
    "            current_chunk += section\n",
    "            current_bytes += section_bytes\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def message_claude(claude_client, user_prompt, content_text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Sends a message to Claude API with the provided prompt and content.\n",
    "    \"\"\"\n",
    "    # Construct the full prompt\n",
    "    full_prompt = f\"\"\"Here is the content of an HL7 FHIR Implementation Guide:\n",
    "\n",
    "{content_text}\n",
    "\n",
    "{user_prompt}\"\"\"\n",
    "    # Check content length in bytes\n",
    "    prompt_bytes = len(full_prompt.encode('utf-8'))\n",
    "    if prompt_bytes > 9000000:  # Claude's limit\n",
    "        raise ValueError(f\"Content too large: {prompt_bytes} bytes\")\n",
    "    \n",
    "    # Set up heartbeat\n",
    "    start_time = time.time()\n",
    "    stop_event = threading.Event()\n",
    "    heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "    heartbeat_thread.start()\n",
    "    \n",
    "    retry_count = 0\n",
    "    last_error = None\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            print(f\"Sending request to Claude API (attempt {retry_count + 1}/{max_retries})...\")\n",
    "            \n",
    "            response = claude_client.messages.create(\n",
    "                model=claude_version,\n",
    "                max_tokens=claude_max_output_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            print(\"Successfully received response from Claude API\")\n",
    "            response_text = response.content[0].text\n",
    "            return response, response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2 ** retry_count\n",
    "                print(f\"Error occurred: {str(e)}\")\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {max_retries} attempts. Last error: {str(e)}\")\n",
    "                raise\n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            heartbeat_thread.join()\n",
    "\n",
    "def analyze_ig_in_chunks(txt_directory, prompt):\n",
    "    \"\"\"\n",
    "    Analyzes the IG content in chunks and combines the results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Claude client\n",
    "        claude_client = claude_client = Anthropic()\n",
    "        \n",
    "        # Combine all txt files\n",
    "        print(\"Combining text files...\")\n",
    "        combined_content = combine_txt_files(txt_directory)\n",
    "        \n",
    "        if not combined_content:\n",
    "            raise ValueError(\"No content found in text files\")\n",
    "        \n",
    "        print(f\"Total combined content length: {len(combined_content)} characters\")\n",
    "        \n",
    "        # Split content into chunks\n",
    "        chunks = split_content(combined_content)\n",
    "        print(f\"Split content into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Process each chunk\n",
    "        all_responses = []\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\nProcessing chunk {i} of {len(chunks)}...\")\n",
    "            \n",
    "            # Modify prompt for chunks after the first one\n",
    "            if i > 1:\n",
    "                chunk_prompt = f\"\"\"This is chunk {i} of {len(chunks)} from the same Implementation Guide. \n",
    "                Please continue the analysis, focusing on any new information in this chunk. \n",
    "                Do not repeat information you've already covered, only add new findings.\n",
    "                \n",
    "                {prompt}\"\"\"\n",
    "            else:\n",
    "                chunk_prompt = prompt\n",
    "            \n",
    "            response, response_text = message_claude(claude_client, chunk_prompt, chunk)\n",
    "            all_responses.append(response_text)\n",
    "            \n",
    "            print(f\"Completed chunk {i}\")\n",
    "        \n",
    "        # Combine all responses\n",
    "        final_response = \"\\n\\n=== Combined Analysis ===\\n\\n\" + \"\\n\\n=== Additional Findings ===\\n\\n\".join(all_responses)\n",
    "        \n",
    "        return final_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_ig_in_chunks: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = analyze_ig_in_chunks(txt_directory, prompt)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Images to LLM (Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in ./.venv/lib/python3.12/site-packages (0.37.1)\n",
      "Requirement already satisfied: IPython in ./.venv/lib/python3.12/site-packages (8.29.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (0.20.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.12/site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from IPython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from IPython) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from IPython) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from IPython) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.12/site-packages (from IPython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./.venv/lib/python3.12/site-packages (from IPython) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from IPython) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->IPython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->IPython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.23.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers>=0.13.0->anthropic) (0.26.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack-data->IPython) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack-data->IPython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack-data->IPython) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->IPython) (1.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#ensure you have installed IPython\n",
    "#%pip install anthropic IPython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from IPython.display import Image\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up claude instance\n",
    "client = Anthropic()\n",
    "MODEL_NAME = \"claude-3-opus-20240229\"\n",
    "\n",
    "#function to decode base64 encoded image\n",
    "def get_base64_encoded_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        binary_data = image_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode('utf-8')\n",
    "        return base64_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to image\n",
    "image_path=''\n",
    "\n",
    "#set message and prompt to Claude API\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": 'user',\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/jpeg\", \"data\": get_base64_encoded_image(image_path)}},\n",
    "            {\"type\": \"text\", \"text\": \"Explain the diagram\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# generate response\n",
    "response = client.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=2048,\n",
    "    messages=message_list\n",
    ")\n",
    "\n",
    "#print the text of claude's response\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Read in relevant context files \n",
    "- IG_golden_rules\n",
    "- IG_example\n",
    "- IG_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
