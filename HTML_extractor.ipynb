{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting items from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores extracting narrative elements from HTML files using multiple methods incuding LangChain's `AsyncHtmlLoader` to convert URLs to to Markdown, LangChain's `UnstructuredHTMLLoader` and `BSHTMLLoader` methods to convert HTML from local folders, as well as a custom content extractor method `ContextExtractor` using BeautifulSoup to parse HTMLMTL content and extract specific elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amathur/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from bs4.element import Tag\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.document_transformers import MarkdownifyTransformer\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain tool (Markdownify) to convert HTML to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to store Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where you want to save the markdown files\n",
    "output_dir = 'PlanNet/site/markdown'\n",
    "# Create output directory if it doesn't exist\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary libraries if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  markdownify\n",
    "# %pip install -U lxml\n",
    "# %pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_url(url):\n",
    "    \"\"\"Extract filename from URL and convert to markdown filename.\"\"\"\n",
    "    # Parse the URL and get the path\n",
    "    path = urlparse(url).path\n",
    "    \n",
    "    # Get the last part of the path (filename)\n",
    "    filename = path.split('/')[-1]\n",
    "    \n",
    "    # Remove .html extension if present\n",
    "    filename = filename.replace('.html', '')\n",
    "    \n",
    "    # Convert to title case and replace special characters\n",
    "    filename = filename.replace('-', '_')\n",
    "    \n",
    "    # Add .md extension\n",
    "    return f\"{filename}.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_urls_to_markdown(urls):\n",
    "    \"\"\"Convert multiple URLs to markdown files.\"\"\"\n",
    "    # Initialize loaders and transformers\n",
    "    loader = AsyncHtmlLoader(urls)\n",
    "    md_transformer = MarkdownifyTransformer()\n",
    "    \n",
    "    # Load all documents\n",
    "    docs = loader.load()\n",
    "    converted_docs = md_transformer.transform_documents(docs)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"PlanNet/site/markdown\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each document\n",
    "    for url, doc in zip(urls, converted_docs):\n",
    "        # Generate filename from URL\n",
    "        filename = get_filename_from_url(url)\n",
    "        \n",
    "        # Create full file path\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Write content to file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(doc.page_content)\n",
    "        \n",
    "        print(f\"Created: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/index.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/ChangeHistory.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/examples.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/implementation.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/profiles.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/artifacts.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/CapabilityStatement-plan-net.html\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 7/7 [00:00<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: index.md\n",
      "Created: ChangeHistory.md\n",
      "Created: examples.md\n",
      "Created: implementation.md\n",
      "Created: profiles.md\n",
      "Created: artifacts.md\n",
      "Created: CapabilityStatement_plan_net.md\n"
     ]
    }
   ],
   "source": [
    "convert_urls_to_markdown(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files should be stored in the `PlanNet/site/markdown` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental coverters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from local HTML files to Markdown\n",
    "\n",
    "WIP: While this function works, it's not perfect. It's not preserving the structure of the HTML well. Would reccomend using the langchain tool above instead. I've tried using two document loader functions `BSHTMLLoader` as well as `UnstructuredHTMLLoader` but neither seem to work well at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_html(html_file):\n",
    "    \"\"\"Convert HTML filename to markdown filename.\"\"\"\n",
    "    # Get the base filename without extension\n",
    "    base_name = os.path.splitext(os.path.basename(html_file))[0]\n",
    "    \n",
    "    # Convert to title case and replace special characters\n",
    "    filename = base_name.replace('-', '_')\n",
    "    \n",
    "    # Add .md extension\n",
    "    return f\"{filename}.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_html_to_markdown(html_files):\n",
    "    \"\"\"Convert multiple local HTML files to markdown files.\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = \"PlanNet/site/markdown\" #NOTE: This is the same directory as the url to markdown converter above. Change to keep the results seprate. \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each HTML file\n",
    "    for html_file in html_files:\n",
    "        try:\n",
    "            # Load and convert the HTML file\n",
    "            loader = UnstructuredHTMLLoader(html_file)\n",
    "            doc = loader.load()[0]  # BSHTMLLoader returns a list\n",
    "            \n",
    "            # Transform to markdown\n",
    "            md_transformer = MarkdownifyTransformer()\n",
    "            converted_doc = md_transformer.transform_documents([doc])[0]\n",
    "            \n",
    "            # Generate output filename\n",
    "            output_filename = get_filename_from_html(html_file)\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Write content to file\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(converted_doc.page_content)\n",
    "            \n",
    "            print(f\"Successfully converted {html_file} to {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {html_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_files = [\n",
    "    \"PlanNet/site/CapabilityStatement-plan-net.html\",\n",
    "    \"PlanNet/site/ChangeHistory.html\",\n",
    "    \"PlanNet/site/index.html\",\n",
    "    \"PlanNet/site/examples.html\",\n",
    "    \"PlanNet/site/implementation.html\",\n",
    "    \"PlanNet/site/profiles.html\",\n",
    "    \"PlanNet/site/artifacts.html\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_html_to_markdown(html_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Content Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class `ContentExtractor` to to extract content from HTML files. This class has methods to extract text, tables, and list elements from the HTML. The extracted content is then formatted as markdown and written to a file. The class has methods that also check to see if elements have been processed to avoid duplicates. From images, there is a method (`_extract_images`) to pull the src and alt text and format as markdown image with additional source info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextExtractor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the context extractor\"\"\"\n",
    "        self.processed_elements = set()\n",
    "        \n",
    "    def _has_been_processed(self, element):\n",
    "        \"\"\"Check if an element has already been processed\"\"\"\n",
    "        if not isinstance(element, Tag):\n",
    "            return False\n",
    "        return element.get('data-processed') == 'true'\n",
    "    \n",
    "    def _mark_processed(self, element):\n",
    "        \"\"\"Mark an element as processed\"\"\"\n",
    "        if isinstance(element, Tag):\n",
    "            element['data-processed'] = 'true'\n",
    "            self.processed_elements.add(element)\n",
    "\n",
    "    def _extract_images(self, element):\n",
    "        \"\"\"\n",
    "        Extract image information including src and alt text\n",
    "        \n",
    "        Args:\n",
    "            element: BeautifulSoup element containing images\n",
    "        Returns:\n",
    "            list: Formatted image information in Markdown\n",
    "        \"\"\"\n",
    "        if self._has_been_processed(element):\n",
    "            return []\n",
    "            \n",
    "        images = []\n",
    "        for img in element.find_all('img', recursive=False):\n",
    "            src = img.get('src', '')\n",
    "            alt = img.get('alt', '')\n",
    "            if src:\n",
    "                # Format as Markdown image with additional source info\n",
    "                images.append(f\"![{alt}]({src})\")\n",
    "                images.append(f\"*Image source: {src}*\")\n",
    "                images.append(f\"*Image description: {alt}*\")\n",
    "                images.append(\"\")  # Add blank line after each image\n",
    "                \n",
    "        self._mark_processed(element)\n",
    "        return images\n",
    "\n",
    "    def _extract_list_items(self, list_element, level=0, parent_type=None):\n",
    "        \"\"\"Extract list items with improved nested list handling\"\"\"\n",
    "        if self._has_been_processed(list_element):\n",
    "            return []\n",
    "            \n",
    "        items = []\n",
    "        for item in list_element.find_all('li', recursive=False):\n",
    "            if not self._has_been_processed(item):\n",
    "                # Get direct text content of the li element (excluding nested list text)\n",
    "                item_text = ''\n",
    "                for content in item.children:\n",
    "                    if isinstance(content, Tag):\n",
    "                        if content.name not in ['ul', 'ol']:\n",
    "                            if content.name == 'img':\n",
    "                                # Handle images within list items\n",
    "                                image_info = self._extract_images(content.parent)\n",
    "                                items.extend([f\"{'    ' * level}{line}\" for line in image_info])\n",
    "                            else:\n",
    "                                item_text += content.get_text(strip=True) + ' '\n",
    "                    else:\n",
    "                        item_text += content.strip() + ' '\n",
    "                item_text = item_text.strip()\n",
    "                \n",
    "                # Format the list item\n",
    "                prefix = '    ' * level\n",
    "                if list_element.name == 'ol':\n",
    "                    items.append(f\"{prefix}1. {item_text}\")\n",
    "                else:\n",
    "                    items.append(f\"{prefix}- {item_text}\")\n",
    "                \n",
    "                # Handle nested lists\n",
    "                nested_lists = item.find_all(['ul', 'ol'], recursive=False)\n",
    "                for nested_list in nested_lists:\n",
    "                    nested_items = []\n",
    "                    for nested_item in nested_list.find_all('li', recursive=False):\n",
    "                        nested_text = nested_item.get_text(strip=True)\n",
    "                        if nested_list.name == 'ol':\n",
    "                            nested_items.append(f\"{prefix}    * {nested_text}\")\n",
    "                        else:\n",
    "                            nested_items.append(f\"{prefix}    * {nested_text}\")\n",
    "                    items.extend(nested_items)\n",
    "                \n",
    "                self._mark_processed(item)\n",
    "        \n",
    "        self._mark_processed(list_element)\n",
    "        return items\n",
    "\n",
    "    def _extract_table(self, table):\n",
    "        \"\"\"Extract table content in Markdown format\"\"\"\n",
    "        if self._has_been_processed(table):\n",
    "            return \"\"\n",
    "            \n",
    "        rows = []\n",
    "        headers = []\n",
    "        \n",
    "        header_row = table.find('thead') or table.find('tr')\n",
    "        if header_row:\n",
    "            headers = [cell.get_text(strip=True) for cell in header_row.find_all(['th', 'td'])]\n",
    "        \n",
    "        for row in table.find_all('tr')[1:] if headers else table.find_all('tr'):\n",
    "            row_data = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\n",
    "            if any(row_data):\n",
    "                rows.append(row_data)\n",
    "        \n",
    "        table_str = []\n",
    "        if headers:\n",
    "            table_str.append(\"| \" + \" | \".join(headers) + \" |\")\n",
    "            table_str.append(\"|\" + \"|\".join([\" --- \" for _ in headers]) + \"|\")\n",
    "        \n",
    "        for row in rows:\n",
    "            if headers:\n",
    "                row.extend([''] * (len(headers) - len(row)))\n",
    "            table_str.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "        \n",
    "        self._mark_processed(table)\n",
    "        return \"\\n\".join(table_str)\n",
    "\n",
    "    def extract_context(self, html_content):\n",
    "        \"\"\"Extract content with improved list and image handling\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        self.processed_elements.clear()\n",
    "        context_elements = []\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup(['script', 'style', 'nav', 'footer']):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Process headers and their content\n",
    "        for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            if not self._has_been_processed(header):\n",
    "                level = int(header.name[1])\n",
    "                header_text = header.get_text().strip()\n",
    "                \n",
    "                if header_text:\n",
    "                    context_elements.append(f\"\\n{'#' * level} {header_text}\\n\")\n",
    "                    self._mark_processed(header)\n",
    "                    \n",
    "                    # Process content until next header\n",
    "                    next_element = header.find_next()\n",
    "                    while next_element and not next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                        if not self._has_been_processed(next_element):\n",
    "                            if next_element.name == 'p':\n",
    "                                # Handle images within paragraphs\n",
    "                                if next_element.find('img'):\n",
    "                                    image_info = self._extract_images(next_element)\n",
    "                                    context_elements.extend(image_info)\n",
    "                                else:\n",
    "                                    text = next_element.get_text().strip()\n",
    "                                    if text:\n",
    "                                        context_elements.append(text)\n",
    "                                        context_elements.append(\"\")\n",
    "                            elif next_element.name in ['ul', 'ol']:\n",
    "                                list_items = self._extract_list_items(next_element)\n",
    "                                if list_items:\n",
    "                                    context_elements.extend(list_items)\n",
    "                                    context_elements.append(\"\")\n",
    "                            elif next_element.name == 'table':\n",
    "                                table_content = self._extract_table(next_element)\n",
    "                                if table_content:\n",
    "                                    context_elements.append(table_content)\n",
    "                                    context_elements.append(\"\")\n",
    "                        \n",
    "                        next_element = next_element.find_next()\n",
    "\n",
    "        # Process any remaining top-level images\n",
    "        for img_container in soup.find_all('p'):\n",
    "            if img_container.find('img') and not self._has_been_processed(img_container):\n",
    "                image_info = self._extract_images(img_container)\n",
    "                if image_info:\n",
    "                    context_elements.extend(image_info)\n",
    "        \n",
    "        # Clean up repeated empty lines\n",
    "        cleaned_elements = []\n",
    "        prev_empty = False\n",
    "        for element in context_elements:\n",
    "            if element.strip() == \"\":\n",
    "                if not prev_empty:\n",
    "                    cleaned_elements.append(element)\n",
    "                    prev_empty = True\n",
    "            else:\n",
    "                cleaned_elements.append(element)\n",
    "                prev_empty = False\n",
    "        \n",
    "        return cleaned_elements\n",
    "\n",
    "    def save_context(self, context_elements, output_file):\n",
    "        \"\"\"Save context to Markdown file\"\"\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for element in context_elements:\n",
    "                f.write(f\"{element}\\n\")\n",
    "\n",
    "    def process_html_file(self, input_file, output_file):\n",
    "        \"\"\"Process HTML file to Markdown\"\"\"\n",
    "        try:\n",
    "            output_file = os.path.splitext(output_file)[0] + '.md'\n",
    "            \n",
    "            with open(input_file, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "            \n",
    "            context_elements = self.extract_context(html_content)\n",
    "            self.save_context(context_elements, output_file)\n",
    "            self._display_summary(input_file, output_file, len(context_elements))\n",
    "            \n",
    "            return context_elements\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f'<div style=\"color: red;\">Error processing file: {str(e)}</div>'))\n",
    "            return []\n",
    "\n",
    "    def process_directory(self, input_dir, output_dir):\n",
    "        \"\"\"Process directory of HTML files\"\"\"\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for file in Path(input_dir).glob('*.html'):\n",
    "            output_file = Path(output_dir) / f\"{file.stem}.md\"\n",
    "            self.process_html_file(str(file), str(output_file))\n",
    "\n",
    "    def _display_summary(self, input_file, output_file, num_elements):\n",
    "        \"\"\"Display processing summary\"\"\"\n",
    "        summary_html = f\"\"\"\n",
    "        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "            <p><strong>Processed file:</strong> {os.path.basename(input_file)}</p>\n",
    "            <p><strong>Output saved to:</strong> {os.path.basename(output_file)}</p>\n",
    "            <p><strong>Extracted elements:</strong> {num_elements}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(summary_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use of Custom Content Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ContentExtractor` is called by creating an instance of the class and then calling the `process_html_file` or `process_directory` method. The `process_html_file` method takes two arguments: the path to the input HTML file and the desired name of the output Markdown file. The `process_directory` method takes two arguments: the path to the input directory containing HTML files and the path to the output directory where the Markdown files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ContextExtractor()\n",
    "context = extractor.process_html_file(\n",
    "    input_file='PlanNet/site/index.html',\n",
    "    output_file='context'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, element in enumerate(context[:5]):  # Show first 5 elements\n",
    "    print(f\"\\nElement {i+1}:\")\n",
    "    print(element[:200] + \"...\" if len(element) > 200 else element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your HTML files\n",
    "input_dir = 'PlanNet/site'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all HTML files in the input directory\n",
    "for html_file in Path(input_dir).glob('**/*.html'):\n",
    "    # Create corresponding output path while preserving directory structure\n",
    "    relative_path = html_file.relative_to(input_dir)\n",
    "    output_path = Path(output_dir) / relative_path.with_suffix('.md')\n",
    "    \n",
    "    # Create necessary subdirectories\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process the file\n",
    "    context = extractor.process_html_file(\n",
    "        input_file=str(html_file),\n",
    "        output_file=str(output_path)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
