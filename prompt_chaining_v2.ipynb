{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script aims to develop a prompt chain structure to send large amounts of text/content to LLM APIs including through multiple calls\n",
    "\n",
    "#### Note: still in experimentation mode/progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON file organization:\n",
    "- Copies relevant JSON files from source to working directory\n",
    "- Groups related files based on their names\n",
    "- Creates organized folder structure\n",
    "- Excludes certain file types (like .ttl.json, .jsonld.json)\n",
    "\n",
    "JSON Consolidation:\n",
    "- Combines related JSON files into single consolidated files\n",
    "- Creates files like Organization_combined.json, StructureDefinition_combined.json, etc.\n",
    "\n",
    "JSON Processing:\n",
    "- Splits large JSON files into manageable chunks\n",
    "- Processes each chunk through Claude\n",
    "- Gets technical analysis of configurations and structures\n",
    "- Combines chunk analyses into coherent summaries\n",
    "\n",
    "Markdown processing:\n",
    "- Processes documentation files\n",
    "- Extracts key technical information\n",
    "- Creates summaries of documentation content\n",
    "\n",
    "Image processing:\n",
    "- Processes technical diagrams and figures\n",
    "- Extracts architectural and design information\n",
    "- Creates descriptions of visual technical content\n",
    "\n",
    "Meta-analysis creation:\n",
    "- Combines all processed information\n",
    "- Creates comprehensive technical analysis covering:\n",
    "    - Technical requirements and architecture\n",
    "    - Implementation details\n",
    "    - Visual documentation analysis\n",
    "    - Cross-reference analysis\n",
    "    - Patterns and potential issues\n",
    "\n",
    "Output generation:\n",
    "- Creates output directory\n",
    "- Saves final analysis as markdown file\n",
    "- Includes comprehensive technical documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import base64\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import os\n",
    "#import google.generativeai as gemini\n",
    "#from openai import OpenAI\n",
    "import io, threading, time, re\n",
    "import pandas as pd\n",
    "from json_repair import repair_json\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in API keys for Claude, Gemini, and GPT from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "claude_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "#gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "#OpenAI.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = Anthropic(api_key = claude_api_key)\n",
    "claude_version = \"claude-3-5-sonnet-20240620\"  # \"claude-3-opus-20240229\"   \"claude-3-5-sonnet-20240620\" \"claude-3-sonnet-20240229\" \"claude-3-haiku-20240307\"\n",
    "claude_max_output_tokens = 8192  # claude 3 opus is only 4096 tokens, sonnet is 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CERT_PATH = '/Users/amathur/ca-certificates.crt'\n",
    "CERT_PATH = '/opt/homebrew/etc/openssl@3/cert.pem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anthropic_client():\n",
    "    \"\"\"Create Anthropic client with proper certificate verification\"\"\"\n",
    "    verify_path = CERT_PATH if os.path.exists(CERT_PATH) else True\n",
    "    http_client = httpx.Client(\n",
    "        verify=verify_path,\n",
    "        timeout=30.0\n",
    "    )\n",
    "    return Anthropic(\n",
    "        api_key=claude_api_key,\n",
    "        http_client=http_client\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling in files of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'full-ig/site'\n",
    "destination_folder = 'full-ig/json_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_json_files():\n",
    "    \"\"\"\n",
    "    Copy JSON files from full-ig/site to full-ig/json_only directory,\n",
    "    excluding compound extensions and creating the directory if needed\n",
    "    \"\"\"\n",
    "    source_folder = 'full-ig/site'\n",
    "    destination_folder = 'full-ig/json_only'\n",
    "\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    json_files = []\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        # Check if the file ends with .json but not with compound extensions\n",
    "        if file_name.endswith('.json') and not (file_name.endswith('.ttl.json') or \n",
    "                                             file_name.endswith('.jsonld.json') or \n",
    "                                             file_name.endswith('.xml.json') or \n",
    "                                             file_name.endswith('.change.history.json')):\n",
    "            json_files.append(file_name)\n",
    "            # Copy the file to the destination folder\n",
    "            shutil.copy(os.path.join(source_folder, file_name), destination_folder)\n",
    "            \n",
    "    logging.info(f\"Copied {len(json_files)} JSON files to {destination_folder}\")\n",
    "    return json_files\n",
    "\n",
    "def group_files_by_base_name(directory_path, delimiter='-'):\n",
    "    \"\"\"\n",
    "    Group files in the directory by their base name (portion before a delimiter).\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    delimiter (str): The delimiter to split the file name on (default is '-').\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are base names and values are lists of files that share the same base name.\n",
    "    \"\"\"\n",
    "    grouped_files = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):  # Only process .json files\n",
    "            if delimiter in filename:  # Only consider files with the delimiter\n",
    "                # Get the base name (before the first delimiter)\n",
    "                base_name = filename.split(delimiter)[0]\n",
    "                \n",
    "                # Append the file to the group corresponding to its base name\n",
    "                grouped_files[base_name].append(filename)\n",
    "    \n",
    "    return grouped_files\n",
    "\n",
    "def copy_files_to_folders(directory_path, grouped_files):\n",
    "    \"\"\"\n",
    "    Copy files to folders if the base name group has more than 1 file,\n",
    "    and remove them from the original directory.\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    grouped_files (dict): Dictionary of grouped files by base name.\n",
    "    \"\"\"\n",
    "    for base_name, files in grouped_files.items():\n",
    "        if len(files) >= 1:  # Process groups with one or more files\n",
    "            # Create a folder for the base name in the same directory\n",
    "            base_folder = os.path.join(directory_path, base_name)\n",
    "            if not os.path.exists(base_folder):\n",
    "                os.makedirs(base_folder)  # Create the folder if it doesn't exist\n",
    "            logging.info(f\"Created folder: {base_folder}\")\n",
    "            \n",
    "            # Copy each file in the group to the new folder\n",
    "            for file in files:\n",
    "                source_file = os.path.join(directory_path, file)\n",
    "                destination_file = os.path.join(base_folder, file)\n",
    "                shutil.copy(source_file, destination_file)  # Copy the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing files for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_json(json_data, max_size=2000):\n",
    "    \"\"\"\n",
    "    Split JSON array into chunks while maintaining complete JSON objects\n",
    "    Returns list of chunks, where each chunk contains complete JSON objects\n",
    "    \"\"\"\n",
    "    if isinstance(json_data, dict):\n",
    "        json_data = [json_data]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for item in json_data:\n",
    "        item_size = len(json.dumps(item))\n",
    "        \n",
    "        # Handle large individual items\n",
    "        if item_size > max_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "            chunks.append([item])\n",
    "            continue\n",
    "        \n",
    "        # Start new chunk if current would exceed max_size\n",
    "        if current_size + item_size > max_size and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        \n",
    "        current_chunk.append(item)\n",
    "        current_size += item_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def prepare_json_for_processing(json_file_path):\n",
    "    \"\"\"Read and prepare JSON file for processing\"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    if isinstance(data, dict) and 'entry' in data:\n",
    "        return data['entry']\n",
    "    return data\n",
    "\n",
    "def create_json_summary_prompt(chunk, chunk_num, total_chunks):\n",
    "    \"\"\"Create prompt for summarizing JSON chunk\"\"\"\n",
    "    return f\"\"\"Analyze this portion ({chunk_num} of {total_chunks}) of a FHIR Implementation Guide JSON resource bundle.\n",
    "    Focus on key technical details, requirements, and relationships.\n",
    "    \n",
    "    JSON Content:\n",
    "    {json.dumps(chunk, indent=2)}\n",
    "    \n",
    "    Please provide:\n",
    "    1. Resource Types and Profiles present\n",
    "    2. Key technical requirements and constraints\n",
    "    3. Dependencies and relationships between resources\n",
    "    4. Notable patterns or unique configurations\n",
    "    \n",
    "    Focus on new information not covered in previous chunks.\"\"\"\n",
    "\n",
    "def process_json_file(client, json_file_path, model_name=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Process a JSON file while maintaining object integrity\"\"\"\n",
    "    # Load and prepare JSON data\n",
    "    json_data = prepare_json_for_processing(json_file_path)\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = split_json(json_data)\n",
    "    \n",
    "    # Process each chunk\n",
    "    chunk_summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        prompt = create_json_summary_prompt(chunk, i+1, len(chunks))\n",
    "        response = client.messages.create(\n",
    "            model=model_name,\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": \"Here is the technical summary: <summary>\"}\n",
    "            ],\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        chunk_summaries.append(response.content[0].text)\n",
    "    \n",
    "    # Combine summaries\n",
    "    return combine_summaries(chunk_summaries)\n",
    "\n",
    "\n",
    "def combine_summaries(summaries):\n",
    "    \"\"\"Combine chunk summaries into a cohesive analysis\"\"\"\n",
    "    prompt = f\"\"\"Synthesize these related summaries into a unified technical analysis:\n",
    "\n",
    "    {json.dumps(summaries, indent=2)}\n",
    "    \n",
    "    Create a comprehensive analysis that:\n",
    "    1. Eliminates redundant information\n",
    "    2. Maintains technical accuracy\n",
    "    3. Preserves important relationships\n",
    "    4. Highlights key patterns\n",
    "    5. Notes any conflicts or inconsistencies\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=2000,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": \"Here is the combined analysis: <summary>\"}\n",
    "        ],\n",
    "        stop_sequences=[\"</summary>\"]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text\n",
    "\n",
    "def consolidate_jsons(base_directory='full-ig/json_only'):\n",
    "    \"\"\"Consolidate related JSON files while maintaining object integrity\"\"\"\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "              if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        folder_path = os.path.join(base_directory, subdir)\n",
    "        combined_data = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        json_content = json.load(f)\n",
    "                        if isinstance(json_content, dict) and 'entry' in json_content:\n",
    "                            combined_data.extend(json_content['entry'])\n",
    "                        else:\n",
    "                            combined_data.append(json_content)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.error(f\"Error decoding JSON from {filename}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if combined_data:\n",
    "            output_filename = f\"{subdir}_combined.json\"\n",
    "            output_path = os.path.join(base_directory, output_filename)\n",
    "            \n",
    "            try:\n",
    "                with open(output_path, 'w') as outfile:\n",
    "                    json.dump({\n",
    "                        \"resourceType\": subdir,\n",
    "                        \"total\": len(combined_data),\n",
    "                        \"entry\": combined_data\n",
    "                    }, outfile, indent=2)\n",
    "                logging.info(f\"Created {output_filename} with {len(combined_data)} entries\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error writing {output_filename}: {e}\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Convert image to base64 encoding for API consumption\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def process_image(client, image_path):\n",
    "    \"\"\"Process a single image and generate a technical description\"\"\"\n",
    "    try:\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/\" + image_path.split('.')[-1],\n",
    "                            \"data\": base64_image\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Analyze this technical diagram/figure. Focus on:\\n1. Key components and their relationships\\n2. Technical workflows or processes shown\\n3. Architecture or design patterns illustrated\\n4. Important technical details or annotations\\nProvide a detailed technical description.\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Here is the technical analysis of the image: <summary>\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20240620\",\n",
    "            max_tokens=1000,\n",
    "            messages=messages,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_all_content(client, base_directory='full-ig'):\n",
    "    \"\"\"Process all content types (JSON, Markdown, and Images) and create summaries\"\"\"\n",
    "    # First consolidate JSONs\n",
    "    consolidate_jsons(os.path.join(base_directory, 'json_only'))\n",
    "    \n",
    "    # Process consolidated JSONs\n",
    "    json_summaries = {}\n",
    "    json_dir = os.path.join(base_directory, 'json_only')\n",
    "    for filename in os.listdir(json_dir):\n",
    "        if filename.endswith('_combined.json'):\n",
    "            file_path = os.path.join(json_dir, filename)\n",
    "            json_summaries[filename] = process_json_file(client, file_path)\n",
    "    \n",
    "    # Process markdown files\n",
    "    markdown_summaries = {}\n",
    "    markdown_dir = os.path.join(base_directory, 'markdown')\n",
    "    for filename in os.listdir(markdown_dir):\n",
    "        if filename.endswith('.md'):\n",
    "            with open(os.path.join(markdown_dir, filename)) as f:\n",
    "                content = clean_markdown(f.read())\n",
    "                markdown_summaries[filename] = summarize_markdown(client, content)\n",
    "    \n",
    "    # Process images\n",
    "    image_summaries = {}\n",
    "    image_dir = os.path.join(base_directory, 'site/Figures')\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            file_path = os.path.join(image_dir, filename)\n",
    "            image_summaries[filename] = process_image(client, file_path)\n",
    "    \n",
    "    # Create final meta-summary\n",
    "    return create_meta_summary(client, json_summaries, markdown_summaries, image_summaries)\n",
    "\n",
    "def create_meta_summary(client, json_summaries, markdown_summaries, image_summaries):\n",
    "    \"\"\"Create a comprehensive meta-summary incorporating all content types\"\"\"\n",
    "    prompt = f\"\"\"Synthesize information from multiple content types into a comprehensive technical analysis:\n",
    "\n",
    "    JSON Configuration Summaries:\n",
    "    {json.dumps(json_summaries, indent=2)}\n",
    "\n",
    "    Documentation Summaries:\n",
    "    {json.dumps(markdown_summaries, indent=2)}\n",
    "\n",
    "    Diagram/Figure Analyses:\n",
    "    {json.dumps(image_summaries, indent=2)}\n",
    "\n",
    "    Create a comprehensive technical analysis that:\n",
    "    1. Technical Requirements and Architecture\n",
    "       - Core technical requirements\n",
    "       - System architecture and patterns\n",
    "       - Integration points and interfaces\n",
    "       \n",
    "    2. Implementation Details\n",
    "       - Key configurations and settings\n",
    "       - Resource profiles and extensions\n",
    "       - Validation rules and constraints\n",
    "       \n",
    "    3. Visual Documentation Analysis\n",
    "       - Technical workflows and processes\n",
    "       - Component relationships\n",
    "       - Architecture diagrams insights\n",
    "       \n",
    "    4. Cross-Reference Analysis\n",
    "       - Relationships between documentation, config, and diagrams\n",
    "       - Consistency validation\n",
    "       - Dependencies and prerequisites\n",
    "       \n",
    "    Highlight any important patterns, potential issues, or implementation considerations.\"\"\"\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240307\",\n",
    "        max_tokens=4000,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": \"Here is the comprehensive technical analysis: <summary>\"}\n",
    "        ],\n",
    "        stop_sequences=[\"</summary>\"]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text\n",
    "\n",
    "def save_processed_content(base_directory='full-ig', output_directory='processed_output'):\n",
    "    \"\"\"Save all processed content and summaries in an organized structure\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories for different content types\n",
    "    summaries_dir = os.path.join(output_directory, 'summaries')\n",
    "    os.makedirs(summaries_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize client\n",
    "    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "    \n",
    "    # Process all content\n",
    "    final_summary = process_all_content(client, base_directory)\n",
    "    \n",
    "    # Save final meta-summary\n",
    "    with open(os.path.join(output_directory, 'final_technical_analysis.md'), 'w') as f:\n",
    "        f.write(final_summary)\n",
    "    \n",
    "    # Log completion\n",
    "    logging.info(f\"Processing complete. Results saved to {output_directory}\")\n",
    "    \n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Copied 166 JSON files to full-ig/json_only\n",
      "INFO:root:Created folder: full-ig/json_only/Location\n",
      "INFO:root:Created folder: full-ig/json_only/StructureDefinition\n",
      "INFO:root:Created folder: full-ig/json_only/ValueSet\n",
      "INFO:root:Created folder: full-ig/json_only/CodeSystem\n",
      "INFO:root:Created folder: full-ig/json_only/OrganizationAffiliation\n",
      "INFO:root:Created folder: full-ig/json_only/SearchParameter\n",
      "INFO:root:Created folder: full-ig/json_only/HealthcareService\n",
      "INFO:root:Created folder: full-ig/json_only/usage\n",
      "INFO:root:Created folder: full-ig/json_only/Organization\n",
      "INFO:root:Created folder: full-ig/json_only/CapabilityStatement\n",
      "INFO:root:Created folder: full-ig/json_only/PractitionerRole\n",
      "INFO:root:Created folder: full-ig/json_only/ImplementationGuide\n",
      "INFO:root:Created folder: full-ig/json_only/InsurancePlan\n",
      "INFO:root:Created folder: full-ig/json_only/Practitioner\n",
      "INFO:root:Created folder: full-ig/json_only/Endpoint\n",
      "INFO:root:Created folder: full-ig/json_only/plan\n",
      "INFO:root:Created Organization_combined.json with 11 entries\n",
      "INFO:root:Created StructureDefinition_combined.json with 21 entries\n",
      "INFO:root:Created CapabilityStatement_combined.json with 1 entries\n",
      "INFO:root:Created Practitioner_combined.json with 3 entries\n",
      "ERROR:root:Error decoding JSON from plan-net.openapi.json: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)\n",
      "INFO:root:Created Location_combined.json with 9 entries\n",
      "INFO:root:Created CodeSystem_combined.json with 14 entries\n",
      "INFO:root:Created usage_combined.json with 1 entries\n",
      "INFO:root:Created ValueSet_combined.json with 24 entries\n",
      "INFO:root:Created HealthcareService_combined.json with 10 entries\n",
      "INFO:root:Created SearchParameter_combined.json with 51 entries\n",
      "INFO:root:Created Endpoint_combined.json with 1 entries\n",
      "INFO:root:Created InsurancePlan_combined.json with 2 entries\n",
      "INFO:root:Created ImplementationGuide_combined.json with 1 entries\n",
      "INFO:root:Created PractitionerRole_combined.json with 6 entries\n",
      "INFO:root:Created OrganizationAffiliation_combined.json with 7 entries\n",
      "INFO:root:Created Organization_combined.json with 11 entries\n",
      "INFO:root:Created StructureDefinition_combined.json with 21 entries\n",
      "INFO:root:Created CapabilityStatement_combined.json with 1 entries\n",
      "INFO:root:Created Practitioner_combined.json with 3 entries\n",
      "ERROR:root:Error decoding JSON from plan-net.openapi.json: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)\n",
      "INFO:root:Created Location_combined.json with 9 entries\n",
      "INFO:root:Created CodeSystem_combined.json with 14 entries\n",
      "INFO:root:Created usage_combined.json with 1 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 166 JSON files\n",
      "Files grouped by base name\n",
      "Files organized into folders\n",
      "JSONs consolidated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created ValueSet_combined.json with 24 entries\n",
      "INFO:root:Created HealthcareService_combined.json with 10 entries\n",
      "INFO:root:Created SearchParameter_combined.json with 51 entries\n",
      "INFO:root:Created Endpoint_combined.json with 1 entries\n",
      "INFO:root:Created InsurancePlan_combined.json with 2 entries\n",
      "INFO:root:Created ImplementationGuide_combined.json with 1 entries\n",
      "INFO:root:Created PractitionerRole_combined.json with 6 entries\n",
      "INFO:root:Created OrganizationAffiliation_combined.json with 7 entries\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 11.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.439963 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.936191 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSONs consolidated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 7. Process all content types and create final analysis\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m final_analysis \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull-ig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 8. Save the results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_output\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[17], line 212\u001b[0m, in \u001b[0;36mprocess_all_content\u001b[0;34m(client, base_directory)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_combined.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    211\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(json_dir, filename)\n\u001b[0;32m--> 212\u001b[0m         json_summaries[filename] \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Process markdown files\u001b[39;00m\n\u001b[1;32m    215\u001b[0m markdown_summaries \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[17], line 76\u001b[0m, in \u001b[0;36mprocess_json_file\u001b[0;34m(client, json_file_path, model_name)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m     75\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m create_json_summary_prompt(chunk, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chunks))\n\u001b[0;32m---> 76\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHere is the technical summary: <summary>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m</summary>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     chunk_summaries\u001b[38;5;241m.\u001b[39mappend(response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Combine summaries\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/resources/messages.py:888\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    882\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    884\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    885\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[0;32m--> 888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}"
     ]
    }
   ],
   "source": [
    "# 1. First set up logging and import required packages\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "client = create_anthropic_client()\n",
    "\n",
    "# 3. Copy JSON files from site to json_only folder\n",
    "copied_files = copy_json_files()\n",
    "print(f\"Copied {len(copied_files)} JSON files\")\n",
    "\n",
    "# 4. Group related files\n",
    "grouped_files = group_files_by_base_name('full-ig/json_only')\n",
    "print(\"Files grouped by base name\")\n",
    "\n",
    "# 5. Create folders and organize files\n",
    "copy_files_to_folders('full-ig/json_only', grouped_files)\n",
    "print(\"Files organized into folders\")\n",
    "\n",
    "# 6. Consolidate related JSONs\n",
    "consolidate_jsons('full-ig/json_only')\n",
    "print(\"JSONs consolidated\")\n",
    "\n",
    "# 7. Process all content types and create final analysis\n",
    "final_analysis = process_all_content(client, base_directory='full-ig')\n",
    "\n",
    "# 8. Save the results\n",
    "output_dir = 'processed_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'technical_analysis.md'), 'w') as f:\n",
    "    f.write(final_analysis)\n",
    "\n",
    "print(f\"Analysis complete and saved to {output_dir}/technical_analysis.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
