{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Summarization of IG Documents\n",
    "This script aims to develop a prompt chain structure to send large amounts of text/content to LLM APIs through multiple calls. \n",
    "\n",
    "The current approach takes in all JSON files from the Plan Net IG, all figure diagrams, and key narrative information in markdown form (formerly extracted from HTML files). The script then summarizes each type of information in batches, and creates a meta-summarization of all documents to outline the technical information it can glean from all submitted documentation. The goal is to identify if this approach can produce all technical information at an appropriate level of deatil that an LLM would need to know to help design a test kit for a given IG. Best practices from the Claude API were used.\n",
    "\n",
    "Current status: We were able to run through the script fully one time using the Claude API with all JSONs, markdown content, and images. The process took over 93 minutes. The summarization is saved in the file final_technical_analysis.md and is pasted at the end of this script. We can see that the first iteration did not produce detailed enough information about requirements, etc. \n",
    "\n",
    "Need to do: Further work is needed to edit the prompts to require that that level of detail is kept in the outputs of each summarization step. In addition, this process will have to be configured for the Gemini and GPT APIs as well for comparison. We will also still need to revise this work to allow for additional prompting for confirming understanding of an IG, for test kit development, and for documents to be included, such as a 'golden rules' document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of Notebook:\n",
    "JSON file organization:\n",
    "- Copies relevant JSON files from source to working directory\n",
    "- Groups related files based on their names\n",
    "- Creates organized folder structure\n",
    "- Excludes certain file types (like .ttl.json, .jsonld.json)\n",
    "\n",
    "JSON Consolidation:\n",
    "- Combines related JSON files into single consolidated files\n",
    "- Creates files like Organization_combined.json, StructureDefinition_combined.json, etc.\n",
    "\n",
    "JSON Processing:\n",
    "- Splits large JSON files into manageable chunks\n",
    "- Processes each chunk through Claude\n",
    "- Combines chunk analyses into coherent summaries\n",
    "\n",
    "Markdown processing:\n",
    "- Processes documentation files\n",
    "- Extracts key technical information\n",
    "- Creates summaries of documentation content\n",
    "\n",
    "Image processing:\n",
    "- Processes technical diagrams and figures\n",
    "- Creates descriptions of visual technical content\n",
    "\n",
    "Meta-analysis creation:\n",
    "- Combines all processed information\n",
    "- Creates comprehensive technical analysis covering:\n",
    "    - Technical requirements and architecture\n",
    "    - Implementation details\n",
    "    - Visual documentation analysis\n",
    "\n",
    "Output generation:\n",
    "- Creates output directory\n",
    "- Saves final analysis as markdown file\n",
    "- Includes comprehensive technical documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import base64\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import os\n",
    "#import google.generativeai as gemini\n",
    "#from openai import OpenAI\n",
    "import io, threading, time, re\n",
    "import pandas as pd\n",
    "from json_repair import repair_json\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from anthropic import RateLimitError\n",
    "from anthropic import Anthropic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in API keys for Claude, Gemini, and GPT from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "claude_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "#gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "#OpenAI.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Claude API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = Anthropic(api_key = claude_api_key)\n",
    "claude_version = \"claude-3-5-sonnet-20240620\"  # \"claude-3-opus-20240229\"   \"claude-3-5-sonnet-20240620\" \"claude-3-sonnet-20240229\" \"claude-3-haiku-20240307\"\n",
    "claude_max_output_tokens = 8192  # claude 3 opus is only 4096 tokens, sonnet is 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CERT_PATH = '/Users/amathur/ca-certificates.crt'\n",
    "CERT_PATH = '/opt/homebrew/etc/openssl@3/cert.pem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anthropic_client():\n",
    "    \"\"\"Create Anthropic client with proper certificate verification\"\"\"\n",
    "    verify_path = CERT_PATH if os.path.exists(CERT_PATH) else True\n",
    "    http_client = httpx.Client(\n",
    "        verify=verify_path,\n",
    "        timeout=30.0\n",
    "    )\n",
    "    return Anthropic(\n",
    "        api_key=claude_api_key,\n",
    "        http_client=http_client\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling in files of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'full-ig/site'\n",
    "destination_folder = 'full-ig/json_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_json_files():\n",
    "    \"\"\"\n",
    "    Copy JSON files from full-ig/site to full-ig/json_only directory,\n",
    "    excluding compound extensions and creating the directory if needed\n",
    "    \"\"\"\n",
    "    source_folder = 'full-ig/site'\n",
    "    destination_folder = 'full-ig/json_only'\n",
    "\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    json_files = []\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        # Check if the file ends with .json but not with compound extensions\n",
    "        if file_name.endswith('.json') and not (file_name.endswith('.ttl.json') or \n",
    "                                             file_name.endswith('.jsonld.json') or \n",
    "                                             file_name.endswith('.xml.json') or \n",
    "                                             file_name.endswith('.change.history.json')):\n",
    "            json_files.append(file_name)\n",
    "            # Copy the file to the destination folder\n",
    "            shutil.copy(os.path.join(source_folder, file_name), destination_folder)\n",
    "            \n",
    "    logging.info(f\"Copied {len(json_files)} JSON files to {destination_folder}\")\n",
    "    return json_files\n",
    "\n",
    "def group_files_by_base_name(directory_path, delimiter='-'):\n",
    "    \"\"\"\n",
    "    Group files in the directory by their base name (portion before a delimiter).\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    delimiter (str): The delimiter to split the file name on (default is '-').\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are base names and values are lists of files that share the same base name.\n",
    "    \"\"\"\n",
    "    grouped_files = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):  # Only process .json files\n",
    "            if delimiter in filename:  # Only consider files with the delimiter\n",
    "                # Get the base name (before the first delimiter)\n",
    "                base_name = filename.split(delimiter)[0]\n",
    "                \n",
    "                # Append the file to the group corresponding to its base name\n",
    "                grouped_files[base_name].append(filename)\n",
    "    \n",
    "    return grouped_files\n",
    "\n",
    "def copy_files_to_folders(directory_path, grouped_files):\n",
    "    \"\"\"\n",
    "    Copy files to folders if the base name group has more than 1 file,\n",
    "    and remove them from the original directory.\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    grouped_files (dict): Dictionary of grouped files by base name.\n",
    "    \"\"\"\n",
    "    for base_name, files in grouped_files.items():\n",
    "        if len(files) >= 1:  # Process groups with one or more files\n",
    "            # Create a folder for the base name in the same directory\n",
    "            base_folder = os.path.join(directory_path, base_name)\n",
    "            if not os.path.exists(base_folder):\n",
    "                os.makedirs(base_folder)  # Create the folder if it doesn't exist\n",
    "            logging.info(f\"Created folder: {base_folder}\")\n",
    "            \n",
    "            # Copy each file in the group to the new folder\n",
    "            for file in files:\n",
    "                source_file = os.path.join(directory_path, file)\n",
    "                destination_file = os.path.join(base_folder, file)\n",
    "                shutil.copy(source_file, destination_file)  # Copy the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing JSON Files for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_json(json_data, max_size=2000):\n",
    "    \"\"\"\n",
    "    Split JSON array into chunks while maintaining complete JSON objects\n",
    "    Returns list of chunks, where each chunk contains complete JSON objects\n",
    "    \"\"\"\n",
    "    if isinstance(json_data, dict):\n",
    "        json_data = [json_data]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for item in json_data:\n",
    "        item_size = len(json.dumps(item))\n",
    "        \n",
    "        # Handle large individual items\n",
    "        if item_size > max_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "            chunks.append([item])\n",
    "            continue\n",
    "        \n",
    "        # Start new chunk if current would exceed max_size\n",
    "        if current_size + item_size > max_size and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        \n",
    "        current_chunk.append(item)\n",
    "        current_size += item_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def prepare_json_for_processing(json_file_path):\n",
    "    \"\"\"Read and prepare JSON file for processing\"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    if isinstance(data, dict) and 'entry' in data:\n",
    "        return data['entry']\n",
    "    return data\n",
    "\n",
    "def create_json_summary_prompt(chunk, chunk_num, total_chunks):\n",
    "    \"\"\"Create prompt for summarizing JSON chunk\"\"\"\n",
    "    return f\"\"\"Analyze this portion ({chunk_num} of {total_chunks}) of a FHIR Implementation Guide JSON resource bundle.\n",
    "    Focus on key technical details, requirements, and relationships.\n",
    "    \n",
    "    JSON Content:\n",
    "    {json.dumps(chunk, indent=2)}\n",
    "    \n",
    "    Please provide:\n",
    "    1. Resource Types and Profiles present\n",
    "    2. Key technical requirements and constraints\n",
    "    3. Dependencies and relationships between resources\n",
    "    4. Notable patterns or unique configurations\n",
    "    \n",
    "    Focus on new information not covered in previous chunks.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def consolidate_jsons(base_directory='full-ig/json_only'):\n",
    "    \"\"\"Consolidate related JSON files while maintaining object integrity\"\"\"\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "              if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        folder_path = os.path.join(base_directory, subdir)\n",
    "        combined_data = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        json_content = json.load(f)\n",
    "                        if isinstance(json_content, dict) and 'entry' in json_content:\n",
    "                            combined_data.extend(json_content['entry'])\n",
    "                        else:\n",
    "                            combined_data.append(json_content)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.error(f\"Error decoding JSON from {filename}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if combined_data:\n",
    "            output_filename = f\"{subdir}_combined.json\"\n",
    "            output_path = os.path.join(base_directory, output_filename)\n",
    "            \n",
    "            try:\n",
    "                with open(output_path, 'w') as outfile:\n",
    "                    json.dump({\n",
    "                        \"resourceType\": subdir,\n",
    "                        \"total\": len(combined_data),\n",
    "                        \"entry\": combined_data\n",
    "                    }, outfile, indent=2)\n",
    "                logging.info(f\"Created {output_filename} with {len(combined_data)} entries\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error writing {output_filename}: {e}\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Convert image to base64 encoding for API consumption\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Rate Limiting & Safe Call Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rate_limiter(max_requests_per_minute=50):\n",
    "    \"\"\"Create a simple rate limiter\"\"\"\n",
    "    class RateLimiter:\n",
    "        def __init__(self):\n",
    "            self.requests = []\n",
    "            self.max_requests = max_requests_per_minute\n",
    "            self.time_window = 60  # seconds\n",
    "\n",
    "        def wait_if_needed(self):\n",
    "            now = time.time()\n",
    "            # Remove requests older than our time window\n",
    "            self.requests = [req_time for req_time in self.requests \n",
    "                           if now - req_time < self.time_window]\n",
    "            \n",
    "            if len(self.requests) >= self.max_requests:\n",
    "                # Wait until oldest request expires\n",
    "                sleep_time = self.time_window - (now - self.requests[0])\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "                self.requests = self.requests[1:]\n",
    "            \n",
    "            self.requests.append(now)\n",
    "            \n",
    "    return RateLimiter()\n",
    "\n",
    "# Create a global rate limiter\n",
    "rate_limiter = create_rate_limiter(max_requests_per_minute=25)  # More conservative limit\n",
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def safe_claude_request(client, model_name, messages, max_tokens=8192, stop_sequences=None):\n",
    "    \"\"\"Make a rate-limited request to Claude with retries\"\"\"\n",
    "    rate_limiter.wait_if_needed()\n",
    "    try:\n",
    "        return client.messages.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            stop_sequences=stop_sequences\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in Claude request: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_markdown(client, content, model_name=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Process markdown content and generate a technical summary\"\"\"\n",
    "    try:\n",
    "        # Create a prompt for markdown analysis\n",
    "        prompt = f\"\"\"Analyze this technical documentation markdown content:\n",
    "\n",
    "        {content}\n",
    "\n",
    "        Please provide:\n",
    "        1. Key technical concepts and definitions\n",
    "        2. Important requirements and specifications\n",
    "        3. Technical workflows or processes described\n",
    "        4. Any dependencies or prerequisites mentioned\n",
    "        5. Notable implementation details or guidelines\n",
    "\n",
    "        Focus on extracting the most important technical information.\"\"\"\n",
    "\n",
    "        response = safe_claude_request(\n",
    "            client,\n",
    "            model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": \"Here is the technical summary: <summary>\"}\n",
    "            ],\n",
    "            max_tokens=8192,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing markdown content: {str(e)}\")\n",
    "        return \"Error processing markdown content: \" + str(e)\n",
    "    \n",
    "def clean_markdown(text):\n",
    "    \"\"\"Clean markdown content by removing unnecessary whitespace and formatting\"\"\"\n",
    "    # Remove multiple newlines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove excessive punctuation \n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    \n",
    "    # Remove escaped characters\n",
    "    text = re.sub(r'\\\\(.)', r'\\1', text)\n",
    "    \n",
    "    # Remove table formatting but keep content\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'[-\\s]*\\n[-\\s]*', '\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def combine_summaries(client, summaries, model_name=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Combine chunk summaries into a cohesive analysis\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Synthesize these related summaries into a unified technical analysis:\n",
    "\n",
    "        {json.dumps(summaries, indent=2)}\n",
    "        \n",
    "        Create a comprehensive analysis that:\n",
    "        1. Eliminates redundant information\n",
    "        2. Maintains technical accuracy\n",
    "        3. Includes specific technical information about search parameters for resource types, resource profiles, Must Supports and other requirements\n",
    "        4. Preserves important relationships, such as specific process flows outlined in image diagrams\"\"\"\n",
    "        \n",
    "        response = safe_claude_request(\n",
    "            client,\n",
    "            model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": \"Here is the combined analysis: <summary>\"}\n",
    "            ],\n",
    "            max_tokens=8192,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error combining summaries: {str(e)}\")\n",
    "        return \"Unable to combine summaries due to error\"\n",
    "    \n",
    "def process_json_file(client, json_file_path, model_name=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Process a JSON file while maintaining object integrity\"\"\"\n",
    "    try:\n",
    "        json_data = prepare_json_for_processing(json_file_path)\n",
    "        chunks = split_json(json_data)\n",
    "        chunk_summaries = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prompt = create_json_summary_prompt(chunk, i+1, len(chunks))\n",
    "            try:\n",
    "                response = safe_claude_request(\n",
    "                    client,\n",
    "                    model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                        {\"role\": \"assistant\", \"content\": \"Here is the technical summary: <summary>\"}\n",
    "                    ],\n",
    "                    stop_sequences=[\"</summary>\"]\n",
    "                )\n",
    "                chunk_summaries.append(response.content[0].text)\n",
    "                # Add small delay between chunks\n",
    "                time.sleep(2)  # Added delay between chunks\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing chunk {i+1} of {len(chunks)} for {json_file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not chunk_summaries:\n",
    "            return \"Unable to process file due to errors\"\n",
    "        \n",
    "        return combine_summaries(client, chunk_summaries, model_name)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {json_file_path}: {str(e)}\")\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "\n",
    "\n",
    "def process_image(client, image_path, model_name=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Process a single image and generate a technical description\"\"\"\n",
    "    try:\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/\" + image_path.split('.')[-1],\n",
    "                            \"data\": base64_image\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Analyze this technical diagram/figure. Focus on:\\n1. Key components and their relationships\\n2. Technical workflows or processes shown\\n3. Architecture or design patterns illustrated\\n4. Important technical details or annotations\\nProvide a detailed technical description.\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Here is the technical analysis of the image: <summary>\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = safe_claude_request(\n",
    "            client,\n",
    "            model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=8192,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_content(client, file_path, content_type, model_name=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Generic content processor that handles any content type\"\"\"\n",
    "    try:\n",
    "        if content_type == \"json\":\n",
    "            json_data = prepare_json_for_processing(file_path)\n",
    "            chunks = split_json(json_data)\n",
    "            summaries = []\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                prompt = create_json_summary_prompt(chunk, i+1, len(chunks))\n",
    "                response = safe_claude_request(\n",
    "                    client,\n",
    "                    model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                        {\"role\": \"assistant\", \"content\": \"Here is the technical summary: <summary>\"}\n",
    "                    ],\n",
    "                    stop_sequences=[\"</summary>\"]\n",
    "                )\n",
    "                summaries.append(response.content[0].text)\n",
    "                time.sleep(2)\n",
    "            \n",
    "            return combine_summaries(client, summaries) if summaries else \"No content processed\"\n",
    "            \n",
    "        elif content_type == \"markdown\":\n",
    "            with open(file_path) as f:\n",
    "                content = clean_markdown(f.read())\n",
    "            return summarize_markdown(client, content)\n",
    "            \n",
    "        elif content_type == \"image\":\n",
    "            return process_image(client, file_path)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {content_type} file {file_path}: {str(e)}\")\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "    \n",
    "\n",
    "def process_batch(client, files, content_type, batch_size=3):\n",
    "    \"\"\"Process any type of files in batches\"\"\"\n",
    "    results = {}\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i:i + batch_size]\n",
    "        for file in batch:\n",
    "            try:\n",
    "                results[file] = process_content(client, file, content_type)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file}: {str(e)}\")\n",
    "                results[file] = f\"Error: {str(e)}\"\n",
    "        time.sleep(10)  # Delay between batches\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def process_all_content(client, base_directory='full-ig'):\n",
    "    \"\"\"Process all content types using unified batch processing\"\"\"\n",
    "    try:\n",
    "        # Process JSONs\n",
    "        json_files = [\n",
    "            os.path.join(base_directory, 'json_only', f) \n",
    "            for f in os.listdir(os.path.join(base_directory, 'json_only')) \n",
    "            if f.endswith('_combined.json')\n",
    "        ]\n",
    "        json_summaries = process_batch(client, json_files, \"json\", batch_size=3)\n",
    "        \n",
    "        # Process markdown files\n",
    "        markdown_summaries = {}\n",
    "        markdown_dir = os.path.join(base_directory, 'markdown')\n",
    "        if os.path.exists(markdown_dir):\n",
    "            md_files = [\n",
    "                os.path.join(markdown_dir, f) \n",
    "                for f in os.listdir(markdown_dir) \n",
    "                if f.endswith('.md')\n",
    "            ]\n",
    "            markdown_summaries = process_batch(client, md_files, \"markdown\", batch_size=3)\n",
    "        \n",
    "        # Process images\n",
    "        image_summaries = {}\n",
    "        image_dir = os.path.join(base_directory, 'site/Figures')\n",
    "        if os.path.exists(image_dir):\n",
    "            img_files = [\n",
    "                os.path.join(image_dir, f) \n",
    "                for f in os.listdir(image_dir) \n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            ]\n",
    "            image_summaries = process_batch(client, img_files, \"image\", batch_size=2)\n",
    "        \n",
    "        time.sleep(5)  # Delay before meta-summary\n",
    "        \n",
    "        return create_meta_summary(\n",
    "            client, \n",
    "            json_summaries, \n",
    "            markdown_summaries, \n",
    "            image_summaries\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing content: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_meta_summary(client, json_summaries, markdown_summaries, image_summaries, model_name=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Create a comprehensive meta-summary incorporating all content types\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Synthesize information from multiple content types into a comprehensive technical analysis, while maintaining high level of detail:\n",
    "\n",
    "        JSON Configuration Summaries:\n",
    "        {json.dumps(json_summaries, indent=2)}\n",
    "\n",
    "        Documentation Summaries:\n",
    "        {json.dumps(markdown_summaries, indent=2)}\n",
    "\n",
    "        Diagram/Figure Analyses:\n",
    "        {json.dumps(image_summaries, indent=2)}\n",
    "\n",
    "        Create a comprehensive technical analysis that outlines:\n",
    "        1. Technical Requirements and Architecture\n",
    "           - Core technical requirements, including search parameters for resource types\n",
    "           - System architecture and patterns\n",
    "           - Integration points and interfaces\n",
    "           \n",
    "        2. Implementation Details\n",
    "           - Key configurations and settings\n",
    "           - Resource profiles and extensions\n",
    "           - Validation rules and constraints (e.g., Must Haves, conformance verbs)\n",
    "           \n",
    "        3. Visual Documentation Analysis\n",
    "           - Technical workflows and processes\n",
    "           - Component relationships\n",
    "           \n",
    "        4. Culminating Analysis\n",
    "           - Connection of information contained between documentation, config, and diagrams\n",
    "           - Dependencies and prerequisites\n",
    "           \n",
    "        Highlight any overarching implementation considerations.\"\"\"\n",
    "\n",
    "        response = safe_claude_request(\n",
    "            client,\n",
    "            model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": \"Here is the comprehensive technical analysis: <summary>\"}\n",
    "            ],\n",
    "            max_tokens=8192,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating meta-summary: {str(e)}\")\n",
    "        return \"Unable to create meta-summary due to error\"\n",
    "\n",
    "def save_processed_content(base_directory='full-ig', output_directory='processed_output'):\n",
    "    \"\"\"Save all processed content with progress tracking\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    client = create_anthropic_client()\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting content processing...\")\n",
    "        final_summary = process_all_content(client, base_directory)\n",
    "        \n",
    "        with open(os.path.join(output_directory, 'final_technical_analysis.md'), 'w') as f:\n",
    "            f.write(final_summary)\n",
    "        \n",
    "        print(f\"Processing complete. Results saved to {output_directory}\")\n",
    "        return final_summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        logging.error(f\"Processing failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Processor Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Copied 166 JSON files to full-ig/json_only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 166 JSON files\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "client = create_anthropic_client()\n",
    "\n",
    "# Create rate limiter\n",
    "rate_limiter = create_rate_limiter(max_requests_per_minute=25)\n",
    "\n",
    "# Copy and organize files\n",
    "copied_files = copy_json_files()\n",
    "print(f\"Copied {len(copied_files)} JSON files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created folder: full-ig/json_only/Location\n",
      "INFO:root:Created folder: full-ig/json_only/StructureDefinition\n",
      "INFO:root:Created folder: full-ig/json_only/ValueSet\n",
      "INFO:root:Created folder: full-ig/json_only/CodeSystem\n",
      "INFO:root:Created folder: full-ig/json_only/OrganizationAffiliation\n",
      "INFO:root:Created folder: full-ig/json_only/SearchParameter\n",
      "INFO:root:Created folder: full-ig/json_only/HealthcareService\n",
      "INFO:root:Created folder: full-ig/json_only/usage\n",
      "INFO:root:Created folder: full-ig/json_only/Organization\n",
      "INFO:root:Created folder: full-ig/json_only/CapabilityStatement\n",
      "INFO:root:Created folder: full-ig/json_only/PractitionerRole\n",
      "INFO:root:Created folder: full-ig/json_only/ImplementationGuide\n",
      "INFO:root:Created folder: full-ig/json_only/InsurancePlan\n",
      "INFO:root:Created folder: full-ig/json_only/Practitioner\n",
      "INFO:root:Created folder: full-ig/json_only/Endpoint\n",
      "INFO:root:Created folder: full-ig/json_only/plan\n",
      "INFO:root:Created Organization_combined.json with 11 entries\n",
      "INFO:root:Created StructureDefinition_combined.json with 21 entries\n",
      "INFO:root:Created CapabilityStatement_combined.json with 1 entries\n",
      "INFO:root:Created Practitioner_combined.json with 3 entries\n",
      "ERROR:root:Error decoding JSON from plan-net.openapi.json: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)\n",
      "INFO:root:Created Location_combined.json with 9 entries\n",
      "INFO:root:Created CodeSystem_combined.json with 14 entries\n",
      "INFO:root:Created usage_combined.json with 1 entries\n",
      "INFO:root:Created ValueSet_combined.json with 24 entries\n",
      "INFO:root:Created HealthcareService_combined.json with 10 entries\n",
      "INFO:root:Created SearchParameter_combined.json with 51 entries\n",
      "INFO:root:Created Endpoint_combined.json with 1 entries\n",
      "INFO:root:Created InsurancePlan_combined.json with 2 entries\n",
      "INFO:root:Created ImplementationGuide_combined.json with 1 entries\n",
      "INFO:root:Created PractitionerRole_combined.json with 6 entries\n",
      "INFO:root:Created OrganizationAffiliation_combined.json with 7 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files grouped by base name\n",
      "Files organized into folders\n",
      "JSONs consolidated\n"
     ]
    }
   ],
   "source": [
    "grouped_files = group_files_by_base_name('full-ig/json_only')\n",
    "print(\"Files grouped by base name\")\n",
    "\n",
    "copy_files_to_folders('full-ig/json_only', grouped_files)\n",
    "print(\"Files organized into folders\")\n",
    "\n",
    "consolidate_jsons('full-ig/json_only')\n",
    "print(\"JSONs consolidated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.396119 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.891250 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.442574 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.976791 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Error in Claude request: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.386984 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.903030 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Error in Claude request: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.399305 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.998518 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Error in Claude request: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.400208 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.764099 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Error in Claude request: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.425470 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.933865 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "ERROR:root:Error in Claude request: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "ERROR:root:Error processing json file full-ig/json_only/StructureDefinition_combined.json: RetryError[<Future at 0x15c277dd0 state=finished raised RateLimitError>]\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 51.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.420183 seconds\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.758032 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.427545 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.963497 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:root:Error in Claude request: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "ERROR:root:Error processing json file full-ig/json_only/Location_combined.json: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.459782 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.467237 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.816741 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:root:Error in Claude request: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "ERROR:root:Error processing json file full-ig/json_only/HealthcareService_combined.json: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.434874 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.857933 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:root:Error in Claude request: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "ERROR:root:Error combining summaries: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.389086 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.439577 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.394893 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.378364 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.445548 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.887180 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:root:Error in Claude request: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "ERROR:root:Error processing json file full-ig/json_only/ValueSet_combined.json: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.463232 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.381529 seconds\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.969238 seconds\n",
      "ERROR:root:Error in Claude request: Connection error.\n",
      "ERROR:root:Error processing json file full-ig/json_only/SearchParameter_combined.json: Connection error.\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.496378 seconds\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.889909 seconds\n",
      "ERROR:root:Error in Claude request: Connection error.\n",
      "ERROR:root:Error processing json file full-ig/json_only/CodeSystem_combined.json: Connection error.\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.453183 seconds\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.874105 seconds\n",
      "ERROR:root:Error in Claude request: Connection error.\n",
      "ERROR:root:Error processing json file full-ig/json_only/OrganizationAffiliation_combined.json: Connection error.\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.462815 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.752281 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:root:Error in Claude request: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "ERROR:root:Error processing image full-ig/site/Figures/Slide3.png: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "ERROR:root:Error processing image file full-ig/site/Figures/Slide3.png: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.459565 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.822123 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:root:Error in Claude request: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "ERROR:root:Error processing image full-ig/site/Figures/Slide1.png: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "ERROR:root:Error processing image file full-ig/site/Figures/Slide1.png: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All content processed\n"
     ]
    }
   ],
   "source": [
    "# Process with rate limiting\n",
    "final_analysis = process_all_content(client, base_directory='full-ig')\n",
    "print(\"All content processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Output to Markdown File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved to processed_output\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "output_dir = 'summarized_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'technical_analysis_summary.md'), 'w') as f:\n",
    "    f.write(final_analysis)\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Successful Run - Output from Claude LLM: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Technical Requirements and Architecture\n",
    "\n",
    "Core technical requirements:\n",
    "- FHIR R4 (v4.0.1) implementation for provider directories and insurance networks\n",
    "- RESTful API supporting GET requests only (query-only)\n",
    "- JSON support required, XML recommended\n",
    "- No authentication required, no consumer identifying information stored\n",
    "- Conformance to US Core profiles where applicable\n",
    "- Support for _include, _revinclude, and chained searches\n",
    "\n",
    "System architecture and patterns:\n",
    "- Centralized Validated Healthcare Directory (VHDir) with surrounding processes\n",
    "- Local workflow environments interfacing with central directory \n",
    "- Modular design separating validation, core storage, and exchange functions\n",
    "- Use of FHIR as standardized communication protocol\n",
    "\n",
    "Integration points and interfaces:\n",
    "- FHIR-based API for third-party applications to query provider network data\n",
    "- Primary data sources input to VHDir via FHIR\n",
    "- Exchange processes between VHDir and local workflow environments\n",
    "- Attestation process involving external attesters\n",
    "\n",
    "2. Implementation Details\n",
    "\n",
    "Key configurations and settings:\n",
    "- Specific search parameters defined for each resource type\n",
    "- _lastUpdated parameter used for change detection\n",
    "- Capability statements define expected server behaviors\n",
    "\n",
    "Resource profiles and extensions:\n",
    "- Custom profiles for Endpoint, HealthcareService, InsurancePlan, Location, Network, Organization, OrganizationAffiliation, Practitioner, and PractitionerRole\n",
    "- Extensions for additional data elements (e.g. accessibility, qualifications)\n",
    "- Use of US Core profiles as foundation where applicable\n",
    "\n",
    "Validation rules and constraints:\n",
    "- Must Support flags on profile elements\n",
    "- Specific conformance verbs (SHALL, SHOULD, MAY) define requirements\n",
    "- Value sets and code systems for controlled terminologies\n",
    "\n",
    "3. Visual Documentation Analysis\n",
    "\n",
    "Technical workflows and processes:\n",
    "- Data flow from primary sources to VHDir\n",
    "- Initial and recurring validation processes within VHDir\n",
    "- Exchange processes facilitating data sharing\n",
    "- Attestation workflow involving external attesters\n",
    "\n",
    "Component relationships:\n",
    "- VHDir as central component containing core data and processes\n",
    "- Local workflow environments interfacing with VHDir\n",
    "- Primary sources providing input data\n",
    "- Attesters providing external validation\n",
    "\n",
    "Architecture diagrams insights:\n",
    "- Centralized directory model with distributed local environments\n",
    "- Clear separation of core directory functions (validation, storage, exchange)\n",
    "- Multiple data flow paths using FHIR standard\n",
    "- Scope delineation between VHDir and Plan-Net implementation guides\n",
    "\n",
    "4. Culminating Analysis\n",
    "\n",
    "Relationships between documentation, config, and diagrams:\n",
    "- Consistent emphasis on FHIR R4 and US Core profiles across all sources\n",
    "- Configuration JSON aligns with profile definitions in documentation\n",
    "- Diagrams reinforce centralized directory concept described in text\n",
    "\n",
    "Dependencies and prerequisites:\n",
    "- FHIR R4 (v4.0.1) as foundational standard\n",
    "- US Core profiles as starting point for custom profiles\n",
    "- Sushi 1.0.0 for IG directory structure\n",
    "- Part of larger Da Vinci project initiative\n",
    "\n",
    "Important considerations:\n",
    "- Extensive use of profiling and extensions to tailor FHIR for provider directories\n",
    "- Strong focus on search capabilities and resource relationships\n",
    "- Centralized directory model with local interfaces may require careful performance optimization\n",
    "- Lack of authentication could limit use cases involving sensitive data\n",
    "- Query-only API may restrict some advanced directory management scenarios\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
