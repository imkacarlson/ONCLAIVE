{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Summarization of IG Documents\n",
    "This script aims to develop a prompt chain structure to send large amounts of text/content to LLM APIs through multiple calls. \n",
    "\n",
    "The current approach takes in all JSON files from the Plan Net IG, all figure diagrams, and key narrative information in markdown form (formerly extracted from HTML files). The script then summarizes each type of information in batches, and creates a meta-summarization of all documents to outline the technical information it can glean from all submitted documentation. The goal is to identify if this approach can produce all technical information at an appropriate level of deatil that an LLM would need to know to help design a test kit for a given IG. Best practices from the Claude API were used.\n",
    "\n",
    "First attempts: We were able to run through the script fully using the Claude API with all JSONs, markdown content, and images. The process took over 93 minutes. The meta summary is saved in the file final_technical_analysis.md and is pasted at the end of this script. We can see that the first iteration did not produce detailed enough information about requirements, etc. \n",
    "\n",
    "In progress: \n",
    "- Doing a full run for Gemini and GPT, and instead of outputting summaries of the content, using this meta summarization process to produce list of test requirements, revising the prompting based on Inferno requirements extraction process documentation. \n",
    "- Reviewing LangChain iterative refinement capabilities to improve summary quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Organization:\n",
    "1. Imports and Basic Setup\n",
    "2. Configuration\n",
    "3. File Processing Functions\n",
    "4. Core Processing Functions\n",
    "5. Batch Processing Functions\n",
    "6. Verification and Query Functions\n",
    "7. Main Execution Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPORTS AND BASIC SETUP\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "from json_repair import repair_json\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from anthropic import RateLimitError\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Basic setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "CERT_PATH = '/opt/homebrew/etc/openssl@3/cert.pem'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in API keys for Claude, Gemini, and GPT from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "#OpenAI.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Claude API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CONFIGURATION\n",
    "# Global configuration variables\n",
    "SYSTEM_ROLE = \"\"\"You are a seasoned Healthcare Integration Test Engineer with extensive FHIR experience \n",
    "that is looking to build a FHIR Test Kit for a specific Implementation Guide. You analyze technical documentation \n",
    "and requirements with a focus on testability, implementation verification, and conformance testing.\"\"\"\n",
    "\n",
    "CLAUDE_CONFIG = {\n",
    "    \"model_name\": \"claude-3-5-sonnet-20240620\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"requests_per_minute\": 25,\n",
    "    \"delay_between_chunks\": 2,\n",
    "    \"delay_between_batches\": 10,\n",
    "    \"default_batch_size\": 3\n",
    "}\n",
    "\n",
    "def create_claude_messages(prompt: str, assistant_prefix: str = None) -> list:\n",
    "    \"\"\"Helper function to create standardized message format\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": assistant_prefix or \"As a Healthcare Integration Test Engineer, here is my analysis: \"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def create_claude_request(messages: list, stop_sequences: list = None) -> dict:\n",
    "    \"\"\"Helper function to create standardized request parameters\"\"\"\n",
    "    return {\n",
    "        \"model\": CLAUDE_CONFIG[\"model_name\"],\n",
    "        \"messages\": messages,\n",
    "        \"system\": SYSTEM_ROLE,\n",
    "        \"max_tokens\": CLAUDE_CONFIG[\"max_tokens\"],\n",
    "        \"stop_sequences\": stop_sequences\n",
    "    }\n",
    "\n",
    "# Client setup\n",
    "def create_anthropic_client():\n",
    "    \"\"\"Create Anthropic client with proper certificate verification\"\"\"\n",
    "    verify_path = CERT_PATH if os.path.exists(CERT_PATH) else True\n",
    "    http_client = httpx.Client(\n",
    "        verify=verify_path,\n",
    "        timeout=30.0\n",
    "    )\n",
    "    return Anthropic(\n",
    "        api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "        http_client=http_client\n",
    "    )\n",
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def safe_claude_request(client, **kwargs):\n",
    "    \"\"\"Make a rate-limited request to Claude with retries\"\"\"\n",
    "    rate_limiter.wait_if_needed()\n",
    "    try:\n",
    "        return client.messages.create(**kwargs)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in Claude request: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Rate limiting setup\n",
    "def create_rate_limiter(max_requests_per_minute=50):\n",
    "    \"\"\"Create a simple rate limiter\"\"\"\n",
    "    class RateLimiter:\n",
    "        def __init__(self):\n",
    "            self.requests = []\n",
    "            self.max_requests = max_requests_per_minute\n",
    "            self.time_window = 60  # seconds\n",
    "\n",
    "        def wait_if_needed(self):\n",
    "            now = time.time()\n",
    "            self.requests = [req_time for req_time in self.requests \n",
    "                           if now - req_time < self.time_window]\n",
    "            \n",
    "            if len(self.requests) >= self.max_requests:\n",
    "                sleep_time = self.time_window - (now - self.requests[0])\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "                self.requests = self.requests[1:]\n",
    "            \n",
    "            self.requests.append(now)\n",
    "            \n",
    "    return RateLimiter()\n",
    "\n",
    "rate_limiter = create_rate_limiter(max_requests_per_minute=CLAUDE_CONFIG[\"requests_per_minute\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling in files of interest\n",
    "Sourcing JSON files from the IG, copying them from full-ig directory into json_only folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FILE PROCESSING FUNCTIONS\n",
    "\n",
    "def copy_json_files(source_folder='full-ig/site', destination_folder='full-ig/json_only'):\n",
    "    \"\"\"\n",
    "    Copy JSON files from source to destination directory,\n",
    "    excluding compound extensions and creating the directory if needed\n",
    "    \"\"\"\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    json_files = []\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        # Check if the file ends with .json but not with compound extensions\n",
    "        if (file_name.endswith('.json') and \n",
    "            not any(file_name.endswith(ext) for ext in [\n",
    "                '.ttl.json', \n",
    "                '.jsonld.json', \n",
    "                '.xml.json', \n",
    "                '.change.history.json'\n",
    "            ])):\n",
    "            json_files.append(file_name)\n",
    "            # Copy the file to the destination folder\n",
    "            shutil.copy(os.path.join(source_folder, file_name), destination_folder)\n",
    "            \n",
    "    logging.info(f\"Copied {len(json_files)} JSON files to {destination_folder}\")\n",
    "    return json_files\n",
    "\n",
    "def group_files_by_base_name(directory_path: str, delimiter: str = '-') -> dict:\n",
    "    \"\"\"\n",
    "    Group files in the directory by their base name.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to the directory containing files\n",
    "        delimiter: The delimiter to split the file name on\n",
    "    Returns:\n",
    "        Dictionary mapping base names to lists of related files\n",
    "    \"\"\"\n",
    "    grouped_files = defaultdict(list)\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json') and delimiter in filename:\n",
    "            base_name = filename.split(delimiter)[0]\n",
    "            grouped_files[base_name].append(filename)\n",
    "    \n",
    "    return grouped_files\n",
    "\n",
    "def copy_files_to_folders(directory_path: str, grouped_files: dict):\n",
    "    \"\"\"\n",
    "    Create folders for each base name and copy related files into them.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to the directory containing files\n",
    "        grouped_files: Dictionary of grouped files by base name\n",
    "    \"\"\"\n",
    "    for base_name, files in grouped_files.items():\n",
    "        if len(files) >= 1:\n",
    "            # Create base name folder\n",
    "            base_folder = os.path.join(directory_path, base_name)\n",
    "            if not os.path.exists(base_folder):\n",
    "                os.makedirs(base_folder)\n",
    "            logging.info(f\"Created folder: {base_folder}\")\n",
    "            \n",
    "            # Copy files to their respective folders\n",
    "            for file in files:\n",
    "                source_file = os.path.join(directory_path, file)\n",
    "                destination_file = os.path.join(base_folder, file)\n",
    "                shutil.copy(source_file, destination_file)\n",
    "\n",
    "def consolidate_jsons(base_directory: str = 'full-ig/json_only'):\n",
    "    \"\"\"\n",
    "    Consolidate related JSON files while maintaining object integrity.\n",
    "    Creates combined files for each resource type.\n",
    "    \"\"\"\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "              if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        folder_path = os.path.join(base_directory, subdir)\n",
    "        combined_data = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        json_content = json.load(f)\n",
    "                        if isinstance(json_content, dict) and 'entry' in json_content:\n",
    "                            combined_data.extend(json_content['entry'])\n",
    "                        else:\n",
    "                            combined_data.append(json_content)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.error(f\"Error decoding JSON from {filename}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if combined_data:\n",
    "            output_filename = f\"{subdir}_combined.json\"\n",
    "            output_path = os.path.join(base_directory, output_filename)\n",
    "            \n",
    "            try:\n",
    "                with open(output_path, 'w') as outfile:\n",
    "                    json.dump({\n",
    "                        \"resourceType\": subdir,\n",
    "                        \"total\": len(combined_data),\n",
    "                        \"entry\": combined_data\n",
    "                    }, outfile, indent=2)\n",
    "                logging.info(f\"Created {output_filename} with {len(combined_data)} entries\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error writing {output_filename}: {e}\")\n",
    "\n",
    "def prepare_json_for_processing(json_file_path: str) -> Union[dict, list]:\n",
    "    \"\"\"\n",
    "    Read and prepare JSON file for processing.\n",
    "    Extracts entries if present or returns whole content.\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    if isinstance(data, dict) and 'entry' in data:\n",
    "        return data['entry']\n",
    "    return data\n",
    "\n",
    "def split_json(json_data: Union[dict, list], max_size: int = 2000) -> List[list]:\n",
    "    \"\"\"\n",
    "    Split JSON array into chunks while maintaining complete JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        json_data: JSON data to split\n",
    "        max_size: Maximum size for each chunk in characters\n",
    "    Returns:\n",
    "        List of chunks, where each chunk contains complete JSON objects\n",
    "    \"\"\"\n",
    "    if isinstance(json_data, dict):\n",
    "        json_data = [json_data]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for item in json_data:\n",
    "        item_size = len(json.dumps(item))\n",
    "        \n",
    "        # Handle large individual items\n",
    "        if item_size > max_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "            chunks.append([item])\n",
    "            continue\n",
    "        \n",
    "        # Start new chunk if current would exceed max_size\n",
    "        if current_size + item_size > max_size and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        \n",
    "        current_chunk.append(item)\n",
    "        current_size += item_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    \"\"\"Convert image to base64 encoding for API consumption\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def clean_markdown(text: str) -> str:\n",
    "    \"\"\"Clean markdown content by removing unnecessary whitespace and formatting\"\"\"\n",
    "    # Remove multiple newlines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove excessive punctuation \n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    \n",
    "    # Remove escaped characters\n",
    "    text = re.sub(r'\\\\(.)', r'\\1', text)\n",
    "    \n",
    "    # Remove table formatting but keep content\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'[-\\s]*\\n[-\\s]*', '\\n', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Files for LLM\n",
    "\n",
    "1. Because we have so many JSONs, we cannot feed them all to an LLM at once. These functions split combined JSONs into chunks that can fit in one LLM call. Each chunk only contains full JSONs so individual JSONs are not split up. We included this feature to try and maintain summarization quality- the LLM should receive all relevant information together instead of in pieces, to help it understand what it is receiving.\n",
    "\n",
    "2. We also convert images to base 64 encoding so they can be read by an LLM.\n",
    "\n",
    "3. We set up a prompt for summarizing JSON chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CORE PROCESSING FUNCTIONS\n",
    "\n",
    "def create_json_summary_prompt(chunk: Union[dict, list], chunk_num: int, total_chunks: int) -> str:\n",
    "    \"\"\"\n",
    "    Create standardized prompt for summarizing JSON chunk with test engineering focus.\n",
    "    \n",
    "    Args:\n",
    "        chunk: The JSON chunk to analyze\n",
    "        chunk_num: Current chunk number\n",
    "        total_chunks: Total number of chunks\n",
    "    \"\"\"\n",
    "    return f\"\"\"Analyze this portion ({chunk_num} of {total_chunks}) of a FHIR Implementation Guide JSON resource bundle.\n",
    "    Focus on key technical details, requirements, and relationships that need testing.\n",
    "    \n",
    "    JSON Content:\n",
    "    {json.dumps(chunk, indent=2)}\n",
    "    \n",
    "    Please provide:\n",
    "    1. An overview of what is included in the group of JSONs and what piece of the IG they represent\n",
    "    2. Resource Types and Profiles present that will need test coverage\n",
    "    3. Key technical requirements and constraints that must be validated\n",
    "    4. Dependencies and relationships between resources that need testing\n",
    "    5. Specific conformance requirements that need verification\n",
    "    \n",
    "    Focus on new information not covered in previous chunks.\"\"\"\n",
    "\n",
    "def process_json_file(client: Anthropic, json_file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a JSON file while maintaining object integrity.\n",
    "    Splits into chunks if needed and processes each chunk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = prepare_json_for_processing(json_file_path)\n",
    "        chunks = split_json(json_data)\n",
    "        chunk_summaries = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prompt = create_json_summary_prompt(chunk, i+1, len(chunks))\n",
    "            \n",
    "            messages = create_claude_messages(\n",
    "                prompt=prompt,\n",
    "                assistant_prefix=\"Here is the technical summary with testing implications: <summary>\"\n",
    "            )\n",
    "            \n",
    "            kwargs = create_claude_request(\n",
    "                messages=messages,\n",
    "                stop_sequences=[\"</summary>\"]\n",
    "            )\n",
    "            \n",
    "            response = safe_claude_request(client, **kwargs)\n",
    "            chunk_summaries.append(response.content[0].text)\n",
    "            time.sleep(CLAUDE_CONFIG[\"delay_between_chunks\"])\n",
    "        \n",
    "        return combine_summaries(client, chunk_summaries)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {json_file_path}: {str(e)}\")\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "\n",
    "def process_markdown(client: Anthropic, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Process markdown content and generate a technical summary focused on testing requirements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Analyze this technical documentation markdown content from a testing perspective:\n",
    "\n",
    "        {content}\n",
    "\n",
    "        Please provide:\n",
    "        1. Key technical concepts and definitions that need validation\n",
    "        2. Important requirements and specifications that must be tested\n",
    "        3. Technical workflows or processes that require test coverage\n",
    "        4. Dependencies or prerequisites that need verification\n",
    "        5. Implementation details and guidelines that should be validated\n",
    "        \n",
    "        Focus on extracting testable requirements and validation criteria.\"\"\"\n",
    "        \n",
    "        messages = create_claude_messages(\n",
    "            prompt=prompt,\n",
    "            assistant_prefix=\"Here is the technical summary with testing implications: <summary>\"\n",
    "        )\n",
    "        \n",
    "        kwargs = create_claude_request(\n",
    "            messages=messages,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        response = safe_claude_request(client, **kwargs)\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing markdown content: {str(e)}\")\n",
    "        return \"Error processing markdown content: \" + str(e)\n",
    "\n",
    "def process_image(client: Anthropic, image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a single image and generate a technical description focused on testing implications.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/\" + image_path.split('.')[-1],\n",
    "                            \"data\": base64_image\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"Analyze this technical diagram/figure from a testing perspective. Focus on:\n",
    "                        1. Key components and their relationships that need validation\n",
    "                        2. Technical workflows or processes that require test coverage\n",
    "                        3. Architecture or design patterns that need verification\n",
    "                        4. Integration points that must be tested\n",
    "                        5. Important technical details or annotations that affect testing\n",
    "                        Provide a detailed technical description with testing implications.\"\"\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        kwargs = create_claude_request(\n",
    "            messages=messages,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        response = safe_claude_request(client, **kwargs)\n",
    "        return response.content[0].text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def combine_summaries(client: Anthropic, summaries: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Combine chunk summaries into a cohesive analysis focused on testing requirements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Synthesize these related summaries into a unified technical analysis \n",
    "        focused on testing requirements:\n",
    "\n",
    "        {json.dumps(summaries, indent=2)}\n",
    "        \n",
    "        Create a comprehensive analysis that:\n",
    "        1. Eliminates redundant information\n",
    "        2. Maintains technical accuracy\n",
    "        3. Includes specific technical information about:\n",
    "           - Search parameters that need testing\n",
    "           - Resource profiles that require validation\n",
    "           - Must Support elements that need verification\n",
    "           - Conformance requirements that must be tested\n",
    "        4. Preserves important relationships and workflows that need test coverage\"\"\"\n",
    "        \n",
    "        messages = create_claude_messages(\n",
    "            prompt=prompt,\n",
    "            assistant_prefix=\"Here is the combined analysis with testing implications: <summary>\"\n",
    "        )\n",
    "        \n",
    "        kwargs = create_claude_request(\n",
    "            messages=messages,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        response = safe_claude_request(client, **kwargs)\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error combining summaries: {str(e)}\")\n",
    "        return \"Unable to combine summaries due to error\"\n",
    "\n",
    "def create_meta_summary(client: Anthropic, \n",
    "                       json_summaries: Dict[str, str], \n",
    "                       markdown_summaries: Dict[str, str], \n",
    "                       image_summaries: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Create a comprehensive meta-summary focused on testing requirements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Synthesize information from multiple content types into a comprehensive \n",
    "        technical analysis focused on testing requirements:\n",
    "\n",
    "        JSON Configuration Summaries:\n",
    "        {json.dumps(json_summaries, indent=2)}\n",
    "\n",
    "        Documentation Summaries:\n",
    "        {json.dumps(markdown_summaries, indent=2)}\n",
    "\n",
    "        Diagram/Figure Analyses:\n",
    "        {json.dumps(image_summaries, indent=2)}\n",
    "\n",
    "        Create a comprehensive technical analysis that outlines:\n",
    "        1. Technical Requirements and Architecture\n",
    "           - Core technical requirements that need testing\n",
    "           - System architecture components requiring validation\n",
    "           - Integration points needing test coverage\n",
    "           \n",
    "        2. Implementation Details\n",
    "           - Key configurations requiring verification\n",
    "           - Resource profiles needing validation\n",
    "           - Constraints and rules requiring testing\n",
    "           \n",
    "        3. Testing Implications\n",
    "           - Required test scenarios\n",
    "           - Validation approaches\n",
    "           - Conformance verification methods\n",
    "           \n",
    "        4. Culminating Analysis\n",
    "           - Overall testing strategy\n",
    "           - Key risk areas requiring thorough testing\n",
    "           - Implementation considerations for test development\"\"\"\n",
    "\n",
    "        messages = create_claude_messages(\n",
    "            prompt=prompt,\n",
    "            assistant_prefix=\"Here is the comprehensive technical analysis for test development: <summary>\"\n",
    "        )\n",
    "        \n",
    "        kwargs = create_claude_request(\n",
    "            messages=messages,\n",
    "            stop_sequences=[\"</summary>\"]\n",
    "        )\n",
    "        \n",
    "        response = safe_claude_request(client, **kwargs)\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating meta-summary: {str(e)}\")\n",
    "        return \"Unable to create meta-summary due to error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Rate Limiting & Safe Call Functions\n",
    "\n",
    "Because of the amount of content we are sending to APIs, we need to include rate limiting in our prompt chaining process to avoid hitting rate limits. This includes a function to create a reate limiter and to make calls to the Claude LLM with the rate limiter included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Processing Functions\n",
    "This set of functions allows for summarizing of batches of each information type (e.g., JSONs, markdown, and images) from individual files, combining those summaries at the directory level/file type level, and then sending those combined summaries to the LLM at once to ask for one meta-summarization. The meta-summary is saved to an output file for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. BATCH PROCESSING FUNCTIONS\n",
    "\n",
    "def process_content(client: Anthropic, file_path: str, content_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Generic content processor that handles any content type.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        file_path: Path to the file to process\n",
    "        content_type: Type of content ('json', 'markdown', or 'image')\n",
    "    Returns:\n",
    "        Processed content summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if content_type == \"json\":\n",
    "            return process_json_file(client, file_path)\n",
    "        elif content_type == \"markdown\":\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = clean_markdown(f.read())\n",
    "            return process_markdown(client, content)\n",
    "        elif content_type == \"image\":\n",
    "            return process_image(client, file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {content_type} file {file_path}: {str(e)}\")\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "\n",
    "def process_batch(client: Anthropic, \n",
    "                 files: List[str], \n",
    "                 content_type: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Process a batch of files with rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        files: List of file paths to process\n",
    "        content_type: Type of content ('json', 'markdown', or 'image')\n",
    "    Returns:\n",
    "        Dictionary mapping file paths to their processed summaries\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for i in range(0, len(files), CLAUDE_CONFIG[\"default_batch_size\"]):\n",
    "        batch = files[i:i + CLAUDE_CONFIG[\"default_batch_size\"]]\n",
    "        logging.info(f\"Processing batch {i//CLAUDE_CONFIG['default_batch_size'] + 1} of {math.ceil(len(files)/CLAUDE_CONFIG['default_batch_size'])}\")\n",
    "        \n",
    "        for file in batch:\n",
    "            try:\n",
    "                logging.info(f\"Processing {content_type} file: {os.path.basename(file)}\")\n",
    "                results[file] = process_content(client, file, content_type)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file}: {str(e)}\")\n",
    "                results[file] = f\"Error: {str(e)}\"\n",
    "                \n",
    "        if len(batch) < CLAUDE_CONFIG[\"default_batch_size\"]:\n",
    "            logging.info(\"Processed final partial batch\")\n",
    "        time.sleep(CLAUDE_CONFIG[\"delay_between_batches\"])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_all_content(client: Anthropic, base_directory: str = 'full-ig') -> str:\n",
    "    \"\"\"\n",
    "    Process all content types using unified batch processing with rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        base_directory: Base directory containing all content\n",
    "    Returns:\n",
    "        Meta-summary of all processed content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Process JSONs\n",
    "        logging.info(\"Processing JSON files...\")\n",
    "        json_files = [\n",
    "            os.path.join(base_directory, 'json_only', f) \n",
    "            for f in os.listdir(os.path.join(base_directory, 'json_only')) \n",
    "            if f.endswith('_combined.json')\n",
    "        ]\n",
    "        json_summaries = process_batch(client, json_files, \"json\")\n",
    "        logging.info(f\"Processed {len(json_files)} JSON files\")\n",
    "        \n",
    "        # Process markdown files\n",
    "        markdown_summaries = {}\n",
    "        markdown_dir = os.path.join(base_directory, 'markdown')\n",
    "        if os.path.exists(markdown_dir):\n",
    "            logging.info(\"Processing markdown files...\")\n",
    "            md_files = [\n",
    "                os.path.join(markdown_dir, f) \n",
    "                for f in os.listdir(markdown_dir) \n",
    "                if f.endswith('.md')\n",
    "            ]\n",
    "            markdown_summaries = process_batch(client, md_files, \"markdown\")\n",
    "            logging.info(f\"Processed {len(md_files)} markdown files\")\n",
    "        \n",
    "        # Process images\n",
    "        image_summaries = {}\n",
    "        image_dir = os.path.join(base_directory, 'site/Figures')\n",
    "        if os.path.exists(image_dir):\n",
    "            logging.info(\"Processing image files...\")\n",
    "            img_files = [\n",
    "                os.path.join(image_dir, f) \n",
    "                for f in os.listdir(image_dir) \n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            ]\n",
    "            image_summaries = process_batch(client, img_files, \"image\")\n",
    "            logging.info(f\"Processed {len(img_files)} image files\")\n",
    "        \n",
    "        logging.info(\"Creating meta-summary...\")\n",
    "        time.sleep(CLAUDE_CONFIG[\"delay_between_batches\"])  # Extra delay before meta-summary\n",
    "        \n",
    "        return create_meta_summary(\n",
    "            client, \n",
    "            json_summaries, \n",
    "            markdown_summaries, \n",
    "            image_summaries\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing content: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def save_processed_content(base_directory: str = 'full-ig', \n",
    "                         output_directory: str = 'processed_output') -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Save all processed content with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        base_directory: Base directory containing all content\n",
    "        output_directory: Directory to save processed content\n",
    "    Returns:\n",
    "        Final technical analysis or None if error occurs\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    client = create_anthropic_client()\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Starting content processing...\")\n",
    "        final_summary = process_all_content(client, base_directory)\n",
    "        \n",
    "        output_file = os.path.join(output_directory, 'final_technical_analysis.md')\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(final_summary)\n",
    "        \n",
    "        logging.info(f\"Processing complete. Results saved to {output_file}\")\n",
    "        return final_summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing failed: {str(e)}\")\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying LLM after Passing through all content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. VERIFICATION AND QUERY FUNCTIONS\n",
    "\n",
    "def create_verification_questions() -> List[str]:\n",
    "    \"\"\"\n",
    "    Create a standardized list of verification questions focused on test kit development.\n",
    "    Returns a list of questions to verify understanding of the IG.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        \"What is the purpose of the FHIR DaVinci PlanNet Implementation Guide?\",\n",
    "        \"Who are the intended users and actors of the FHIR DaVinci PlanNet Implementation Guide?\",\n",
    "        \"Are there one or more workflows defined in the FHIR DaVinci PDex Plan Net Implementation Guide? Please use all the information you know\",\n",
    "        \"What data is being exchanged in the FHIR DaVinci PDex Plan Net Implementation Guide and why?\",\n",
    "        \"How is that data represented by the resources and profiles in the FHIR DaVinci PDex Plan Net Implementation Guide? Create a list of the CRUDs + search parameters, create a code skeleton that would test each of the items in the list and then write the code\",\n",
    "        \"What actions (REST/CRUD) or operations can be used in the FHIR DaVinci PDex Plan Net Implementation Guide?\",\n",
    "        \"What are all the mandatory requirements and rules from the DaVinci PDex Plan Net Implementation Guide for compliant implementations? Which ones are fulfilled from the code you wrote to test those items from the last question?\",\n",
    "        \"What are all the optional requirements and rules from the DaVinci PDex Plan Net Implementation Guide for compliant implementations?\",\n",
    "        \"Create a test plan for the FHIR DaVinci RDcx Plan Net Implementation Guide\"\n",
    "    ]\n",
    "\n",
    "def process_query(client: Anthropic,\n",
    "                 question: str, \n",
    "                 context: str, \n",
    "                 conversation_history: Optional[List[Tuple[str, str]]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Process a single query about the technical content with conversation context.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        question: The question to ask\n",
    "        context: Technical analysis context\n",
    "        conversation_history: Optional list of previous Q&A pairs\n",
    "    Returns:\n",
    "        Claude's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"{SYSTEM_ROLE}\n",
    "\n",
    "        Technical Analysis Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\"\"\"\n",
    "        \n",
    "        if conversation_history:\n",
    "            prompt += \"\\n\\nPrevious relevant discussion:\\n\"\n",
    "            for q, a in conversation_history[-3:]:  # Keep last 3 exchanges\n",
    "                prompt += f\"\\nQ: {q}\\nA: {a}\\n\"\n",
    "        \n",
    "        messages = create_claude_messages(prompt)\n",
    "        kwargs = create_claude_request(messages)\n",
    "        \n",
    "        response = safe_claude_request(client, **kwargs)\n",
    "        return response.content[0].text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing query: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def run_verification_analysis(client: Anthropic,\n",
    "                            context_file: str,\n",
    "                            output_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Run verification questions on the processed technical analysis.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        context_file: Path to the technical analysis file\n",
    "        output_dir: Directory to save verification results\n",
    "    Returns:\n",
    "        Dictionary of questions and answers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the technical analysis\n",
    "        with open(context_file, 'r') as f:\n",
    "            context = f.read()\n",
    "        \n",
    "        # Process verification questions\n",
    "        conversation_history = []\n",
    "        verification_results = {}\n",
    "        \n",
    "        logging.info(\"Starting verification questions analysis...\")\n",
    "        questions = create_verification_questions()\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            logging.info(f\"Processing question {i} of {len(questions)}: {question[:100]}...\")\n",
    "            \n",
    "            answer = process_query(\n",
    "                client=client,\n",
    "                question=question,\n",
    "                context=context,\n",
    "                conversation_history=conversation_history\n",
    "            )\n",
    "            \n",
    "            verification_results[question] = answer\n",
    "            conversation_history.append((question, answer))\n",
    "            logging.info(f\"Answer received: {answer[:200]}...\")\n",
    "            \n",
    "            time.sleep(CLAUDE_CONFIG[\"delay_between_chunks\"])\n",
    "        \n",
    "        # Save verification results\n",
    "        verification_output = os.path.join(output_dir, 'verification_results.md')\n",
    "        with open(verification_output, 'w') as f:\n",
    "            f.write(\"# Implementation Guide Test Kit Development Analysis\\n\\n\")\n",
    "            for question, answer in verification_results.items():\n",
    "                f.write(f\"## Question\\n{question}\\n\\n\")\n",
    "                f.write(f\"## Test Engineering Analysis\\n{answer}\\n\\n\")\n",
    "                f.write(\"---\\n\\n\")\n",
    "        \n",
    "        logging.info(f\"Verification analysis complete. Results saved to {verification_output}\")\n",
    "        return verification_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during verification analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def ask_interactive_question(client: Anthropic,\n",
    "                           question: str,\n",
    "                           context_file: str,\n",
    "                           conversation_history: Optional[List[Tuple[str, str]]] = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Interactive function to ask additional questions about the IG.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        question: Question to ask\n",
    "        context_file: Path to the technical analysis file\n",
    "        conversation_history: Optional list of previous Q&A pairs\n",
    "    Returns:\n",
    "        Answer to the question or None if error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the technical analysis\n",
    "        with open(context_file, 'r') as f:\n",
    "            context = f.read()\n",
    "        \n",
    "        answer = process_query(\n",
    "            client=client,\n",
    "            question=question,\n",
    "            context=context,\n",
    "            conversation_history=conversation_history\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nQ: {question}\")\n",
    "        print(f\"\\nA: {answer}\")\n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing question: {str(e)}\")\n",
    "        print(f\"Error processing question: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "class InteractiveQuerySession:\n",
    "    \"\"\"Class to maintain state for an interactive query session\"\"\"\n",
    "    \n",
    "    def __init__(self, client: Anthropic, context_file: str):\n",
    "        self.client = client\n",
    "        self.context_file = context_file\n",
    "        self.conversation_history: List[Tuple[str, str]] = []\n",
    "    \n",
    "    def ask_question(self, question: str) -> Optional[str]:\n",
    "        \"\"\"Ask a question and maintain conversation history\"\"\"\n",
    "        answer = ask_interactive_question(\n",
    "            self.client,\n",
    "            question,\n",
    "            self.context_file,\n",
    "            self.conversation_history\n",
    "        )\n",
    "        \n",
    "        if answer:\n",
    "            self.conversation_history.append((question, answer))\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def save_conversation(self, output_file: str):\n",
    "        \"\"\"Save the conversation history to a file\"\"\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"# Interactive Query Session\\n\\n\")\n",
    "            for question, answer in self.conversation_history:\n",
    "                f.write(f\"## Question\\n{question}\\n\\n\")\n",
    "                f.write(f\"## Answer\\n{answer}\\n\\n\")\n",
    "                f.write(\"---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive query session\n",
    "session = InteractiveQuerySession(client, 'summarized_output/technical_summary1.md')\n",
    "\n",
    "# Ask questions\n",
    "session.ask_question(\"What specific test cases should be created for the Location resource?\")\n",
    "session.ask_question(\"How should we validate the search parameters?\")\n",
    "\n",
    "# Save the conversation\n",
    "session.save_conversation('summarized_output/query_session.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. MAIN EXECUTION FUNCTIONS\n",
    "\n",
    "def initialize_environment(base_directory: str = 'full-ig',\n",
    "                         output_directory: str = 'processed_output') -> Tuple[Anthropic, str, str]:\n",
    "    \"\"\"\n",
    "    Initialize the processing environment.\n",
    "    \n",
    "    Args:\n",
    "        base_directory: Base directory for input files\n",
    "        output_directory: Directory for output files\n",
    "    Returns:\n",
    "        Tuple of (client, base_directory, output_directory)\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Initialize client\n",
    "    client = create_anthropic_client()\n",
    "    \n",
    "    # Create rate limiter\n",
    "    rate_limiter = create_rate_limiter(\n",
    "        max_requests_per_minute=CLAUDE_CONFIG[\"requests_per_minute\"]\n",
    "    )\n",
    "    \n",
    "    return client, base_directory, output_directory\n",
    "\n",
    "def run_initial_analysis(client: Anthropic,\n",
    "                        base_directory: str,\n",
    "                        output_directory: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Run the initial analysis phase including file processing and meta-summary.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        base_directory: Base directory for input files\n",
    "        output_directory: Directory for output files\n",
    "    Returns:\n",
    "        Path to the technical analysis file or None if error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting initial analysis phase...\")\n",
    "        \n",
    "        # Process and organize files\n",
    "        logging.info(\"Organizing files...\")\n",
    "        copied_files = copy_json_files()\n",
    "        grouped_files = group_files_by_base_name('full-ig/json_only')\n",
    "        copy_files_to_folders('full-ig/json_only', grouped_files)\n",
    "        consolidate_jsons('full-ig/json_only')\n",
    "        \n",
    "        # Generate technical analysis\n",
    "        logging.info(\"Generating technical analysis...\")\n",
    "        final_analysis = process_all_content(client, base_directory)\n",
    "        \n",
    "        # Save technical analysis\n",
    "        analysis_file = os.path.join(output_directory, 'technical_summary1.md')\n",
    "        with open(analysis_file, 'w') as f:\n",
    "            f.write(final_analysis)\n",
    "        \n",
    "        logging.info(f\"Initial analysis complete. Results saved to {analysis_file}\")\n",
    "        return analysis_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during initial analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_verification_phase(client: Anthropic,\n",
    "                         analysis_file: str,\n",
    "                         output_directory: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Run the verification phase using the completed technical analysis.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        analysis_file: Path to the technical analysis file\n",
    "        output_directory: Directory for output files\n",
    "    Returns:\n",
    "        Dictionary of verification results or None if error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting verification phase...\")\n",
    "        verification_results = run_verification_analysis(\n",
    "            client,\n",
    "            analysis_file,\n",
    "            output_directory\n",
    "        )\n",
    "        logging.info(\"Verification phase complete\")\n",
    "        return verification_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during verification phase: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def setup_interactive_session(client: Anthropic,\n",
    "                            analysis_file: str) -> InteractiveQuerySession:\n",
    "    \"\"\"\n",
    "    Set up an interactive query session for additional questions.\n",
    "    \n",
    "    Args:\n",
    "        client: Anthropic client instance\n",
    "        analysis_file: Path to the technical analysis file\n",
    "    Returns:\n",
    "        Configured InteractiveQuerySession instance\n",
    "    \"\"\"\n",
    "    return InteractiveQuerySession(client, analysis_file)\n",
    "\n",
    "def run_full_analysis(base_directory: str = 'full-ig',\n",
    "                     output_directory: str = 'processed_output') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run the complete analysis pipeline including verification.\n",
    "    \n",
    "    Args:\n",
    "        base_directory: Base directory for input files\n",
    "        output_directory: Directory for output files\n",
    "    Returns:\n",
    "        Dictionary containing results and session objects\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize environment\n",
    "        client, base_dir, output_dir = initialize_environment(\n",
    "            base_directory,\n",
    "            output_directory\n",
    "        )\n",
    "        \n",
    "        # Run initial analysis\n",
    "        analysis_file = run_initial_analysis(client, base_dir, output_dir)\n",
    "        if not analysis_file:\n",
    "            raise RuntimeError(\"Initial analysis failed\")\n",
    "            \n",
    "        # Run verification\n",
    "        verification_results = run_verification_phase(\n",
    "            client,\n",
    "            analysis_file,\n",
    "            output_dir\n",
    "        )\n",
    "        \n",
    "        # Setup interactive session\n",
    "        query_session = setup_interactive_session(client, analysis_file)\n",
    "        \n",
    "        return {\n",
    "            'analysis_file': analysis_file,\n",
    "            'verification_results': verification_results,\n",
    "            'query_session': query_session,\n",
    "            'output_directory': output_dir\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during full analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full analysis\n",
    "results = run_full_analysis()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nAnalysis Complete!\")\n",
    "print(f\"Technical analysis saved to: {results['analysis_file']}\")\n",
    "print(f\"Verification results available in: {results['output_directory']}\")\n",
    "print(\"\\nYou can now use the query session for additional questions:\")\n",
    "print(\"query_session = results['query_session']\")\n",
    "print(\"answer = query_session.ask_question('Your question here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the interactive session\n",
    "session = results['query_session']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask additional questions\n",
    "answer = session.ask_question(\"What are the key test scenarios for the Location resource?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the conversation\n",
    "session.save_conversation('summarized_output/interactive_session.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
