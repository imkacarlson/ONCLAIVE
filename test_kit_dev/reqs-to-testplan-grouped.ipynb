{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Test Plan Generator\n",
    "\n",
    "This notebook generates a consolidated test plan markdown file from FHIR Implementation Guide requirements. The output serves as a complete specification that can be used by an LLM to generate executable test scripts.\n",
    "\n",
    "#### What it does\n",
    "\n",
    "- Processes each requirement from a markdown input file\n",
    "- Based on the IG capability statement, generates comprehensive test specifications including:\n",
    "  - Testability assessment (Automatically testable/assertion/not testable) and level of complexity\n",
    "  - Implementation strategy with specific FHIR operations\n",
    "  - Required pre-reqs, inputs including required FHIR resources, and expected outputs\n",
    "  - Validation criteria\n",
    "- Creates a single, well-structured markdown file with a table of contents\n",
    "\n",
    "#### How to use\n",
    "\n",
    "1. **Setup**: Individual cert setup may need to be modified in `setup_clients()` function. API keys should be in .env file. Make sure you have API keys for at least one of:\n",
    "   - Anthropic Claude (`ANTHROPIC_API_KEY`)\n",
    "   - Google Gemini (`GEMINI_API_KEY`) \n",
    "   - OpenAI GPT-4 (`OPENAI_API_KEY`)\n",
    "\n",
    "2. **Input**: A markdown file with requirements in the following format:\n",
    "   ```markdown\n",
    "   # REQ-ID\n",
    "   **Summary**: Requirement summary\n",
    "   **Description**: Detailed description\n",
    "   **Verification**: Test approach\n",
    "   **Actor**: System component responsible\n",
    "   **Conformance**: SHALL/SHOULD/MAY\n",
    "   **Conditional**: True/False\n",
    "   **Source**: Original requirement sources\n",
    "   ---\n",
    "   ```\n",
    "   And an IG capability statement file in markdown format.\n",
    "\n",
    "3. **Run**: Execute the `run_test_plan_generator()` function and follow the prompts:\n",
    "   - Specify the input directory or use the default\n",
    "   - Select which requirements list file to use or provide the path to a requirements file\n",
    "   - Enter the Implementation Guide name\n",
    "   - Specify the output directory, or use the default\n",
    "   - Select which LLM to use\n",
    "\n",
    "4. **Output**: A single markdown file will be generated with the format:\n",
    "   `[llm]_test_plan_[timestamp].md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import httpx\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:08:18,570 - __main__ - INFO - Current working directory: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev\n",
      "2025-04-16 12:08:18,571 - __main__ - INFO - Project root: /Users/ceadams/Documents/onclaive/onclaive\n",
      "2025-04-16 12:08:18,571 - __main__ - INFO - Default input directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs\n",
      "2025-04-16 12:08:18,572 - __main__ - INFO - Default output directory: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_plan_output\n",
      "2025-04-16 12:08:18,572 - __main__ - INFO - Default capability statement directory: /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level to project root\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory (test_kit_dev)\n",
    "DEFAULT_INPUT_DIR = Path(PROJECT_ROOT, 'reqs_extraction', 'revised_reqs_output')  # Default input directory\n",
    "DEFAULT_OUTPUT_DIR = Path(CURRENT_DIR, 'test_plan_output')  # Default output directory\n",
    "DEFAULT_CAPABILITY_DIR = Path(PROJECT_ROOT, 'full-ig', 'markdown7_cleaned')  # Default capability statement directory\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "DEFAULT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Log directory information\n",
    "logger.info(f\"Current working directory: {CURRENT_DIR}\")\n",
    "logger.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "logger.info(f\"Default input directory: {DEFAULT_INPUT_DIR}\")\n",
    "logger.info(f\"Default output directory: {DEFAULT_OUTPUT_DIR}\")\n",
    "logger.info(f\"Default capability statement directory: {DEFAULT_CAPABILITY_DIR}\")\n",
    "\n",
    "# Function to find capability statement files\n",
    "def find_capability_statement_files(directory=DEFAULT_CAPABILITY_DIR):\n",
    "    \"\"\"Find files containing 'CapabilityStatement' in the filename\"\"\"\n",
    "    if not directory.exists():\n",
    "        logger.warning(f\"Capability statement directory {directory} does not exist\")\n",
    "        return []\n",
    "    \n",
    "    capability_files = list(directory.glob(\"*CapabilityStatement*.md\"))\n",
    "    capability_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    return capability_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Configuration & Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "API_CONFIGS = {\n",
    "    \"claude\": {\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\", \n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,  # Lower temperature for more consistent output\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 1,\n",
    "        \"delay_between_batches\": 3,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.1\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-1.5-pro-001\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 50000,\n",
    "        \"delay_between_requests\": 0.1,\n",
    "        \"timeout\": 60\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 450,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.15\n",
    "    }\n",
    "}\n",
    "\n",
    "# System prompts for test generation\n",
    "SYSTEM_PROMPT = \"\"\"You are a specialized FHIR testing engineer with expertise in healthcare interoperability.\n",
    "Your task is to analyze FHIR Implementation Guide requirements and generate practical, implementable test specifications.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        verify_path = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "        http_client = httpx.Client(\n",
    "            verify=verify_path if os.path.exists(verify_path) else True,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        gemini.configure(api_key=gemini_api_key)\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "        openai_client = OpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gpt\": openai_client,\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSOLIDATED_TEST_PLAN_WITH_CAPABILITY_PROMPT = \"\"\"\n",
    "Analyze the following FHIR Implementation Guide requirement and create a comprehensive test specification, \n",
    "taking into account the relevant Capability Statement information. \n",
    "\n",
    "For the requirement:\n",
    "{requirement}\n",
    "\n",
    "Relevant Capability Statement information for this requirement:\n",
    "{capability_info}\n",
    "\n",
    "Create a structured test specification with the following sections:\n",
    "\n",
    "1. Requirement ID\n",
    "\n",
    "2. Requirement Analysis:\n",
    "   - Testability Assessment: Classify as automatically testable, an attestation, or not testable due to being too vague or covered by the validator\n",
    "   - Complexity: Simple, Moderate, or Complex\n",
    "   - Prerequisites: Required system configurations, data, or setup\n",
    "\n",
    "3. Test Implementation Strategy:\n",
    "   - Required inputs including required FHIR resources and expected outputs for the test\n",
    "   - Required FHIR Operations: List any specific API calls/operations needed (ensure these are suported in the Capability Statement)\n",
    "   - Validation Criteria: Specific checks to verify conformance; what assertions or results should there be to indicate passing of a test\n",
    "\n",
    "Format your response as markdown with clear headers.\n",
    "\"\"\"\n",
    "\n",
    "# New prompt to identify requirement groups\n",
    "REQUIREMENT_GROUPING_PROMPT = \"\"\"\n",
    "Analyze the following requirement from a FHIR Implementation Guide and identify the most appropriate category or group it belongs to.\n",
    "\n",
    "Requirement:\n",
    "{requirement}\n",
    "\n",
    "Group the requirements by the resource profiles that make up the implementation guide from which these requirements were extracted:\n",
    "Endpoint, HealthcareService, InsurancePlan, Location, Network, Organization, OrganizationAffiliation, Practitioner, and PractionerRole\n",
    "\n",
    "- Plan-Net Endpoint: The technical details of an endpoint that can be used for electronic services, such as a portal or FHIR REST services, messaging or operations, or DIRECT messaging.\n",
    "- Plan-Net HealthcareService: The HealthCareService resource typically describes services offered by an organization/practitioner at a location. The resource may be used to encompass a variety of services covering the entire healthcare spectrum, including promotion, prevention, diagnostics, pharmacy, hospital and ambulatory care, home care, long-term care, and other health-related and community services.\n",
    "- Plan-Net InsurancePlan: An InsurancePlan is a discrete package of health insurance coverage benefits that are offered under a particular network type. A given payer’s products typically differ by network type and/or covered benefits. A plan pairs a product’s covered benefits with the particular cost sharing structure offered to a consumer. A given product may comprise multiple plans (i.e. each plan offers different cost sharing requirements for the same set of covered benefits). InsurancePlan describes a health insurance offering comprised of a list of covered benefits (i.e. the product), costs associated with those benefits (i.e. the plan), and additional information about the offering, such as who it is owned and administered by, a coverage area, contact information, etc.\n",
    "- Plan-Net Location: A Location is the physical place where healthcare services are provided, practitioners are employed, organizations are based, etc. Locations can range in scope from a room in a building to a geographic region/area.\n",
    "- Plan-Net Network: A Network refers to a healthcare provider insurance network. A healthcare provider insurance network is an aggregation of organizations and individuals that deliver a set of services across a geography through health insurance products/plans. A network is typically owned by a payer. In the PlanNet IG, individuals and organizations are represented as participants in a PLan-Net Network through the practitionerRole and Plan-Net-organizationAffiliation resources, respectively.\n",
    "- Plan-Net Organization: An organization is a formal or informal grouping of people or organizations with a common purpose, such as a company, institution, corporation, community group, or healthcare practice. Guidance: When the contact is a department name, rather than a human (e.g., patient help line), include a blank family and given name, and provide the department name in contact.name.text\n",
    "- Plan-Net OrganizationAffiliation: The OrganizationAffiliation resource describes relationships between two or more organizations, including the services one organization provides another, the location(s) where they provide services, the availability of those services, electronic endpoints, and other relevant information.\n",
    "- Plan-Net Practitioner: Practitioner is a person who is directly or indirectly involved in the provisioning of healthcare.\n",
    "- Plan-Net PractitionerRole: PractitionerRole typically describes details about a provider. When the provider is a practitioner, there may be a relationship to an organization. A provider renders services to patients at a location. Practitioner participation in healthcare provider insurance networks may be direct or through their role at an organization. PractitionerRole involves either the actual or potential (hence the optionality on Practitioner) of an individual to play this role on behalf of or under the auspices of an organization. The absence of a Practitioner resource does not imply that the Organization itself is playing the role of a Practitioner, instead it implies that that role has been established by the Organization and MAY apply that to a specific Practitioner.\n",
    "\n",
    "Return only the category name that best represents this requirement's grouping, with no additional text or explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in API_CONFIGS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    config = API_CONFIGS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= config['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= config['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:] \n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < config['delay_between_requests']:\n",
    "        time.sleep(config['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capability Statement Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_capability_statement(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a FHIR Capability Statement markdown file into a structured dictionary\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Capability Statement markdown file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing structured Capability Statement information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract resource capabilities\n",
    "    resource_sections = {}\n",
    "    \n",
    "    # Find resource sections - they typically start with \"#### ResourceName\"\n",
    "    resource_matches = re.finditer(r'#### ([A-Za-z]+)\\n', content)\n",
    "    \n",
    "    for match in resource_matches:\n",
    "        resource_name = match.group(1)\n",
    "        start_pos = match.start()\n",
    "        \n",
    "        # Find the next resource section or end of document\n",
    "        next_match = re.search(r'#### ([A-Za-z]+)\\n', content[start_pos + len(match.group(0)):])\n",
    "        if next_match:\n",
    "            end_pos = start_pos + len(match.group(0)) + next_match.start()\n",
    "            resource_section = content[start_pos:end_pos]\n",
    "        else:\n",
    "            resource_section = content[start_pos:]\n",
    "        \n",
    "        # Extract specific capabilities\n",
    "        search_params = []\n",
    "        search_param_section = re.search(r'Search Parameter Summary:.*?\\| Conformance \\| Parameter \\| Type \\| Example \\|\\n\\| --- \\| --- \\| --- \\| --- \\|(.*?)(?:\\n\\n---|\\Z)', \n",
    "                                       resource_section, re.DOTALL)\n",
    "        \n",
    "        if search_param_section:\n",
    "            param_lines = search_param_section.group(1).strip().split('\\n')\n",
    "            for line in param_lines:\n",
    "                if '|' in line:\n",
    "                    parts = [p.strip() for p in line.split('|')]\n",
    "                    if len(parts) >= 5 and parts[1] and parts[2]:\n",
    "                        conformance = parts[1].replace('**', '')\n",
    "                        param_name = parts[2]\n",
    "                        param_type = parts[3]\n",
    "                        search_params.append({\n",
    "                            'name': param_name,\n",
    "                            'type': param_type,\n",
    "                            'conformance': conformance\n",
    "                        })\n",
    "        \n",
    "        # Extract supported operations\n",
    "        operations = []\n",
    "        operations_section = re.search(r'Supported Operations:(.*?)(?:\\n\\n|\\Z)', resource_section, re.DOTALL)\n",
    "        if operations_section:\n",
    "            op_lines = operations_section.group(1).strip().split('\\n')\n",
    "            for line in op_lines:\n",
    "                if line.strip():\n",
    "                    operations.append(line.strip())\n",
    "        \n",
    "        # Extract includes and revincludes\n",
    "        includes = []\n",
    "        includes_section = re.search(r'A Server \\*\\*SHALL\\*\\* be capable of supporting the following \\_includes:(.*?)(?:\\n\\n|\\Z)', \n",
    "                                   resource_section, re.DOTALL)\n",
    "        if includes_section:\n",
    "            include_lines = includes_section.group(1).strip().split('\\n')\n",
    "            for line in include_lines:\n",
    "                if line.strip():\n",
    "                    include_match = re.search(r'([A-Za-z]+):([A-Za-z\\-]+)', line)\n",
    "                    if include_match:\n",
    "                        includes.append(f\"{include_match.group(1)}:{include_match.group(2)}\")\n",
    "        \n",
    "        revincludes = []\n",
    "        revincludes_section = re.search(r'A Server \\*\\*SHALL\\*\\* be capable of supporting the following \\_revincludes:(.*?)(?:\\n\\n|\\Z)', \n",
    "                                      resource_section, re.DOTALL)\n",
    "        if revincludes_section:\n",
    "            revinclude_lines = revincludes_section.group(1).strip().split('\\n')\n",
    "            for line in revinclude_lines:\n",
    "                if line.strip():\n",
    "                    revinclude_match = re.search(r'([A-Za-z]+):([A-Za-z\\-]+)', line)\n",
    "                    if revinclude_match:\n",
    "                        revincludes.append(f\"{revinclude_match.group(1)}:{revinclude_match.group(2)}\")\n",
    "        \n",
    "        resource_sections[resource_name] = {\n",
    "            'search_parameters': search_params,\n",
    "            'operations': operations,\n",
    "            'includes': includes,\n",
    "            'revincludes': revincludes\n",
    "        }\n",
    "    \n",
    "    # Extract general capabilities\n",
    "    general_capabilities = {}\n",
    "    general_section = re.search(r'### FHIR RESTful Capabilities(.*?)(?:###|$)', content, re.DOTALL)\n",
    "    if general_section:\n",
    "        shall_match = re.search(r'The Plan-Net Server \\*\\*SHALL\\*\\*:(.*?)(?:The Plan-Net Server \\*\\*SHOULD\\*\\*:|\\n\\n\\*\\*Security:\\*\\*|\\Z)', \n",
    "                              general_section.group(1), re.DOTALL)\n",
    "        should_match = re.search(r'The Plan-Net Server \\*\\*SHOULD\\*\\*:(.*?)(?:\\n\\n\\*\\*Security:\\*\\*|\\Z)', \n",
    "                               general_section.group(1), re.DOTALL)\n",
    "        \n",
    "        if shall_match:\n",
    "            shall_items = re.findall(r'\\d+\\.\\s*(.*?)(?:\\n\\d+\\.|\\Z)', shall_match.group(1), re.DOTALL)\n",
    "            general_capabilities['SHALL'] = [item.strip() for item in shall_items]\n",
    "        \n",
    "        if should_match:\n",
    "            should_items = re.findall(r'\\d+\\.\\s*(.*?)(?:\\n\\d+\\.|\\Z)', should_match.group(1), re.DOTALL)\n",
    "            general_capabilities['SHOULD'] = [item.strip() for item in should_items]\n",
    "    \n",
    "    return {\n",
    "        'resources': resource_sections,\n",
    "        'general_capabilities': general_capabilities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_capability_info(requirement: Dict[str, str], capability_statement: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extract relevant capability statement information for a specific requirement\n",
    "    \n",
    "    Args:\n",
    "        requirement: Requirement dictionary\n",
    "        capability_statement: Parsed capability statement\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with relevant capability information\n",
    "    \"\"\"\n",
    "    # Determine which resource types are relevant to this requirement\n",
    "    requirement_text = f\"{requirement.get('description', '')} {requirement.get('summary', '')}\"\n",
    "    resource_types = []\n",
    "    \n",
    "    # Common FHIR resource types\n",
    "    fhir_resources = [\n",
    "        \"Patient\", \"Practitioner\", \"Organization\", \"Location\", \"Endpoint\", \n",
    "        \"HealthcareService\", \"PractitionerRole\", \"OrganizationAffiliation\",\n",
    "        \"InsurancePlan\", \"Network\"\n",
    "    ]\n",
    "    \n",
    "    # Check if requirement mentions specific resources\n",
    "    for resource in fhir_resources:\n",
    "        if resource in requirement_text:\n",
    "            resource_types.append(resource)\n",
    "    \n",
    "    # If no specific resources found, check for general requirements\n",
    "    if not resource_types:\n",
    "        # If it's a server requirement\n",
    "        if \"Server\" in requirement.get('actor', ''):\n",
    "            resource_types = [\"General Server Capabilities\"]\n",
    "        # If it's a client requirement\n",
    "        elif \"Client\" in requirement.get('actor', '') or \"Application\" in requirement.get('actor', ''):\n",
    "            resource_types = [\"General Client Capabilities\"]\n",
    "    \n",
    "    # Build relevant capability information\n",
    "    relevant_info = \"### Applicable Capability Statement Information\\n\\n\"\n",
    "    \n",
    "    # Add general capabilities\n",
    "    relevant_info += \"#### General Capabilities\\n\"\n",
    "    if \"general_capabilities\" in capability_statement:\n",
    "        for level in [\"SHALL\", \"SHOULD\"]:\n",
    "            if level in capability_statement[\"general_capabilities\"]:\n",
    "                relevant_info += f\"\\n**{level}**:\\n\"\n",
    "                for item in capability_statement[\"general_capabilities\"][level]:\n",
    "                    relevant_info += f\"- {item}\\n\"\n",
    "    \n",
    "    # Add resource-specific capabilities\n",
    "    for resource_type in resource_types:\n",
    "        if resource_type in capability_statement.get(\"resources\", {}):\n",
    "            resource_info = capability_statement[\"resources\"][resource_type]\n",
    "            \n",
    "            relevant_info += f\"\\n#### {resource_type} Resource Capabilities\\n\"\n",
    "            \n",
    "            # Add search parameters\n",
    "            if resource_info.get(\"search_parameters\"):\n",
    "                relevant_info += \"\\n**Supported Search Parameters**:\\n\"\n",
    "                for param in resource_info[\"search_parameters\"]:\n",
    "                    relevant_info += f\"- {param['name']} ({param['type']}): {param['conformance']}\\n\"\n",
    "            \n",
    "            # Add operations\n",
    "            if resource_info.get(\"operations\"):\n",
    "                relevant_info += \"\\n**Supported Operations**:\\n\"\n",
    "                for op in resource_info[\"operations\"]:\n",
    "                    relevant_info += f\"- {op}\\n\"\n",
    "            \n",
    "            # Add includes\n",
    "            if resource_info.get(\"includes\"):\n",
    "                relevant_info += \"\\n**Supported _includes**:\\n\"\n",
    "                for include in resource_info[\"includes\"]:\n",
    "                    relevant_info += f\"- {include}\\n\"\n",
    "            \n",
    "            # Add revincludes\n",
    "            if resource_info.get(\"revincludes\"):\n",
    "                relevant_info += \"\\n**Supported _revincludes**:\\n\"\n",
    "                for revinclude in resource_info[\"revincludes\"]:\n",
    "                    relevant_info += f\"- {revinclude}\\n\"\n",
    "    \n",
    "    return relevant_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_requirements_file(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse an INCOSE requirements markdown file into a structured list of requirements\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the requirements markdown file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing structured requirement information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by requirement sections (separated by ---)\n",
    "    req_sections = content.split('---')\n",
    "    \n",
    "    requirements = []\n",
    "    for section in req_sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "            \n",
    "        # Parse requirement data\n",
    "        req_data = {}\n",
    "        \n",
    "        # Extract ID from format \"# REQ-XX\"\n",
    "        id_match = re.search(r'#\\s+([A-Z0-9\\-]+)', section)\n",
    "        if id_match:\n",
    "            req_data['id'] = id_match.group(1)\n",
    "        \n",
    "        # Extract other fields\n",
    "        for field in ['Summary', 'Description', 'Verification', 'Actor', 'Conformance', 'Conditional', 'Source']:\n",
    "            pattern = rf'\\*\\*{field}\\*\\*:\\s*(.*?)(?:\\n\\*\\*|\\n---|\\\\Z)'\n",
    "            field_match = re.search(pattern, section, re.DOTALL)\n",
    "            if field_match:\n",
    "                req_data[field.lower()] = field_match.group(1).strip()\n",
    "        \n",
    "        if req_data:\n",
    "            requirements.append(req_data)\n",
    "    \n",
    "    return requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_requirement_for_prompt(requirement: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Format a requirement dictionary into markdown for inclusion in prompts\n",
    "    \n",
    "    Args:\n",
    "        requirement: Requirement dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Formatted markdown string\n",
    "    \"\"\"\n",
    "    formatted = f\"# {requirement.get('id', 'UNKNOWN-ID')}\\n\"\n",
    "    formatted += f\"**Summary**: {requirement.get('summary', '')}\\n\"\n",
    "    formatted += f\"**Description**: {requirement.get('description', '')}\\n\"\n",
    "    formatted += f\"**Verification**: {requirement.get('verification', '')}\\n\"\n",
    "    formatted += f\"**Actor**: {requirement.get('actor', '')}\\n\"\n",
    "    formatted += f\"**Conformance**: {requirement.get('conformance', '')}\\n\"\n",
    "    formatted += f\"**Conditional**: {requirement.get('conditional', '')}\\n\"\n",
    "    formatted += f\"**Source**: {requirement.get('source', '')}\\n\"\n",
    "    \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_requirement_group(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    requirement: Dict[str, str],\n",
    "    rate_limit_func\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Identify the appropriate group for a requirement using LLM\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirement: Requirement dictionary\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Identified group name\n",
    "    \"\"\"\n",
    "    # Use actor field as a possible hint if available\n",
    "    actor = requirement.get('actor', '').strip()\n",
    "    if actor and len(actor) > 3 and actor not in ['System', 'User', 'All']:\n",
    "        # Simple heuristic - if actor is specific enough, it might be a good grouping\n",
    "        possible_groups = ['Client', 'Server', 'Patient', 'Practitioner', 'Organization', 'HealthcareService']\n",
    "        for group in possible_groups:\n",
    "            if group.lower() in actor.lower():\n",
    "                return group\n",
    "    \n",
    "    # Use LLM to identify group\n",
    "    logger.info(f\"Identifying group for requirement {requirement.get('id', 'unknown')} using {api_type}...\")\n",
    "    \n",
    "    # Format requirement as markdown\n",
    "    formatted_req = format_requirement_for_prompt(requirement)\n",
    "    \n",
    "    # Create prompt with the requirement\n",
    "    prompt = REQUIREMENT_GROUPING_PROMPT.format(requirement=formatted_req)\n",
    "    \n",
    "    # Make the API request with simplified system prompt\n",
    "    group_system_prompt = \"You are a FHIR expert who categorizes requirements by their functional or resource type.\"\n",
    "    group_name = make_llm_request(client, api_type, prompt, group_system_prompt, rate_limit_func).strip()\n",
    "    \n",
    "    # Clean up response (in case model returns extra text)\n",
    "    if '\\n' in group_name:\n",
    "        group_name = group_name.split('\\n')[0].strip()\n",
    "    \n",
    "    return group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def make_llm_request(client, api_type: str, prompt: str, system_prompt: str, rate_limit_func) -> str:\n",
    "    \"\"\"Make rate-limited API request with retries\"\"\"\n",
    "    rate_limit_func()\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }],\n",
    "                system=system_prompt\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            response = client.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            elif response.candidates:\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Plan Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_specification_with_capability(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    requirement: Dict[str, str],\n",
    "    capability_statement: Dict[str, Any],\n",
    "    rate_limit_func\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive test specification for a single requirement, considering capability statement\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirement: Requirement dictionary\n",
    "        capability_statement: Parsed capability statement\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Test specification for the requirement\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating test specification for {requirement.get('id', 'unknown')} using {api_type}...\")\n",
    "    \n",
    "    # Format requirement as markdown\n",
    "    formatted_req = format_requirement_for_prompt(requirement)\n",
    "    \n",
    "    # Extract relevant capability information\n",
    "    capability_info = extract_relevant_capability_info(requirement, capability_statement)\n",
    "    \n",
    "    # Create prompt with the requirement and capability information\n",
    "    prompt = CONSOLIDATED_TEST_PLAN_WITH_CAPABILITY_PROMPT.format(\n",
    "        requirement=formatted_req,\n",
    "        capability_info=capability_info\n",
    "    )\n",
    "    \n",
    "    # Make the API request\n",
    "    return make_llm_request(client, api_type, prompt, SYSTEM_PROMPT, rate_limit_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consolidated_test_plan(\n",
    "    api_type: str,\n",
    "    requirements_file: str,\n",
    "    capability_statement_file: str = None,\n",
    "    ig_name: str = \"FHIR Implementation Guide\",\n",
    "    output_dir: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process requirements and generate a consolidated test plan\n",
    "    \n",
    "    Args:\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirements_file: Path to requirements markdown file\n",
    "        capability_statement_file: Path to capability statement markdown file (optional)\n",
    "        ig_name: Name of the Implementation Guide\n",
    "        output_dir: Directory for output files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing path to output file\n",
    "    \"\"\"\n",
    "    # Use default output directory if none provided\n",
    "    if output_dir is None:\n",
    "        output_dir = DEFAULT_OUTPUT_DIR\n",
    "    else:\n",
    "        # Ensure output_dir is a Path object\n",
    "        if not isinstance(output_dir, Path):\n",
    "            output_dir = Path(output_dir)\n",
    "    \n",
    "    logger.info(f\"Starting test plan generation with {api_type} for {ig_name}\")\n",
    "    \n",
    "    # Initialize API clients and rate limiters\n",
    "    clients = setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = API_CONFIGS[api_type]\n",
    "    rate_limiter = create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # Parse requirements from file\n",
    "        requirements = parse_requirements_file(requirements_file)\n",
    "        logger.info(f\"Parsed {len(requirements)} requirements from {requirements_file}\")\n",
    "        \n",
    "        # Parse capability statement if provided\n",
    "        capability_statement = None\n",
    "        if capability_statement_file and os.path.exists(capability_statement_file):\n",
    "            capability_statement = parse_capability_statement(capability_statement_file)\n",
    "            logger.info(f\"Parsed capability statement from {capability_statement_file}\")\n",
    "        \n",
    "        # Identify groups for each requirement\n",
    "        req_groups = {}\n",
    "        for req in requirements:\n",
    "            req_id = req.get('id', 'UNKNOWN-ID')\n",
    "            req_groups[req_id] = identify_requirement_group(client, api_type, req, check_limits)\n",
    "            # Add small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Group requirements by identified category\n",
    "        grouped_requirements = defaultdict(list)\n",
    "        for req in requirements:\n",
    "            req_id = req.get('id', 'UNKNOWN-ID')\n",
    "            group = req_groups.get(req_id, 'Uncategorized')\n",
    "            grouped_requirements[group].append(req)\n",
    "            \n",
    "        # Log the grouping results\n",
    "        logger.info(f\"Requirements grouped into {len(grouped_requirements)} categories\")\n",
    "        for group, reqs in grouped_requirements.items():\n",
    "            logger.info(f\"Group '{group}': {len(reqs)} requirements\")\n",
    "        \n",
    "        # Update output file path to use Path object\n",
    "        test_plan_path = output_dir / f\"{api_type}_test_plan_{timestamp}.md\"\n",
    "        \n",
    "        # Initialize test plan content\n",
    "        test_plan = f\"# Consolidated Test Plan for {ig_name}\\n\\n\"\n",
    "        test_plan += f\"## Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "        \n",
    "        # Add capability statement reference if used\n",
    "        if capability_statement:\n",
    "            test_plan += \"## Capability Statement\\n\\n\"\n",
    "            test_plan += f\"This test plan incorporates constraints and requirements from the {ig_name} Capability Statement.\\n\\n\"\n",
    "        \n",
    "        test_plan += \"## Table of Contents\\n\\n\"\n",
    "        \n",
    "        # Add group headers to TOC\n",
    "        for group in sorted(grouped_requirements.keys()):\n",
    "            test_plan += f\"- [{group}](#{group.lower().replace(' ', '-')})\\n\"\n",
    "            for req in grouped_requirements[group]:\n",
    "                req_id = req.get('id', 'UNKNOWN-ID')\n",
    "                req_summary = req.get('summary', 'No summary')\n",
    "                test_plan += f\"  - [{req_id}: {req_summary}](#{req_id.lower()})\\n\"\n",
    "        \n",
    "        # Process each group and its requirements\n",
    "        test_plan += \"\\n## Test Specifications\\n\\n\"\n",
    "        \n",
    "        for group in sorted(grouped_requirements.keys()):\n",
    "            # Add group header with anchor for TOC linking\n",
    "            test_plan += f\"<a id='{group.lower().replace(' ', '-')}'></a>\\n\\n\"\n",
    "            test_plan += f\"## {group}\\n\\n\"\n",
    "            \n",
    "            # Process each requirement in the group\n",
    "            for i, req in enumerate(grouped_requirements[group]):\n",
    "                req_id = req.get('id', 'UNKNOWN-ID')\n",
    "                logger.info(f\"Processing requirement for group '{group}': {req_id}\")\n",
    "                \n",
    "                # Generate test specification with capability statement if available\n",
    "                if capability_statement:\n",
    "                    test_spec = generate_test_specification_with_capability(\n",
    "                        client, api_type, req, capability_statement, check_limits\n",
    "                    )\n",
    "                else:\n",
    "                    # Define a fallback function if needed\n",
    "                    def generate_test_specification(client, api_type, req, check_limits):\n",
    "                        logger.info(f\"Generating basic test specification for {req.get('id', 'unknown')} using {api_type}...\")\n",
    "                        formatted_req = format_requirement_for_prompt(req)\n",
    "                        prompt = f\"\"\"\n",
    "                        Create a test specification for this FHIR Implementation Guide requirement:\n",
    "                        \n",
    "                        {formatted_req}\n",
    "                        \n",
    "                        Include these sections:\n",
    "                        1. Testability Assessment\n",
    "                        2. Complexity\n",
    "                        3. Prerequisites\n",
    "                        4. Required inputs and outputs\n",
    "                        5. Required FHIR Operations\n",
    "                        6. Validation Criteria\n",
    "                        \n",
    "                        Format as markdown with clear headers.\n",
    "                        \"\"\"\n",
    "                        return make_llm_request(client, api_type, prompt, SYSTEM_PROMPT, check_limits)\n",
    "                    \n",
    "                    test_spec = generate_test_specification(client, api_type, req, check_limits)\n",
    "                \n",
    "                # Add to test plan content with proper anchor for TOC linking\n",
    "                test_plan += f\"<a id='{req_id.lower()}'></a>\\n\\n\"\n",
    "                test_plan += f\"### {req_id}: {req.get('summary', 'No summary')}\\n\\n\"\n",
    "                test_plan += f\"**Description**: {req.get('description', '')}\\n\\n\"\n",
    "                test_plan += f\"**Actor**: {req.get('actor', '')}\\n\\n\"\n",
    "                test_plan += f\"**Conformance**: {req.get('conformance', '')}\\n\\n\"\n",
    "                test_plan += f\"{test_spec}\\n\\n\"\n",
    "                test_plan += \"---\\n\\n\"\n",
    "                \n",
    "                # Add delay between requests\n",
    "                if i < len(grouped_requirements[group]) - 1:  # No need to delay after the last request\n",
    "                    time.sleep(config[\"delay_between_chunks\"])\n",
    "            \n",
    "            # Add spacing between groups\n",
    "            test_plan += \"\\n\\n\"\n",
    "        \n",
    "        # Save consolidated test plan\n",
    "        with open(test_plan_path, 'w') as f:\n",
    "            f.write(test_plan)\n",
    "        logger.info(f\"Consolidated test plan saved to {test_plan_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"requirements_count\": len(requirements),\n",
    "            \"group_count\": len(grouped_requirements),\n",
    "            \"test_plan_path\": str(test_plan_path)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing requirements: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_plan_generator():\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get input from user or set default values\n",
    "    print(\"\\nFHIR IG Test Plan Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get input directory or use default\n",
    "    input_dir = input(f\"Enter input directory path (default '{DEFAULT_INPUT_DIR}'): \") or str(DEFAULT_INPUT_DIR)\n",
    "    input_dir_path = Path(input_dir)\n",
    "    \n",
    "    if not input_dir_path.exists():\n",
    "        print(f\"Warning: Input directory {input_dir} does not exist.\")\n",
    "        requirements_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    else:\n",
    "        # List all markdown files in the input directory\n",
    "        md_files = list(input_dir_path.glob(\"*.md\"))\n",
    "        \n",
    "        if md_files:\n",
    "            # Sort files by modification time (newest first)\n",
    "            md_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "            \n",
    "            # Show only the 10 most recent files\n",
    "            recent_files = md_files[:10]\n",
    "            \n",
    "            print(\"\\nMost recent files:\")\n",
    "            for idx, file in enumerate(recent_files, 1):\n",
    "                # Format the modification time as part of the display\n",
    "                mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "            \n",
    "            # Let user select from the list, see more files, or enter a custom path\n",
    "            print(\"\\nOptions:\")\n",
    "            print(\"- Select a number (1-10) to choose a file\")\n",
    "            print(\"- Enter 'all' to see all files\")\n",
    "            print(\"- Enter a full path to use a specific file\")\n",
    "            \n",
    "            selection = input(\"\\nYour selection: \")\n",
    "            \n",
    "            if selection.lower() == 'all':\n",
    "                # Show all files with pagination\n",
    "                all_files = md_files\n",
    "                page_size = 20\n",
    "                total_pages = (len(all_files) + page_size - 1) // page_size\n",
    "                \n",
    "                current_page = 1\n",
    "                while current_page <= total_pages:\n",
    "                    start_idx = (current_page - 1) * page_size\n",
    "                    end_idx = min(start_idx + page_size, len(all_files))\n",
    "                    \n",
    "                    print(f\"\\nAll files (page {current_page}/{total_pages}):\")\n",
    "                    for idx, file in enumerate(all_files[start_idx:end_idx], start_idx + 1):\n",
    "                        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                        print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "                    \n",
    "                    if current_page < total_pages:\n",
    "                        next_action = input(\"\\nPress Enter for next page, 'q' to select, or enter a number to choose a file: \")\n",
    "                        if next_action.lower() == 'q':\n",
    "                            break\n",
    "                        elif next_action.isdigit() and 1 <= int(next_action) <= len(all_files):\n",
    "                            requirements_file = str(all_files[int(next_action) - 1])\n",
    "                            break\n",
    "                        else:\n",
    "                            current_page += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if 'requirements_file' not in locals():\n",
    "                    # If we went through all pages without selection\n",
    "                    file_number = input(\"\\nEnter the file number to process: \")\n",
    "                    if file_number.isdigit() and 1 <= int(file_number) <= len(all_files):\n",
    "                        requirements_file = str(all_files[int(file_number) - 1])\n",
    "                    else:\n",
    "                        requirements_file = file_number  # Treat as a custom path\n",
    "            \n",
    "            elif selection.isdigit() and 1 <= int(selection) <= len(recent_files):\n",
    "                requirements_file = str(recent_files[int(selection) - 1])\n",
    "            else:\n",
    "                requirements_file = selection  # Treat as a custom path\n",
    "        else:\n",
    "            print(f\"No markdown files found in {input_dir}\")\n",
    "            requirements_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    \n",
    "    # Check if requirements file exists\n",
    "    if not os.path.exists(requirements_file):\n",
    "        logger.error(f\"Requirements file not found: {requirements_file}\")\n",
    "        print(f\"Error: Requirements file not found at {requirements_file}\")\n",
    "        return\n",
    "    \n",
    "    # Find capability statement files\n",
    "    capability_files = find_capability_statement_files()\n",
    "    \n",
    "    # Get capability statement file path or select from found files\n",
    "    if capability_files:\n",
    "        print(\"\\nFound capability statement files:\")\n",
    "        for idx, file in enumerate(capability_files, 1):\n",
    "            mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "            print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "        \n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"- Select a number to choose a capability statement file\")\n",
    "        print(\"- Press Enter to use the most recent file\")\n",
    "        print(\"- Enter 'none' to skip using a capability statement\")\n",
    "        print(\"- Enter a full path to use a specific file\")\n",
    "        \n",
    "        cap_selection = input(\"\\nYour selection: \")\n",
    "        \n",
    "        if not cap_selection:\n",
    "            # Use the most recent file\n",
    "            capability_statement_file = str(capability_files[0])\n",
    "        elif cap_selection.lower() == 'none':\n",
    "            capability_statement_file = None\n",
    "        elif cap_selection.isdigit() and 1 <= int(cap_selection) <= len(capability_files):\n",
    "            capability_statement_file = str(capability_files[int(cap_selection) - 1])\n",
    "        else:\n",
    "            capability_statement_file = cap_selection  # Treat as a custom path\n",
    "    else:\n",
    "        # No capability statement files found automatically\n",
    "        capability_statement_file = input(\"\\nEnter path to Capability Statement markdown file (optional, press Enter to skip): \")\n",
    "        if not capability_statement_file:\n",
    "            capability_statement_file = None\n",
    "    \n",
    "    # Verify capability statement file exists if provided\n",
    "    if capability_statement_file and not os.path.exists(capability_statement_file):\n",
    "        logger.warning(f\"Capability Statement file not found: {capability_statement_file}\")\n",
    "        print(f\"Warning: Capability Statement file not found at {capability_statement_file}. Proceeding without it.\")\n",
    "        capability_statement_file = None\n",
    "    \n",
    "    # Get IG name\n",
    "    ig_name = input(\"\\nEnter Implementation Guide name (default 'FHIR Implementation Guide'): \") or \"FHIR Implementation Guide\"\n",
    "    \n",
    "    # Get output directory or use default\n",
    "    output_dir = input(f\"\\nEnter output directory path (default '{DEFAULT_OUTPUT_DIR}'): \") or str(DEFAULT_OUTPUT_DIR)\n",
    "    output_dir_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Let user select the API\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    print(f\"\\nProcessing requirements with {api_type.capitalize()}...\")\n",
    "    if capability_statement_file:\n",
    "        print(f\"Including Capability Statement from {capability_statement_file}\")\n",
    "    print(f\"This may take several minutes depending on the number of requirements.\")\n",
    "    \n",
    "    try:\n",
    "        # Process requirements and generate test plan\n",
    "        result = generate_consolidated_test_plan(\n",
    "            api_type=api_type,\n",
    "            requirements_file=requirements_file,\n",
    "            capability_statement_file=capability_statement_file,\n",
    "            ig_name=ig_name,\n",
    "            output_dir=output_dir_path\n",
    "        )\n",
    "        \n",
    "        # Output results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Test plan generation complete!\")\n",
    "        print(f\"Processed {result['requirements_count']} requirements\")\n",
    "        print(f\"Grouped into {result['group_count']} categories\")\n",
    "        print(f\"Consolidated test plan: {result['test_plan_path']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FHIR IG Test Plan Generator\n",
      "==================================================\n",
      "\n",
      "Most recent files:\n",
      "1. claude_reqs_list_v2_20250416_114610.md (2025-04-16 11:46)\n",
      "2. claude_reqs_list_v220250416_103916.md (2025-04-16 10:39)\n",
      "3. claude_reqs_list_v220250415_110350.md (2025-04-15 11:03)\n",
      "4. plan_net_actual.md (2025-04-09 11:29)\n",
      "5. refined_requirements_gemini_20250402_145827.md (2025-04-02 14:58)\n",
      "6. refined_requirements_claude_20250402_144658.md (2025-04-02 14:46)\n",
      "7. refined_requirements_gpt_20250402_134825.md (2025-04-02 13:48)\n",
      "8. refined_requirements_gemini_20250402_134632.md (2025-04-02 13:46)\n",
      "9. refined_requirements_gemini_20250402_124132.md (2025-04-02 12:41)\n",
      "10. refined_requirements_gemini_20250402_123833.md (2025-04-02 12:38)\n",
      "\n",
      "Options:\n",
      "- Select a number (1-10) to choose a file\n",
      "- Enter 'all' to see all files\n",
      "- Enter a full path to use a specific file\n",
      "\n",
      "Found capability statement files:\n",
      "1. CapabilityStatement_plan_net.md (2025-03-24 16:11)\n",
      "\n",
      "Options:\n",
      "- Select a number to choose a capability statement file\n",
      "- Press Enter to use the most recent file\n",
      "- Enter 'none' to skip using a capability statement\n",
      "- Enter a full path to use a specific file\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:08:35,048 - __main__ - INFO - Starting test plan generation with claude for Plan Net\n",
      "2025-04-16 12:08:35,077 - __main__ - INFO - Parsed 10 requirements from /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs/claude_reqs_list_v2_20250416_114610.md\n",
      "2025-04-16 12:08:35,078 - __main__ - INFO - Parsed capability statement from /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned/CapabilityStatement_plan_net.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing requirements with Claude...\n",
      "Including Capability Statement from /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned/CapabilityStatement_plan_net.md\n",
      "This may take several minutes depending on the number of requirements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:08:37,093 - __main__ - INFO - Identifying group for requirement REQ-05 using claude...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the generator\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mrun_test_plan_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mrun_test_plan_generator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis may take several minutes depending on the number of requirements.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# Process requirements and generate test plan\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     result = \u001b[43mgenerate_consolidated_test_plan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequirements_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequirements_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapability_statement_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapability_statement_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir_path\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# Output results\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mgenerate_consolidated_test_plan\u001b[39m\u001b[34m(api_type, requirements_file, capability_statement_file, ig_name, output_dir)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m req \u001b[38;5;129;01min\u001b[39;00m requirements:\n\u001b[32m     58\u001b[39m     req_id = req.get(\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mUNKNOWN-ID\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     req_groups[req_id] = \u001b[43midentify_requirement_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_limits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# Add small delay to avoid rate limiting\u001b[39;00m\n\u001b[32m     61\u001b[39m     time.sleep(\u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36midentify_requirement_group\u001b[39m\u001b[34m(client, api_type, requirement, rate_limit_func)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Make the API request with simplified system prompt\u001b[39;00m\n\u001b[32m     38\u001b[39m group_system_prompt = \u001b[33m\"\u001b[39m\u001b[33mYou are a FHIR expert who categorizes requirements by their functional or resource type.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m group_name = \u001b[43mmake_llm_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_system_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_limit_func\u001b[49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Clean up response (in case model returns extra text)\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m group_name:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mmake_llm_request\u001b[39m\u001b[34m(client, api_type, prompt, system_prompt, rate_limit_func)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m api_type == \u001b[33m\"\u001b[39m\u001b[33mclaude\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.content[\u001b[32m0\u001b[39m].text\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m api_type == \u001b[33m\"\u001b[39m\u001b[33mgemini\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_utils/_utils.py:274\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/resources/messages.py:888\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    882\u001b[39m     warnings.warn(\n\u001b[32m    883\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    884\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    885\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    886\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1277\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1265\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1273\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1274\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1275\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1276\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:954\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    952\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:990\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m    987\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, request.method, request.url)\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    996\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_client.py:926\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    924\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_client.py:954\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    951\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    960\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_client.py:991\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    989\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m     )\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1031\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    223\u001b[39m req = httpcore.Request(\n\u001b[32m    224\u001b[39m     method=request.method,\n\u001b[32m    225\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m     extensions=request.extensions,\n\u001b[32m    234\u001b[39m )\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    241\u001b[39m     status_code=resp.status,\n\u001b[32m    242\u001b[39m     headers=resp.headers,\n\u001b[32m    243\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    244\u001b[39m     extensions=resp.extensions,\n\u001b[32m    245\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the generator\n",
    "if __name__ == \"__main__\":\n",
    "    run_test_plan_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
