{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Test Plan Generator\n",
    "\n",
    "This notebook generates a consolidated test plan markdown file from FHIR Implementation Guide requirements. The output serves as a complete specification that can be used by an LLM to generate executable test scripts.\n",
    "\n",
    "#### What it does\n",
    "\n",
    "- Processes each requirement from a markdown input file\n",
    "- Based on the IG capability statement, generates comprehensive test specifications including:\n",
    "  - Testability assessment (Automatically testable/assertion/not testable) and level of complexity\n",
    "  - Implementation strategy with specific FHIR operations\n",
    "  - Required pre-reqs, inputs including required FHIR resources, and expected outputs\n",
    "  - Validation criteria\n",
    "- Creates a single, well-structured markdown file with a table of contents\n",
    "\n",
    "#### How to use\n",
    "\n",
    "1. **Setup**: Individual cert setup may need to be modified in `setup_clients()` function. API keys should be in .env file. Make sure you have API keys for at least one of:\n",
    "   - Anthropic Claude (`ANTHROPIC_API_KEY`)\n",
    "   - Google Gemini (`GEMINI_API_KEY`) \n",
    "   - OpenAI GPT-4 (`OPENAI_API_KEY`)\n",
    "\n",
    "2. **Input**: A markdown file with requirements in the following format:\n",
    "   ```markdown\n",
    "   # REQ-ID\n",
    "   **Summary**: Requirement summary\n",
    "   **Description**: Detailed description\n",
    "   **Verification**: Test approach\n",
    "   **Actor**: System component responsible\n",
    "   **Conformance**: SHALL/SHOULD/MAY\n",
    "   **Conditional**: True/False\n",
    "   **Source**: Original requirement sources\n",
    "   ---\n",
    "   ```\n",
    "   And an IG capability statement file in markdown format.\n",
    "\n",
    "3. **Run**: Execute the `run_test_plan_generator()` function and follow the prompts:\n",
    "   - Select which LLM to use\n",
    "   - Provide the path to your requirements file\n",
    "   - Enter the Implementation Guide name\n",
    "   - Specify the output directory, or use the default\n",
    "\n",
    "4. **Output**: A single markdown file will be generated with the format:\n",
    "   `[llm]_test_plan_[timestamp].md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import httpx\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level to project root\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, '/reqs_extraction/test_plan_output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API Configuration\n",
    "API_CONFIGS = {\n",
    "    \"claude\": {\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\", \n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,  # Lower temperature for more consistent output\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 1,\n",
    "        \"delay_between_batches\": 3,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.1\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-1.5-pro-001\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 50000,\n",
    "        \"delay_between_requests\": 0.1,\n",
    "        \"timeout\": 60\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 450,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.15\n",
    "    }\n",
    "}\n",
    "\n",
    "# System prompts for test generation\n",
    "SYSTEM_PROMPT = \"\"\"You are a specialized FHIR testing engineer with expertise in healthcare interoperability.\n",
    "Your task is to analyze FHIR Implementation Guide requirements and generate practical, implementable test specifications.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSOLIDATED_TEST_PLAN_WITH_CAPABILITY_PROMPT = \"\"\"\n",
    "Analyze the following FHIR Implementation Guide requirement and create a comprehensive test specification, \n",
    "taking into account the relevant Capability Statement information. \n",
    "\n",
    "For the requirement:\n",
    "{requirement}\n",
    "\n",
    "Relevant Capability Statement information for this requirement:\n",
    "{capability_info}\n",
    "\n",
    "Create a structured test specification with the following sections:\n",
    "\n",
    "1. Requirement ID\n",
    "\n",
    "2. Requirement Analysis:\n",
    "   - Testability Assessment: Classify as automatically testable, an attestation, or not testable due to being too vague or covered by the validator\n",
    "   - Complexity: Simple, Moderate, or Complex\n",
    "   - Prerequisites: Required system configurations, data, or setup\n",
    "\n",
    "3. Test Implementation Strategy:\n",
    "   - Required inputs including required FHIR resources and expected outputs for the test\n",
    "   - Required FHIR Operations: List any specific API calls/operations needed (ensure these are suported in the Capability Statement)\n",
    "   - Validation Criteria: Specific checks to verify conformance; what assertions or results should there be to indicate passing of a test\n",
    "\n",
    "Format your response as markdown with clear headers.\n",
    "\"\"\"\n",
    "\n",
    "# New prompt to identify requirement groups\n",
    "REQUIREMENT_GROUPING_PROMPT = \"\"\"\n",
    "Analyze the following requirement from a FHIR Implementation Guide and identify the most appropriate category or group it belongs to.\n",
    "\n",
    "Requirement:\n",
    "{requirement}\n",
    "\n",
    "Group the requirements by the resource profiles that make up the implementation guide from which these requirements were extracted:\n",
    "Endpoint, HealthcareService, InsurancePlan, Location, Network, Organization, OrganizationAffiliation, Practitioner, and PractionerRole\n",
    "\n",
    "- Plan-Net Endpoint: The technical details of an endpoint that can be used for electronic services, such as a portal or FHIR REST services, messaging or operations, or DIRECT messaging.\n",
    "- Plan-Net HealthcareService: The HealthCareService resource typically describes services offered by an organization/practitioner at a location. The resource may be used to encompass a variety of services covering the entire healthcare spectrum, including promotion, prevention, diagnostics, pharmacy, hospital and ambulatory care, home care, long-term care, and other health-related and community services.\n",
    "- Plan-Net InsurancePlan: An InsurancePlan is a discrete package of health insurance coverage benefits that are offered under a particular network type. A given payer’s products typically differ by network type and/or covered benefits. A plan pairs a product’s covered benefits with the particular cost sharing structure offered to a consumer. A given product may comprise multiple plans (i.e. each plan offers different cost sharing requirements for the same set of covered benefits). InsurancePlan describes a health insurance offering comprised of a list of covered benefits (i.e. the product), costs associated with those benefits (i.e. the plan), and additional information about the offering, such as who it is owned and administered by, a coverage area, contact information, etc.\n",
    "- Plan-Net Location: A Location is the physical place where healthcare services are provided, practitioners are employed, organizations are based, etc. Locations can range in scope from a room in a building to a geographic region/area.\n",
    "- Plan-Net Network: A Network refers to a healthcare provider insurance network. A healthcare provider insurance network is an aggregation of organizations and individuals that deliver a set of services across a geography through health insurance products/plans. A network is typically owned by a payer. In the PlanNet IG, individuals and organizations are represented as participants in a PLan-Net Network through the practitionerRole and Plan-Net-organizationAffiliation resources, respectively.\n",
    "- Plan-Net Organization: An organization is a formal or informal grouping of people or organizations with a common purpose, such as a company, institution, corporation, community group, or healthcare practice. Guidance: When the contact is a department name, rather than a human (e.g., patient help line), include a blank family and given name, and provide the department name in contact.name.text\n",
    "- Plan-Net OrganizationAffiliation: The OrganizationAffiliation resource describes relationships between two or more organizations, including the services one organization provides another, the location(s) where they provide services, the availability of those services, electronic endpoints, and other relevant information.\n",
    "- Plan-Net Practitioner: Practitioner is a person who is directly or indirectly involved in the provisioning of healthcare.\n",
    "- Plan-Net PractitionerRole: PractitionerRole typically describes details about a provider. When the provider is a practitioner, there may be a relationship to an organization. A provider renders services to patients at a location. Practitioner participation in healthcare provider insurance networks may be direct or through their role at an organization. PractitionerRole involves either the actual or potential (hence the optionality on Practitioner) of an individual to play this role on behalf of or under the auspices of an organization. The absence of a Practitioner resource does not imply that the Organization itself is playing the role of a Practitioner, instead it implies that that role has been established by the Organization and MAY apply that to a specific Practitioner.\n",
    "\n",
    "Return only the category name that best represents this requirement's grouping, with no additional text or explanation. DO NOT group by actor type (e.g. client and server), only resource type.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in API_CONFIGS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    config = API_CONFIGS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= config['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= config['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:] \n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < config['delay_between_requests']:\n",
    "        time.sleep(config['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        verify_path = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "        http_client = httpx.Client(\n",
    "            verify=verify_path if os.path.exists(verify_path) else True,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        gemini.configure(api_key=gemini_api_key)\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "        openai_client = OpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gpt\": openai_client,\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_capability_statement(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a FHIR Capability Statement markdown file into a structured dictionary\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Capability Statement markdown file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing structured Capability Statement information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract resource capabilities\n",
    "    resource_sections = {}\n",
    "    \n",
    "    # Find resource sections - they typically start with \"#### ResourceName\"\n",
    "    resource_matches = re.finditer(r'#### ([A-Za-z]+)\\n', content)\n",
    "    \n",
    "    for match in resource_matches:\n",
    "        resource_name = match.group(1)\n",
    "        start_pos = match.start()\n",
    "        \n",
    "        # Find the next resource section or end of document\n",
    "        next_match = re.search(r'#### ([A-Za-z]+)\\n', content[start_pos + len(match.group(0)):])\n",
    "        if next_match:\n",
    "            end_pos = start_pos + len(match.group(0)) + next_match.start()\n",
    "            resource_section = content[start_pos:end_pos]\n",
    "        else:\n",
    "            resource_section = content[start_pos:]\n",
    "        \n",
    "        # Extract specific capabilities\n",
    "        search_params = []\n",
    "        search_param_section = re.search(r'Search Parameter Summary:.*?\\| Conformance \\| Parameter \\| Type \\| Example \\|\\n\\| --- \\| --- \\| --- \\| --- \\|(.*?)(?:\\n\\n---|\\Z)', \n",
    "                                       resource_section, re.DOTALL)\n",
    "        \n",
    "        if search_param_section:\n",
    "            param_lines = search_param_section.group(1).strip().split('\\n')\n",
    "            for line in param_lines:\n",
    "                if '|' in line:\n",
    "                    parts = [p.strip() for p in line.split('|')]\n",
    "                    if len(parts) >= 5 and parts[1] and parts[2]:\n",
    "                        conformance = parts[1].replace('**', '')\n",
    "                        param_name = parts[2]\n",
    "                        param_type = parts[3]\n",
    "                        search_params.append({\n",
    "                            'name': param_name,\n",
    "                            'type': param_type,\n",
    "                            'conformance': conformance\n",
    "                        })\n",
    "        \n",
    "        # Extract supported operations\n",
    "        operations = []\n",
    "        operations_section = re.search(r'Supported Operations:(.*?)(?:\\n\\n|\\Z)', resource_section, re.DOTALL)\n",
    "        if operations_section:\n",
    "            op_lines = operations_section.group(1).strip().split('\\n')\n",
    "            for line in op_lines:\n",
    "                if line.strip():\n",
    "                    operations.append(line.strip())\n",
    "        \n",
    "        # Extract includes and revincludes\n",
    "        includes = []\n",
    "        includes_section = re.search(r'A Server \\*\\*SHALL\\*\\* be capable of supporting the following \\_includes:(.*?)(?:\\n\\n|\\Z)', \n",
    "                                   resource_section, re.DOTALL)\n",
    "        if includes_section:\n",
    "            include_lines = includes_section.group(1).strip().split('\\n')\n",
    "            for line in include_lines:\n",
    "                if line.strip():\n",
    "                    include_match = re.search(r'([A-Za-z]+):([A-Za-z\\-]+)', line)\n",
    "                    if include_match:\n",
    "                        includes.append(f\"{include_match.group(1)}:{include_match.group(2)}\")\n",
    "        \n",
    "        revincludes = []\n",
    "        revincludes_section = re.search(r'A Server \\*\\*SHALL\\*\\* be capable of supporting the following \\_revincludes:(.*?)(?:\\n\\n|\\Z)', \n",
    "                                      resource_section, re.DOTALL)\n",
    "        if revincludes_section:\n",
    "            revinclude_lines = revincludes_section.group(1).strip().split('\\n')\n",
    "            for line in revinclude_lines:\n",
    "                if line.strip():\n",
    "                    revinclude_match = re.search(r'([A-Za-z]+):([A-Za-z\\-]+)', line)\n",
    "                    if revinclude_match:\n",
    "                        revincludes.append(f\"{revinclude_match.group(1)}:{revinclude_match.group(2)}\")\n",
    "        \n",
    "        resource_sections[resource_name] = {\n",
    "            'search_parameters': search_params,\n",
    "            'operations': operations,\n",
    "            'includes': includes,\n",
    "            'revincludes': revincludes\n",
    "        }\n",
    "    \n",
    "    # Extract general capabilities\n",
    "    general_capabilities = {}\n",
    "    general_section = re.search(r'### FHIR RESTful Capabilities(.*?)(?:###|$)', content, re.DOTALL)\n",
    "    if general_section:\n",
    "        shall_match = re.search(r'The Plan-Net Server \\*\\*SHALL\\*\\*:(.*?)(?:The Plan-Net Server \\*\\*SHOULD\\*\\*:|\\n\\n\\*\\*Security:\\*\\*|\\Z)', \n",
    "                              general_section.group(1), re.DOTALL)\n",
    "        should_match = re.search(r'The Plan-Net Server \\*\\*SHOULD\\*\\*:(.*?)(?:\\n\\n\\*\\*Security:\\*\\*|\\Z)', \n",
    "                               general_section.group(1), re.DOTALL)\n",
    "        \n",
    "        if shall_match:\n",
    "            shall_items = re.findall(r'\\d+\\.\\s*(.*?)(?:\\n\\d+\\.|\\Z)', shall_match.group(1), re.DOTALL)\n",
    "            general_capabilities['SHALL'] = [item.strip() for item in shall_items]\n",
    "        \n",
    "        if should_match:\n",
    "            should_items = re.findall(r'\\d+\\.\\s*(.*?)(?:\\n\\d+\\.|\\Z)', should_match.group(1), re.DOTALL)\n",
    "            general_capabilities['SHOULD'] = [item.strip() for item in should_items]\n",
    "    \n",
    "    return {\n",
    "        'resources': resource_sections,\n",
    "        'general_capabilities': general_capabilities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_capability_info(requirement: Dict[str, str], capability_statement: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extract relevant capability statement information for a specific requirement\n",
    "    \n",
    "    Args:\n",
    "        requirement: Requirement dictionary\n",
    "        capability_statement: Parsed capability statement\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with relevant capability information\n",
    "    \"\"\"\n",
    "    # Determine which resource types are relevant to this requirement\n",
    "    requirement_text = f\"{requirement.get('description', '')} {requirement.get('summary', '')}\"\n",
    "    resource_types = []\n",
    "    \n",
    "    # IG FHIR resource types\n",
    "    fhir_resources = [\n",
    "        \"Patient\", \"Practitioner\", \"Organization\", \"Location\", \"Endpoint\", \n",
    "        \"HealthcareService\", \"PractitionerRole\", \"OrganizationAffiliation\",\n",
    "        \"InsurancePlan\", \"Network\"\n",
    "    ]\n",
    "    \n",
    "    # Check if requirement mentions specific resources\n",
    "    for resource in fhir_resources:\n",
    "        if resource in requirement_text:\n",
    "            resource_types.append(resource)\n",
    "    \n",
    "    # If no specific resources found, check for general requirements\n",
    "    if not resource_types:\n",
    "        # If it's a server requirement\n",
    "        if \"Server\" in requirement.get('actor', ''):\n",
    "            resource_types = [\"General Server Capabilities\"]\n",
    "        # If it's a client requirement\n",
    "        elif \"Client\" in requirement.get('actor', '') or \"Application\" in requirement.get('actor', ''):\n",
    "            resource_types = [\"General Client Capabilities\"]\n",
    "    \n",
    "    # Build relevant capability information\n",
    "    relevant_info = \"### Applicable Capability Statement Information\\n\\n\"\n",
    "    \n",
    "    # Add general capabilities\n",
    "    relevant_info += \"#### General Capabilities\\n\"\n",
    "    if \"general_capabilities\" in capability_statement:\n",
    "        for level in [\"SHALL\", \"SHOULD\"]:\n",
    "            if level in capability_statement[\"general_capabilities\"]:\n",
    "                relevant_info += f\"\\n**{level}**:\\n\"\n",
    "                for item in capability_statement[\"general_capabilities\"][level]:\n",
    "                    relevant_info += f\"- {item}\\n\"\n",
    "    \n",
    "    # Add resource-specific capabilities\n",
    "    for resource_type in resource_types:\n",
    "        if resource_type in capability_statement.get(\"resources\", {}):\n",
    "            resource_info = capability_statement[\"resources\"][resource_type]\n",
    "            \n",
    "            relevant_info += f\"\\n#### {resource_type} Resource Capabilities\\n\"\n",
    "            \n",
    "            # Add search parameters\n",
    "            if resource_info.get(\"search_parameters\"):\n",
    "                relevant_info += \"\\n**Supported Search Parameters**:\\n\"\n",
    "                for param in resource_info[\"search_parameters\"]:\n",
    "                    relevant_info += f\"- {param['name']} ({param['type']}): {param['conformance']}\\n\"\n",
    "            \n",
    "            # Add operations\n",
    "            if resource_info.get(\"operations\"):\n",
    "                relevant_info += \"\\n**Supported Operations**:\\n\"\n",
    "                for op in resource_info[\"operations\"]:\n",
    "                    relevant_info += f\"- {op}\\n\"\n",
    "            \n",
    "            # Add includes\n",
    "            if resource_info.get(\"includes\"):\n",
    "                relevant_info += \"\\n**Supported _includes**:\\n\"\n",
    "                for include in resource_info[\"includes\"]:\n",
    "                    relevant_info += f\"- {include}\\n\"\n",
    "            \n",
    "            # Add revincludes\n",
    "            if resource_info.get(\"revincludes\"):\n",
    "                relevant_info += \"\\n**Supported _revincludes**:\\n\"\n",
    "                for revinclude in resource_info[\"revincludes\"]:\n",
    "                    relevant_info += f\"- {revinclude}\\n\"\n",
    "    \n",
    "    return relevant_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_requirements_file(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse an INCOSE requirements markdown file into a structured list of requirements\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the requirements markdown file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing structured requirement information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by requirement sections (separated by ---)\n",
    "    req_sections = content.split('---')\n",
    "    \n",
    "    requirements = []\n",
    "    for section in req_sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "            \n",
    "        # Parse requirement data\n",
    "        req_data = {}\n",
    "        \n",
    "        # Extract ID from format \"# REQ-XX\"\n",
    "        id_match = re.search(r'#\\s+([A-Z0-9\\-]+)', section)\n",
    "        if id_match:\n",
    "            req_data['id'] = id_match.group(1)\n",
    "        \n",
    "        # Extract other fields\n",
    "        for field in ['Summary', 'Description', 'Verification', 'Actor', 'Conformance', 'Conditional', 'Source']:\n",
    "            pattern = rf'\\*\\*{field}\\*\\*:\\s*(.*?)(?:\\n\\*\\*|\\n---|\\\\Z)'\n",
    "            field_match = re.search(pattern, section, re.DOTALL)\n",
    "            if field_match:\n",
    "                req_data[field.lower()] = field_match.group(1).strip()\n",
    "        \n",
    "        if req_data:\n",
    "            requirements.append(req_data)\n",
    "    \n",
    "    return requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_requirement_for_prompt(requirement: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Format a requirement dictionary into markdown for inclusion in prompts\n",
    "    \n",
    "    Args:\n",
    "        requirement: Requirement dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Formatted markdown string\n",
    "    \"\"\"\n",
    "    formatted = f\"# {requirement.get('id', 'UNKNOWN-ID')}\\n\"\n",
    "    formatted += f\"**Summary**: {requirement.get('summary', '')}\\n\"\n",
    "    formatted += f\"**Description**: {requirement.get('description', '')}\\n\"\n",
    "    formatted += f\"**Verification**: {requirement.get('verification', '')}\\n\"\n",
    "    formatted += f\"**Actor**: {requirement.get('actor', '')}\\n\"\n",
    "    formatted += f\"**Conformance**: {requirement.get('conformance', '')}\\n\"\n",
    "    formatted += f\"**Conditional**: {requirement.get('conditional', '')}\\n\"\n",
    "    formatted += f\"**Source**: {requirement.get('source', '')}\\n\"\n",
    "    \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def make_llm_request(client, api_type: str, prompt: str, system_prompt: str, rate_limit_func) -> str:\n",
    "    \"\"\"Make rate-limited API request with retries\"\"\"\n",
    "    rate_limit_func()\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }],\n",
    "                system=system_prompt\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            response = client.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            elif response.candidates:\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_requirement_group(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    requirement: Dict[str, str],\n",
    "    rate_limit_func\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Identify the appropriate group for a requirement using LLM\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirement: Requirement dictionary\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Identified group name\n",
    "    \"\"\"\n",
    "    # Use actor field as a possible hint if available\n",
    "    actor = requirement.get('actor', '').strip()\n",
    "    if actor and len(actor) > 3 and actor not in ['System', 'User', 'All']:\n",
    "        # Simple heuristic - if actor is specific enough, it might be a good grouping\n",
    "        possible_groups = ['Client', 'Server', 'Patient', 'Practitioner', 'Organization', 'HealthcareService']\n",
    "        for group in possible_groups:\n",
    "            if group.lower() in actor.lower():\n",
    "                return group\n",
    "    \n",
    "    # Use LLM to identify group\n",
    "    logger.info(f\"Identifying group for requirement {requirement.get('id', 'unknown')} using {api_type}...\")\n",
    "    \n",
    "    # Format requirement as markdown\n",
    "    formatted_req = format_requirement_for_prompt(requirement)\n",
    "    \n",
    "    # Create prompt with the requirement\n",
    "    prompt = REQUIREMENT_GROUPING_PROMPT.format(requirement=formatted_req)\n",
    "    \n",
    "    # Make the API request with simplified system prompt\n",
    "    group_system_prompt = \"You are a FHIR expert who categorizes requirements by their functional or resource type.\"\n",
    "    group_name = make_llm_request(client, api_type, prompt, group_system_prompt, rate_limit_func).strip()\n",
    "    \n",
    "    # Clean up response (in case model returns extra text)\n",
    "    if '\\n' in group_name:\n",
    "        group_name = group_name.split('\\n')[0].strip()\n",
    "    \n",
    "    return group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_specification_with_capability(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    requirement: Dict[str, str],\n",
    "    capability_statement: Dict[str, Any],\n",
    "    rate_limit_func\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive test specification for a single requirement, considering capability statement\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirement: Requirement dictionary\n",
    "        capability_statement: Parsed capability statement\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Test specification for the requirement\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating test specification for {requirement.get('id', 'unknown')} using {api_type}...\")\n",
    "    \n",
    "    # Format requirement as markdown\n",
    "    formatted_req = format_requirement_for_prompt(requirement)\n",
    "    \n",
    "    # Extract relevant capability information\n",
    "    capability_info = extract_relevant_capability_info(requirement, capability_statement)\n",
    "    \n",
    "    # Create prompt with the requirement and capability information\n",
    "    prompt = CONSOLIDATED_TEST_PLAN_WITH_CAPABILITY_PROMPT.format(\n",
    "        requirement=formatted_req,\n",
    "        capability_info=capability_info\n",
    "    )\n",
    "    \n",
    "    # Make the API request\n",
    "    return make_llm_request(client, api_type, prompt, SYSTEM_PROMPT, rate_limit_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consolidated_test_plan(\n",
    "    api_type: str,\n",
    "    requirements_file: str,\n",
    "    capability_statement_file: str = None,\n",
    "    ig_name: str = \"FHIR Implementation Guide\",\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process requirements and generate a consolidated test plan\n",
    "    \n",
    "    Args:\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirements_file: Path to requirements markdown file\n",
    "        capability_statement_file: Path to capability statement markdown file (optional)\n",
    "        ig_name: Name of the Implementation Guide\n",
    "        output_dir: Directory for output files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing path to output file\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting test plan generation with {api_type} for {ig_name}\")\n",
    "    \n",
    "    # Initialize API clients and rate limiters\n",
    "    clients = setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = API_CONFIGS[api_type]\n",
    "    rate_limiter = create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # Parse requirements from file\n",
    "        requirements = parse_requirements_file(requirements_file)\n",
    "        logger.info(f\"Parsed {len(requirements)} requirements from {requirements_file}\")\n",
    "        \n",
    "        # Parse capability statement if provided\n",
    "        capability_statement = None\n",
    "        if capability_statement_file and os.path.exists(capability_statement_file):\n",
    "            capability_statement = parse_capability_statement(capability_statement_file)\n",
    "            logger.info(f\"Parsed capability statement from {capability_statement_file}\")\n",
    "        \n",
    "        # Identify groups for each requirement\n",
    "        req_groups = {}\n",
    "        for req in requirements:\n",
    "            req_id = req.get('id', 'UNKNOWN-ID')\n",
    "            req_groups[req_id] = identify_requirement_group(client, api_type, req, check_limits)\n",
    "            # Add small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Group requirements by identified category\n",
    "        grouped_requirements = defaultdict(list)\n",
    "        for req in requirements:\n",
    "            req_id = req.get('id', 'UNKNOWN-ID')\n",
    "            group = req_groups.get(req_id, 'Uncategorized')\n",
    "            grouped_requirements[group].append(req)\n",
    "            \n",
    "        # Log the grouping results\n",
    "        logger.info(f\"Requirements grouped into {len(grouped_requirements)} categories\")\n",
    "        for group, reqs in grouped_requirements.items():\n",
    "            logger.info(f\"Group '{group}': {len(reqs)} requirements\")\n",
    "        \n",
    "        # Output file\n",
    "        test_plan_path = os.path.join(\n",
    "            output_dir, \n",
    "            f\"{api_type}_test_plan_{timestamp}.md\"\n",
    "        )\n",
    "        \n",
    "        # Initialize test plan content\n",
    "        test_plan = f\"# Consolidated Test Plan for {ig_name}\\n\\n\"\n",
    "        test_plan += f\"## Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "        \n",
    "        # Add capability statement reference if used\n",
    "        if capability_statement:\n",
    "            test_plan += \"## Capability Statement\\n\\n\"\n",
    "            test_plan += f\"This test plan incorporates constraints and requirements from the {ig_name} Capability Statement.\\n\\n\"\n",
    "        \n",
    "        test_plan += \"## Table of Contents\\n\\n\"\n",
    "        \n",
    "        # Add group headers to TOC\n",
    "        for group in sorted(grouped_requirements.keys()):\n",
    "            test_plan += f\"- [{group}](#{group.lower().replace(' ', '-')})\\n\"\n",
    "            for req in grouped_requirements[group]:\n",
    "                req_id = req.get('id', 'UNKNOWN-ID')\n",
    "                req_summary = req.get('summary', 'No summary')\n",
    "                test_plan += f\"  - [{req_id}: {req_summary}](#{req_id.lower()})\\n\"\n",
    "        \n",
    "        # Process each group and its requirements\n",
    "        test_plan += \"\\n## Test Specifications\\n\\n\"\n",
    "        \n",
    "        for group in sorted(grouped_requirements.keys()):\n",
    "            # Add group header with anchor for TOC linking\n",
    "            test_plan += f\"<a id='{group.lower().replace(' ', '-')}'></a>\\n\\n\"\n",
    "            test_plan += f\"## {group}\\n\\n\"\n",
    "            \n",
    "            # Process each requirement in the group\n",
    "            for i, req in enumerate(grouped_requirements[group]):\n",
    "                req_id = req.get('id', 'UNKNOWN-ID')\n",
    "                logger.info(f\"Processing requirement for group '{group}': {req_id}\")\n",
    "                \n",
    "                # Generate test specification with capability statement if available\n",
    "                if capability_statement:\n",
    "                    test_spec = generate_test_specification_with_capability(\n",
    "                        client, api_type, req, capability_statement, check_limits\n",
    "                    )\n",
    "                else:\n",
    "                    test_spec = generate_test_specification(client, api_type, req, check_limits)\n",
    "                \n",
    "                # Add to test plan content with proper anchor for TOC linking\n",
    "                test_plan += f\"<a id='{req_id.lower()}'></a>\\n\\n\"\n",
    "                test_plan += f\"### {req_id}: {req.get('summary', 'No summary')}\\n\\n\"\n",
    "                test_plan += f\"**Description**: {req.get('description', '')}\\n\\n\"\n",
    "                test_plan += f\"**Actor**: {req.get('actor', '')}\\n\\n\"\n",
    "                test_plan += f\"**Conformance**: {req.get('conformance', '')}\\n\\n\"\n",
    "                test_plan += f\"{test_spec}\\n\\n\"\n",
    "                test_plan += \"---\\n\\n\"\n",
    "                \n",
    "                # Add delay between requests\n",
    "                if i < len(grouped_requirements[group]) - 1:  # No need to delay after the last request\n",
    "                    time.sleep(config[\"delay_between_chunks\"])\n",
    "            \n",
    "            # Add spacing between groups\n",
    "            test_plan += \"\\n\\n\"\n",
    "        \n",
    "        # Save consolidated test plan\n",
    "        with open(test_plan_path, 'w') as f:\n",
    "            f.write(test_plan)\n",
    "        logger.info(f\"Consolidated test plan saved to {test_plan_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"requirements_count\": len(requirements),\n",
    "            \"group_count\": len(grouped_requirements),\n",
    "            \"test_plan_path\": test_plan_path\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing requirements: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_plan_generator():\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get input from user or set default values\n",
    "    print(\"\\nFHIR IG Test Plan Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Let user select the API\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    # Get requirements file path\n",
    "    requirements_file = input(\"\\nEnter path to requirements markdown file: \")\n",
    "    \n",
    "    # Check if requirements file exists\n",
    "    if not os.path.exists(requirements_file):\n",
    "        logger.error(f\"Requirements file not found: {requirements_file}\")\n",
    "        print(f\"Error: Requirements file not found at {requirements_file}\")\n",
    "        return\n",
    "    \n",
    "    # Get capability statement file path (optional)\n",
    "    capability_statement_file = input(\"\\nEnter path to Capability Statement markdown file (optional, press Enter to skip): \")\n",
    "    \n",
    "    if capability_statement_file and not os.path.exists(capability_statement_file):\n",
    "        logger.warning(f\"Capability Statement file not found: {capability_statement_file}\")\n",
    "        print(f\"Warning: Capability Statement file not found at {capability_statement_file}. Proceeding without it.\")\n",
    "        capability_statement_file = None\n",
    "    \n",
    "    # Get IG name\n",
    "    ig_name = input(\"\\nEnter Implementation Guide name (default 'FHIR Implementation Guide'): \") or \"FHIR Implementation Guide\"\n",
    "    \n",
    "    # Get output directory\n",
    "    output_dir = input(f\"\\nEnter output directory (default '{OUTPUT_DIR}'): \") or OUTPUT_DIR\n",
    "    \n",
    "    print(f\"\\nProcessing requirements with {api_type.capitalize()}...\")\n",
    "    if capability_statement_file:\n",
    "        print(f\"Including Capability Statement from {capability_statement_file}\")\n",
    "    print(f\"This may take several minutes depending on the number of requirements.\")\n",
    "    \n",
    "    try:\n",
    "        # Process requirements and generate test plan\n",
    "        result = generate_consolidated_test_plan(\n",
    "            api_type=api_type,\n",
    "            requirements_file=requirements_file,\n",
    "            capability_statement_file=capability_statement_file,\n",
    "            ig_name=ig_name,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        # Output results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Test plan generation complete!\")\n",
    "        print(f\"Processed {result['requirements_count']} requirements\")\n",
    "        print(f\"Grouped into {result['group_count']} categories\")\n",
    "        print(f\"Consolidated test plan: {result['test_plan_path']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FHIR IG Test Plan Generator\n",
      "==================================================\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:40:41,784 - __main__ - INFO - Starting test plan generation with claude for Plan Net\n",
      "2025-04-16 10:40:41,811 - __main__ - INFO - Parsed 10 requirements from /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs/claude_reqs_list_v220250416_103916.md\n",
      "2025-04-16 10:40:41,813 - __main__ - INFO - Parsed capability statement from /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned/CapabilityStatement_plan_net.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing requirements with Claude...\n",
      "Including Capability Statement from /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned/CapabilityStatement_plan_net.md\n",
      "This may take several minutes depending on the number of requirements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:40:43,326 - __main__ - INFO - Identifying group for requirement REQ-04 using claude...\n",
      "2025-04-16 10:40:45,596 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:40:46,098 - __main__ - INFO - Identifying group for requirement REQ-05 using claude...\n",
      "2025-04-16 10:40:47,336 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:40:47,838 - __main__ - INFO - Identifying group for requirement REQ-06 using claude...\n",
      "2025-04-16 10:40:48,924 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:40:49,425 - __main__ - INFO - Identifying group for requirement REQ-07 using claude...\n",
      "2025-04-16 10:40:50,778 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:40:51,284 - __main__ - INFO - Identifying group for requirement REQ-08 using claude...\n",
      "2025-04-16 10:40:52,917 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:40:53,424 - __main__ - INFO - Identifying group for requirement REQ-09 using claude...\n",
      "2025-04-16 10:40:54,420 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:40:54,926 - __main__ - INFO - Identifying group for requirement REQ-10 using claude...\n",
      "2025-04-16 10:40:56,290 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:40:56,796 - __main__ - INFO - Requirements grouped into 9 categories\n",
      "2025-04-16 10:40:56,798 - __main__ - INFO - Group 'Server': 2 requirements\n",
      "2025-04-16 10:40:56,806 - __main__ - INFO - Group 'Client': 1 requirements\n",
      "2025-04-16 10:40:56,806 - __main__ - INFO - Group 'General/Non-Resource-Specific': 1 requirements\n",
      "2025-04-16 10:40:56,807 - __main__ - INFO - Group 'General': 1 requirements\n",
      "2025-04-16 10:40:56,807 - __main__ - INFO - Group 'Common Base Requirements': 1 requirements\n",
      "2025-04-16 10:40:56,807 - __main__ - INFO - Group 'General Requirements': 1 requirements\n",
      "2025-04-16 10:40:56,807 - __main__ - INFO - Group 'Universal': 1 requirements\n",
      "2025-04-16 10:40:56,807 - __main__ - INFO - Group 'General/Cross-Resource': 1 requirements\n",
      "2025-04-16 10:40:56,808 - __main__ - INFO - Group 'Common Non-Resource Specific': 1 requirements\n",
      "2025-04-16 10:40:56,808 - __main__ - INFO - Processing requirement for group 'Client': REQ-03\n",
      "2025-04-16 10:40:56,808 - __main__ - INFO - Generating test specification for REQ-03 using claude...\n",
      "2025-04-16 10:41:12,206 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:41:12,210 - __main__ - INFO - Processing requirement for group 'Common Base Requirements': REQ-06\n",
      "2025-04-16 10:41:12,211 - __main__ - INFO - Generating test specification for REQ-06 using claude...\n",
      "2025-04-16 10:41:27,866 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:41:27,869 - __main__ - INFO - Processing requirement for group 'Common Non-Resource Specific': REQ-10\n",
      "2025-04-16 10:41:27,870 - __main__ - INFO - Generating test specification for REQ-10 using claude...\n",
      "2025-04-16 10:41:41,245 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:41:41,246 - __main__ - INFO - Processing requirement for group 'General': REQ-05\n",
      "2025-04-16 10:41:41,247 - __main__ - INFO - Generating test specification for REQ-05 using claude...\n",
      "2025-04-16 10:41:55,218 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:41:55,219 - __main__ - INFO - Processing requirement for group 'General Requirements': REQ-07\n",
      "2025-04-16 10:41:55,220 - __main__ - INFO - Generating test specification for REQ-07 using claude...\n",
      "2025-04-16 10:42:08,648 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:42:08,651 - __main__ - INFO - Processing requirement for group 'General/Cross-Resource': REQ-09\n",
      "2025-04-16 10:42:08,652 - __main__ - INFO - Generating test specification for REQ-09 using claude...\n",
      "2025-04-16 10:42:21,593 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:42:21,594 - __main__ - INFO - Processing requirement for group 'General/Non-Resource-Specific': REQ-04\n",
      "2025-04-16 10:42:21,595 - __main__ - INFO - Generating test specification for REQ-04 using claude...\n",
      "2025-04-16 10:42:33,370 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:42:33,373 - __main__ - INFO - Processing requirement for group 'Server': REQ-01\n",
      "2025-04-16 10:42:33,374 - __main__ - INFO - Generating test specification for REQ-01 using claude...\n",
      "2025-04-16 10:42:45,711 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:42:46,718 - __main__ - INFO - Processing requirement for group 'Server': REQ-02\n",
      "2025-04-16 10:42:46,719 - __main__ - INFO - Generating test specification for REQ-02 using claude...\n",
      "2025-04-16 10:42:57,866 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:42:57,867 - __main__ - INFO - Processing requirement for group 'Universal': REQ-08\n",
      "2025-04-16 10:42:57,867 - __main__ - INFO - Generating test specification for REQ-08 using claude...\n",
      "2025-04-16 10:43:12,222 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 10:43:12,225 - __main__ - INFO - Consolidated test plan saved to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_plan_output/claude_test_plan_20250416_104041.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test plan generation complete!\n",
      "Processed 10 requirements\n",
      "Grouped into 9 categories\n",
      "Consolidated test plan: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_plan_output/claude_test_plan_20250416_104041.md\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the generator when executed in a notebook cell\n",
    "if __name__ == \"__main__\":\n",
    "    run_test_plan_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
