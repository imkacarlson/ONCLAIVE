{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHIR Inferno Test Generator\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level to project root\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'inferno_test_output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API Configuration\n",
    "API_CONFIGS = {\n",
    "    \"claude\": {\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\", \n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,  # Lower temperature for code generation\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 1,\n",
    "        \"delay_between_batches\": 3,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.1\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-1.5-pro-001\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 50000,\n",
    "        \"delay_between_requests\": 0.1,\n",
    "        \"timeout\": 60\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 450,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.15\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# System prompts for test generation\n",
    "INFERNO_TEST_SYSTEM_PROMPT = \"\"\"You are a specialized FHIR testing engineer with expertise in healthcare interoperability.\n",
    "Your task is to convert test specifications from a test plan into executable Ruby tests using the Inferno testing framework.\n",
    "You will generate valid, working Ruby code that follows Inferno test patterns and best practices.\"\"\"\n",
    "\n",
    "# Prompt template for test generation\n",
    "INFERNO_TEST_GENERATION_PROMPT = \"\"\"\n",
    "Convert the following FHIR test specification into executable Ruby code using the Inferno testing framework.\n",
    "\n",
    "Test Specification:\n",
    "{test_specification}\n",
    "\n",
    "Create an Inferno test implementation that:\n",
    "1. Follows the Inferno structure (TestGroup containing one or more tests)\n",
    "2. Implements the test logic described in the specification\n",
    "3. Makes appropriate FHIR API calls\n",
    "4. Includes proper assertions to validate the requirements\n",
    "5. Handles both success and error cases appropriately\n",
    "\n",
    "Naming conventions:\n",
    "- Use underscored lowercase names for files (e.g., patient_read_test.rb)\n",
    "- Use CamelCase for class names (e.g., PatientReadTest)\n",
    "- Class names should end with 'Test' or 'Group'\n",
    "- Give each test a unique ID that's descriptive and related to the requirement\n",
    "\n",
    "The test should be comprehensive but focused on exactly what's described in the specification.\n",
    "Include proper documentation in the code and follow Inferno best practices.\n",
    "\n",
    "Requirement ID: {requirement_id}\n",
    "Module Name: {module_name}\n",
    "\n",
    "Guidance on Inferno test development:\n",
    "{inferno_guidance}\n",
    "\n",
    "Return only the Ruby code for the test implementation, with no additional explanation.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for determining if a requirement is testable\n",
    "TEST_FEASIBILITY_PROMPT = \"\"\"\n",
    "Analyze the following test specification from a FHIR Implementation Guide and determine if it's feasible to implement as an automated test.\n",
    "\n",
    "Test Specification:\n",
    "{test_specification}\n",
    "\n",
    "Respond with a JSON object containing the following fields:\n",
    "1. \"testable\": boolean (true if it can be implemented as an automated test, false otherwise)\n",
    "2. \"reason\": brief explanation of your assessment\n",
    "3. \"resource_types\": array of FHIR resource types involved in the test (e.g., [\"Patient\", \"Observation\"])\n",
    "4. \"operations\": array of FHIR operations required (e.g., [\"read\", \"search\"])\n",
    "5. \"complexity\": one of [\"simple\", \"moderate\", \"complex\"]\n",
    "\n",
    "Return only valid JSON with no additional text.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in API_CONFIGS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    config = API_CONFIGS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= config['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= config['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:] \n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < config['delay_between_requests']:\n",
    "        time.sleep(config['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        gemini.configure(api_key=gemini_api_key)\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "        openai_client = OpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gpt\": openai_client,\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def make_llm_request(client, api_type: str, prompt: str, system_prompt: str, rate_limit_func) -> str:\n",
    "    \"\"\"Make rate-limited API request with retries\"\"\"\n",
    "    rate_limit_func()\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }],\n",
    "                system=system_prompt\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            response = client.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            elif response.candidates:\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_test_plan(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse a consolidated test plan into a list of test specifications\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the test plan markdown file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing structured test specifications\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract test specifications by splitting on the triple-dash separator\n",
    "    test_specs_raw = content.split('---')\n",
    "    test_specs = []\n",
    "    \n",
    "    # Parse each test specification section\n",
    "    for i, section in enumerate(test_specs_raw):\n",
    "        if not section.strip():\n",
    "            continue\n",
    "            \n",
    "        # Extract requirement ID with format REQ-XX\n",
    "        id_match = re.search(r'<a id=[\\'\\\"](req-\\d+)[\\'\\\"]\\\\/>', section, re.IGNORECASE) or \\\n",
    "                   re.search(r'### (REQ-\\d+):', section, re.IGNORECASE)\n",
    "        if not id_match:\n",
    "            continue\n",
    "            \n",
    "        req_id = id_match.group(1).upper()\n",
    "        \n",
    "        # Extract requirement description\n",
    "        desc_match = re.search(r'\\*\\*Description\\*\\*:\\s*(.*?)(?:\\n\\n|\\*\\*Actor\\*\\*)', section, re.DOTALL)\n",
    "        description = desc_match.group(1).strip() if desc_match else \"\"\n",
    "        \n",
    "        # Extract actor\n",
    "        actor_match = re.search(r'\\*\\*Actor\\*\\*:\\s*(.*?)(?:\\n\\n|\\*\\*Conformance\\*\\*)', section, re.DOTALL)\n",
    "        actor = actor_match.group(1).strip() if actor_match else \"\"\n",
    "        \n",
    "        # Extract conformance level\n",
    "        conf_match = re.search(r'\\*\\*Conformance\\*\\*:\\s*(.*?)(?:\\n\\n|## Test)', section, re.DOTALL)\n",
    "        conformance = conf_match.group(1).strip() if conf_match else \"\"\n",
    "        \n",
    "        # Extract testability assessment\n",
    "        test_match = re.search(r'\\*\\*Testability Assessment:\\*\\*\\s*(Automatic|Manual|Hybrid)', section, re.DOTALL | re.IGNORECASE)\n",
    "        testability = test_match.group(1).strip() if test_match else \"Unknown\"\n",
    "        \n",
    "        # Extract test implementation strategy\n",
    "        strategy_match = re.search(r'### 3\\. Test Implementation Strategy\\s*(.*?)(?:## |###|$)', section, re.DOTALL)\n",
    "        strategy = strategy_match.group(1).strip() if strategy_match else \"\"\n",
    "        \n",
    "        # Extract validation criteria\n",
    "        validation_match = re.search(r'#### 3\\.\\d+\\. Validation Criteria\\s*(.*?)(?:## |###|$)', section, re.DOTALL)\n",
    "        validation = validation_match.group(1).strip() if validation_match else \"\"\n",
    "        \n",
    "        # Create a test specification dictionary\n",
    "        test_spec = {\n",
    "            'id': req_id,\n",
    "            'description': description,\n",
    "            'actor': actor,\n",
    "            'conformance': conformance,\n",
    "            'testability': testability,\n",
    "            'strategy': strategy,\n",
    "            'validation': validation,\n",
    "            'full_spec': section.strip()\n",
    "        }\n",
    "        \n",
    "        test_specs.append(test_spec)\n",
    "    \n",
    "    return test_specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_inferno_guidance() -> str:\n",
    "    \"\"\"\n",
    "    Load the Inferno guidance document\n",
    "    \n",
    "    Returns:\n",
    "        String containing Inferno test development guidance\n",
    "    \"\"\"\n",
    "    guidance_path = os.path.join(PROJECT_ROOT, 'inferno-guidance.md')\n",
    "    \n",
    "    # Use a default guidance if file not found\n",
    "    if not os.path.exists(guidance_path):\n",
    "        return \"\"\"# Inferno Test Development Guidance\n",
    "        \n",
    "        Inferno is a Ruby-based testing framework for FHIR implementations. Tests should follow the structure:\n",
    "        \n",
    "        ```ruby\n",
    "        module YourTestKit\n",
    "          class YourTestGroup < Inferno::TestGroup\n",
    "            id :unique_id\n",
    "            title 'Test Group Title'\n",
    "            description 'Detailed description'\n",
    "            \n",
    "            test do\n",
    "              id :test_unique_id\n",
    "              title 'Test Title'\n",
    "              description 'Test description'\n",
    "              \n",
    "              run do\n",
    "                # Test implementation\n",
    "                assert condition, 'Failure message'\n",
    "              end\n",
    "            end\n",
    "          end\n",
    "        end\n",
    "        ```\n",
    "        \"\"\"\n",
    "    \n",
    "    with open(guidance_path, 'r') as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assess_test_feasibility(\n",
    "    client,\n",
    "    api_type: str,\n",
    "    test_spec: Dict[str, Any],\n",
    "    rate_limit_func\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Assess if a test specification is feasible to implement as an automated test\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        test_spec: Test specification dictionary\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with assessment information\n",
    "    \"\"\"\n",
    "    logger.info(f\"Assessing feasibility of {test_spec['id']}...\")\n",
    "    \n",
    "    # For specifications already marked as Manual, we can skip the assessment\n",
    "    if test_spec['testability'].lower() == 'manual':\n",
    "        return {\n",
    "            'testable': False,\n",
    "            'reason': 'Requirement is explicitly marked as Manual',\n",
    "            'resource_types': [],\n",
    "            'operations': [],\n",
    "            'complexity': 'N/A'\n",
    "        }\n",
    "    \n",
    "    # For specifications marked as Automatic, we can skip the assessment\n",
    "    if test_spec['testability'].lower() == 'automatic':\n",
    "        # Simple heuristic to determine resource types from the specification\n",
    "        fhir_resources = [\n",
    "            \"Patient\", \"Practitioner\", \"Organization\", \"Location\", \"Endpoint\", \n",
    "            \"HealthcareService\", \"PractitionerRole\", \"OrganizationAffiliation\",\n",
    "            \"InsurancePlan\", \"Network\"\n",
    "        ]\n",
    "        \n",
    "        resource_types = []\n",
    "        for resource in fhir_resources:\n",
    "            if resource in test_spec['full_spec']:\n",
    "                resource_types.append(resource)\n",
    "        \n",
    "        # Simple heuristic to determine operations\n",
    "        operations = []\n",
    "        operation_keywords = {\n",
    "            'read': ['read', 'retrieve', 'get'],\n",
    "            'search': ['search', 'find', 'query'],\n",
    "            'create': ['create', 'post', 'add'],\n",
    "            'update': ['update', 'put', 'modify'],\n",
    "            'delete': ['delete', 'remove']\n",
    "        }\n",
    "        \n",
    "        for op, keywords in operation_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in test_spec['full_spec'].lower():\n",
    "                    operations.append(op)\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'testable': True,\n",
    "            'reason': 'Requirement is explicitly marked as Automatic',\n",
    "            'resource_types': resource_types,\n",
    "            'operations': list(set(operations)),  # Deduplicate\n",
    "            'complexity': test_spec.get('complexity', 'moderate')\n",
    "        }\n",
    "    \n",
    "    # For Hybrid or unknown testability, use LLM to assess\n",
    "    prompt = TEST_FEASIBILITY_PROMPT.format(\n",
    "        test_specification=test_spec['full_spec']\n",
    "    )\n",
    "    \n",
    "    system_prompt = \"You are a FHIR testing expert who evaluates the feasibility of implementing automated tests for FHIR requirements.\"\n",
    "    \n",
    "    response = make_llm_request(client, api_type, prompt, system_prompt, rate_limit_func)\n",
    "    \n",
    "    try:\n",
    "        # Parse the JSON response\n",
    "        assessment = json.loads(response)\n",
    "        return assessment\n",
    "    except json.JSONDecodeError:\n",
    "        # If we can't parse the response, fallback to a conservative assessment\n",
    "        logger.warning(f\"Failed to parse feasibility assessment for {test_spec['id']}\")\n",
    "        return {\n",
    "            'testable': False,\n",
    "            'reason': 'Failed to determine feasibility',\n",
    "            'resource_types': [],\n",
    "            'operations': [],\n",
    "            'complexity': 'unknown'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def determine_test_groups(test_specs: List[Dict[str, Any]]) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Group test specifications based on common characteristics\n",
    "    \n",
    "    Args:\n",
    "        test_specs: List of test specification dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Nested dictionary of grouped test specifications\n",
    "        The first level of keys is the actor (Client/Server)\n",
    "        The second level of keys is the resource type or category\n",
    "    \"\"\"\n",
    "    # First, group by actor (client/server)\n",
    "    actor_groups = defaultdict(list)\n",
    "    for spec in test_specs:\n",
    "        actor = \"Client\" if \"client\" in spec['actor'].lower() else \"Server\"\n",
    "        actor_groups[actor].append(spec)\n",
    "    \n",
    "    # Then, for each actor group, further subdivide by resource type if possible\n",
    "    result_groups = {}\n",
    "    \n",
    "    for actor, specs in actor_groups.items():\n",
    "        if actor not in result_groups:\n",
    "            result_groups[actor] = {}\n",
    "        \n",
    "        resource_groups = defaultdict(list)\n",
    "        \n",
    "        # FHIR resources to look for\n",
    "        fhir_resources = [\n",
    "            \"Patient\", \"Practitioner\", \"Organization\", \"Location\", \"Endpoint\", \n",
    "            \"HealthcareService\", \"PractitionerRole\", \"OrganizationAffiliation\",\n",
    "            \"InsurancePlan\", \"Network\"\n",
    "        ]\n",
    "        \n",
    "        for spec in specs:\n",
    "            # Try to determine resource type from test spec\n",
    "            resource_found = False\n",
    "            for resource in fhir_resources:\n",
    "                if resource in spec['full_spec']:\n",
    "                    resource_groups[resource].append(spec)\n",
    "                    resource_found = True\n",
    "                    break\n",
    "            \n",
    "            # If no specific resource identified, put in a general group\n",
    "            if not resource_found:\n",
    "                if 'search' in spec['full_spec'].lower():\n",
    "                    resource_groups['Search'].append(spec)\n",
    "                elif 'profile' in spec['full_spec'].lower():\n",
    "                    resource_groups['Profiles'].append(spec)\n",
    "                else:\n",
    "                    resource_groups['General'].append(spec)\n",
    "        \n",
    "        # Add the resource groups to the result\n",
    "        result_groups[actor] = dict(resource_groups)\n",
    "    \n",
    "    return result_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_inferno_test(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    test_spec: Dict[str, Any],\n",
    "    inferno_guidance: str,\n",
    "    module_name: str,\n",
    "    rate_limit_func\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate an Inferno test implementation for a test specification\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        test_spec: Test specification dictionary\n",
    "        inferno_guidance: Inferno test development guidance\n",
    "        module_name: Name of the module for the test\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Ruby code implementing the test\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating Inferno test for {test_spec['id']}...\")\n",
    "    \n",
    "    # Prepare the prompt\n",
    "    prompt = INFERNO_TEST_GENERATION_PROMPT.format(\n",
    "        test_specification=test_spec['full_spec'],\n",
    "        requirement_id=test_spec['id'],\n",
    "        module_name=module_name,\n",
    "        inferno_guidance=inferno_guidance\n",
    "    )\n",
    "    \n",
    "    # Make the API request\n",
    "    response = make_llm_request(client, api_type, prompt, INFERNO_TEST_SYSTEM_PROMPT, rate_limit_func)\n",
    "    \n",
    "    # Clean up any markdown formatting if the LLM added code blocks\n",
    "    if response.startswith('```ruby'):\n",
    "        response = re.sub(r'^```ruby\\n', '', response)\n",
    "        response = re.sub(r'\\n```$', '', response)\n",
    "    elif response.startswith('```'):\n",
    "        response = re.sub(r'^```\\n', '', response)\n",
    "        response = re.sub(r'\\n```$', '', response)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_file_structure(grouped_tests: Dict[str, Dict[str, List[Dict[str, Any]]]], module_name: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Create the file structure for the Inferno tests\n",
    "    \n",
    "    Args:\n",
    "        grouped_tests: Nested dictionary of grouped test specs\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping test specs to file paths\n",
    "    \"\"\"\n",
    "    # Create base directories\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    os.makedirs(module_dir, exist_ok=True)\n",
    "    \n",
    "    # Create test group directories\n",
    "    test_file_map = {}\n",
    "    \n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        actor_dir = os.path.join(module_dir, actor.lower())\n",
    "        os.makedirs(actor_dir, exist_ok=True)\n",
    "        \n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            resource_dir = os.path.join(actor_dir, resource.lower())\n",
    "            os.makedirs(resource_dir, exist_ok=True)\n",
    "            \n",
    "            for spec in test_specs:\n",
    "                # Create a file name based on the requirement ID\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                file_name = f\"{req_id}_test.rb\"\n",
    "                file_path = os.path.join(resource_dir, file_name)\n",
    "                \n",
    "                test_file_map[spec['id']] = file_path\n",
    "    \n",
    "    return test_file_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_module_file(module_name: str, output_dir: str, grouped_tests: Dict[str, Dict[str, List[Dict[str, Any]]]]):\n",
    "    \"\"\"\n",
    "    Generate the main module file that includes all test groups\n",
    "    \n",
    "    Args:\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        grouped_tests: Nested dictionary of grouped test specs\n",
    "        \n",
    "    Returns:\n",
    "        Path to the generated module file\n",
    "    \"\"\"\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    module_file_path = os.path.join(module_dir, f\"{module_name.lower()}.rb\")\n",
    "    \n",
    "    # Generate module file content\n",
    "    module_content = f\"# {module_name} Inferno Test Suite\\n\"\n",
    "    module_content += \"# Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\"\n",
    "    module_content += \"require 'inferno/dsl/test_suite'\\n\\n\"\n",
    "    \n",
    "    # Add requires for all test groups\n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            group_path = f\"{module_name.lower()}/{actor.lower()}/{resource.lower()}\"\n",
    "            module_content += f\"# {actor} {resource} tests\\n\"\n",
    "            for spec in test_specs:\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                module_content += f\"require_relative '{actor.lower()}/{resource.lower()}/{req_id}_test'\\n\"\n",
    "            module_content += \"\\n\"\n",
    "    \n",
    "    # Add module definition\n",
    "    module_content += f\"module {module_name}\\n\"\n",
    "    \n",
    "    # Add test suite class\n",
    "    class_name = f\"{module_name}TestSuite\"\n",
    "    module_content += f\"  class {class_name} < Inferno::TestSuite\\n\"\n",
    "    module_content += f\"    id :{module_name.lower()}_suite\\n\"\n",
    "    module_content += f\"    title '{module_name} Test Suite'\\n\"\n",
    "    module_content += f\"    description 'Test suite for validating {module_name} Implementation Guide conformance'\\n\\n\"\n",
    "    \n",
    "    # Add actor groups\n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        module_content += f\"    # {actor} tests\\n\"\n",
    "        module_content += f\"    group do\\n\"\n",
    "        module_content += f\"      id :{actor.lower()}_group\\n\"\n",
    "        module_content += f\"      title '{actor} Tests'\\n\\n\"\n",
    "        \n",
    "        # Add resource groups\n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            module_content += f\"      # {resource} tests\\n\"\n",
    "            module_content += f\"      group do\\n\"\n",
    "            module_content += f\"        id :{actor.lower()}_{resource.lower()}_group\\n\"\n",
    "            module_content += f\"        title '{actor} {resource} Tests'\\n\\n\"\n",
    "            \n",
    "            # Add references to individual tests\n",
    "            for spec in test_specs:\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                test_class = f\"{module_name}::{actor}{resource}{spec['id'].replace('-', '')}Test\"\n",
    "                module_content += f\"        test from: :{req_id}_test\\n\"\n",
    "            \n",
    "            module_content += \"      end\\n\\n\"\n",
    "        \n",
    "        module_content += \"    end\\n\\n\"\n",
    "    \n",
    "    module_content += \"  end\\n\"\n",
    "    module_content += \"end\\n\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(module_file_path, 'w') as f:\n",
    "        f.write(module_content)\n",
    "    \n",
    "    return module_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_inferno_test_kit(\n",
    "    api_type: str,\n",
    "    test_plan_file: str,\n",
    "    guidance_file: str = None,\n",
    "    module_name: str = \"PlanNet\",\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a test plan and generate an Inferno test kit\n",
    "    \n",
    "    Args:\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        test_plan_file: Path to test plan markdown file\n",
    "        guidance_file: Path to Inferno guidance file (optional)\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing statistics and paths\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting Inferno test generation with {api_type} for {module_name}\")\n",
    "    \n",
    "    # Initialize API clients and rate limiters\n",
    "    clients = setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = API_CONFIGS[api_type]\n",
    "    rate_limiter = create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Module name normalization for Ruby\n",
    "    module_name = ''.join(word.capitalize() for word in module_name.split())\n",
    "    \n",
    "    try:\n",
    "        # Parse test plan\n",
    "        test_specs = parse_test_plan(test_plan_file)\n",
    "        logger.info(f\"Parsed {len(test_specs)} test specifications from {test_plan_file}\")\n",
    "        \n",
    "        # Get Inferno guidance\n",
    "        if guidance_file and os.path.exists(guidance_file):\n",
    "            with open(guidance_file, 'r') as f:\n",
    "                inferno_guidance = f.read()\n",
    "            logger.info(f\"Loaded Inferno guidance from {guidance_file}\")\n",
    "        else:\n",
    "            inferno_guidance = get_inferno_guidance()\n",
    "            logger.info(\"Using default Inferno guidance\")\n",
    "        \n",
    "        # Assess feasibility of each test spec\n",
    "        testable_specs = []\n",
    "        untestable_specs = []\n",
    "        \n",
    "        for spec in test_specs:\n",
    "            assessment = assess_test_feasibility(client, api_type, spec, check_limits)\n",
    "            spec['assessment'] = assessment\n",
    "            \n",
    "            if assessment['testable']:\n",
    "                testable_specs.append(spec)\n",
    "            else:\n",
    "                untestable_specs.append(spec)\n",
    "        \n",
    "        logger.info(f\"Identified {len(testable_specs)} testable and {len(untestable_specs)} untestable requirements\")\n",
    "        \n",
    "        # Group testable specifications\n",
    "        grouped_tests = determine_test_groups(testable_specs)\n",
    "        \n",
    "        # Log the grouping results\n",
    "        logger.info(f\"Requirements grouped into {len(grouped_tests)} actor categories\")\n",
    "        for actor, resource_groups in grouped_tests.items():\n",
    "            logger.info(f\"Actor '{actor}': {len(resource_groups)} resource groups\")\n",
    "            for resource, specs in resource_groups.items():\n",
    "                logger.info(f\"  Resource '{resource}': {len(specs)} specifications\")\n",
    "        \n",
    "        # Create the file structure\n",
    "        test_file_map = create_file_structure(grouped_tests, module_name, output_dir)\n",
    "        logger.info(f\"Created file structure in {output_dir}\")\n",
    "        \n",
    "        # Generate and save tests\n",
    "        generated_tests = []\n",
    "        skipped_tests = []\n",
    "        \n",
    "        for actor, resource_groups in grouped_tests.items():\n",
    "            for resource, specs in resource_groups.items():\n",
    "                for i, spec in enumerate(specs):\n",
    "                    try:\n",
    "                        # Generate the test\n",
    "                        test_code = generate_inferno_test(\n",
    "                            client, api_type, spec, inferno_guidance, module_name, check_limits\n",
    "                        )\n",
    "                        \n",
    "                        # Write the test to file\n",
    "                        file_path = test_file_map[spec['id']]\n",
    "                        with open(file_path, 'w') as f:\n",
    "                            f.write(test_code)\n",
    "                        \n",
    "                        generated_tests.append(spec['id'])\n",
    "                        logger.info(f\"Generated test for {spec['id']} in {file_path}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error generating test for {spec['id']}: {str(e)}\")\n",
    "                        skipped_tests.append(spec['id'])\n",
    "                    \n",
    "                    # Add delay between requests\n",
    "                    if i < len(specs) - 1:  # No need to delay after the last request\n",
    "                        time.sleep(config[\"delay_between_chunks\"])\n",
    "        \n",
    "        # Generate the main module file\n",
    "        module_file_path = generate_module_file(module_name, output_dir, grouped_tests)\n",
    "        logger.info(f\"Generated main module file at {module_file_path}\")\n",
    "        \n",
    "        # Generate a report\n",
    "        report_path = os.path.join(output_dir, f\"{module_name.lower()}_test_report_{timestamp}.md\")\n",
    "        with open(\n",
    "            # Generate a report\n",
    "        report_path = os.path.join(output_dir, f\"{module_name.lower()}_test_report_{timestamp}.md\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(f\"# {module_name} Inferno Test Generation Report\\n\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            f.write(f\"## Summary\\n\\n\")\n",
    "            f.write(f\"- Total specifications: {len(test_specs)}\\n\")\n",
    "            f.write(f\"- Testable specifications: {len(testable_specs)}\\n\")\n",
    "            f.write(f\"- Untestable specifications: {len(untestable_specs)}\\n\")\n",
    "            f.write(f\"- Successfully generated tests: {len(generated_tests)}\\n\")\n",
    "            f.write(f\"- Failed to generate tests: {len(skipped_tests)}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"## Generated Tests\\n\\n\")\n",
    "            for spec_id in generated_tests:\n",
    "                f.write(f\"- {spec_id}\\n\")\n",
    "            \n",
    "            f.write(f\"\\n## Skipped Tests\\n\\n\")\n",
    "            for spec_id in skipped_tests:\n",
    "                f.write(f\"- {spec_id}\\n\")\n",
    "            \n",
    "            f.write(f\"\\n## Untestable Requirements\\n\\n\")\n",
    "            for spec in untestable_specs:\n",
    "                f.write(f\"- {spec['id']}: {spec['assessment'].get('reason', 'Unknown reason')}\\n\")\n",
    "        \n",
    "        logger.info(f\"Generated report at {report_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"total_specs\": len(test_specs),\n",
    "            \"testable_specs\": len(testable_specs),\n",
    "            \"untestable_specs\": len(untestable_specs),\n",
    "            \"generated_tests\": len(generated_tests),\n",
    "            \"skipped_tests\": len(skipped_tests),\n",
    "            \"module_dir\": os.path.join(output_dir, module_name.lower()),\n",
    "            \"module_file\": module_file_path,\n",
    "            \"report_file\": report_path\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Inferno tests: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_inferno_test_generator():\n",
    "    \"\"\"\n",
    "    Run the Inferno test generator with user input\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with generation results, or None if an error occurred\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get input from user or set default values\n",
    "    print(\"\\nFHIR Inferno Test Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Let user select the API\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    # Get test plan file path\n",
    "    test_plan_file = input(\"\\nEnter path to test plan markdown file: \")\n",
    "    \n",
    "    # Check if test plan file exists\n",
    "    if not os.path.exists(test_plan_file):\n",
    "        logger.error(f\"Test plan file not found: {test_plan_file}\")\n",
    "        print(f\"Error: Test plan file not found at {test_plan_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Get Inferno guidance file path (optional)\n",
    "    guidance_file = input(\"\\nEnter path to Inferno guidance file (optional, press Enter to skip): \")\n",
    "    \n",
    "    if guidance_file and not os.path.exists(guidance_file):\n",
    "        logger.warning(f\"Inferno guidance file not found: {guidance_file}\")\n",
    "        print(f\"Warning: Inferno guidance file not found at {guidance_file}. Using built-in guidance.\")\n",
    "        guidance_file = None\n",
    "    \n",
    "    # Get module name\n",
    "    module_name = input(\"\\nEnter module name (default 'PlanNet'): \") or \"PlanNet\"\n",
    "    \n",
    "    # Get output directory\n",
    "    output_dir = input(f\"\\nEnter output directory (default '{OUTPUT_DIR}'): \") or OUTPUT_DIR\n",
    "    \n",
    "    print(f\"\\nGenerating Inferno tests with {api_type.capitalize()}...\")\n",
    "    if guidance_file:\n",
    "        print(f\"Using Inferno guidance from {guidance_file}\")\n",
    "    print(f\"This may take several minutes depending on the number of requirements.\")\n",
    "    \n",
    "    try:\n",
    "        # Process test plan and generate Inferno tests\n",
    "        result = generate_inferno_test_kit(\n",
    "            api_type=api_type,\n",
    "            test_plan_file=test_plan_file,\n",
    "            guidance_file=guidance_file,\n",
    "            module_name=module_name,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        # Output results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Inferno test generation complete!\")\n",
    "        print(f\"Total specifications: {result['total_specs']}\")\n",
    "        print(f\"Testable specifications: {result['testable_specs']}\")\n",
    "        print(f\"Successfully generated tests: {result['generated_tests']}\")\n",
    "        print(f\"Module directory: {result['module_dir']}\")\n",
    "        print(f\"Main module file: {result['module_file']}\")\n",
    "        print(f\"Generation report: {result['report_file']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None\n",
    "\n",
    "# Execute the main function when run from a notebook cell\n",
    "if __name__ == \"__main__\":\n",
    "    run_inferno_test_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
