{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHIR Inferno Test Generator\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import httpx\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory\n",
    "OUTPUT_DIR = os.path.join(CURRENT_DIR, 'test_output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "API_CONFIGS = {\n",
    "    \"claude\": {\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\", \n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 1,\n",
    "        \"delay_between_batches\": 3,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.1\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-2.5-pro-preview-03-25\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 50000,\n",
    "        \"delay_between_requests\": 0.1,\n",
    "        \"timeout\": 60\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.3,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 450,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.15\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# System prompts for test generation\n",
    "INFERNO_TEST_SYSTEM_PROMPT = \"\"\"You are a specialized FHIR testing engineer with expertise in healthcare interoperability.\n",
    "Your task is to convert test specifications from a test plan into executable Ruby tests using the Inferno testing framework.\n",
    "You will generate valid, working Ruby code that follows Inferno test patterns and best practices.\"\"\"\n",
    "\n",
    "# Prompt template for test generation\n",
    "INFERNO_TEST_GENERATION_PROMPT = \"\"\"\n",
    "Convert the following FHIR test specification into executable Ruby code using the Inferno testing framework.\n",
    "\n",
    "Test Specification:\n",
    "{test_specification}\n",
    "\n",
    "Create an Inferno test implementation that:\n",
    "1. Follows the Inferno structure (TestGroup containing one or more tests)\n",
    "2. Implements the test logic described in the specification\n",
    "3. Makes appropriate FHIR API calls\n",
    "4. Includes proper assertions to validate the requirements\n",
    "5. Handles both success and error cases appropriately\n",
    "\n",
    "Naming conventions:\n",
    "- Use underscored lowercase names for files (e.g., patient_read_test.rb)\n",
    "- Use CamelCase for class names (e.g., PatientReadTest)\n",
    "- Class names should end with 'Test' or 'Group'\n",
    "- Give each test a unique ID that's descriptive and related to the requirement\n",
    "\n",
    "The test should be comprehensive but focused on exactly what's described in the specification.\n",
    "Include proper documentation in the code and follow Inferno best practices.\n",
    "\n",
    "Requirement ID: {requirement_id}\n",
    "Module Name: {module_name}\n",
    "\n",
    "The test should also be developed in accordance with this guidance on Inferno test development:\n",
    "{inferno_guidance}\n",
    "\n",
    "When developing the test:\n",
    "\n",
    "1. Define a TestGroup with a descriptive ID based on the requirement\n",
    "2. Create individual test cases within the group for each specific aspect to test\n",
    "3. Include detailed documentation in the description fields\n",
    "4. Use proper assertions that directly validate the requirement\n",
    "5. Provide meaningful error and success messages\n",
    "6. Consider both positive and negative test scenarios\n",
    "7. Add appropriate inputs for configurable test parameters\n",
    "\n",
    "Follow this pattern for the TestGroup structure:\n",
    "```ruby\n",
    "module {module_name}\n",
    "  class YourTestGroup < Inferno::TestGroup\n",
    "    id :your_unique_id\n",
    "    title 'Clear Requirement Title'\n",
    "    description %(\n",
    "      Detailed description of what this test validates, including:\n",
    "      - The specific requirement being tested\n",
    "      - How the test works\n",
    "      - What conformance is being verified\n",
    "    )\n",
    "    \n",
    "    # Tests go here\n",
    "  end\n",
    "end\n",
    "\n",
    "Return only the Ruby code for the test implementation, with no additional explanation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in API_CONFIGS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    config = API_CONFIGS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= config['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= config['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:] \n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < config['delay_between_requests']:\n",
    "        time.sleep(config['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        #Claude setup\n",
    "        verify_path = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "        http_client = httpx.Client(\n",
    "            verify=verify_path if os.path.exists(verify_path) else True,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        gemini.configure(api_key=gemini_api_key)\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "        openai_client = OpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gpt\": openai_client,\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def make_llm_request(client, api_type: str, prompt: str, system_prompt: str, rate_limit_func) -> str:\n",
    "    \"\"\"Make rate-limited API request with retries\"\"\"\n",
    "    rate_limit_func()\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }],\n",
    "                system=system_prompt\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            response = client.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            elif response.candidates:\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_plan(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a consolidated test plan into sections and requirements\n",
    "    focusing on the main requirements\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # First identify main sections\n",
    "    sections = {}\n",
    "    \n",
    "    # Define the main section names we're interested in\n",
    "    main_section_names = [\n",
    "        \"Application-Level Requirements\",\n",
    "        \"Authentication\",\n",
    "        \"Base Requirements\",\n",
    "        \"CORE Conformance\"\n",
    "    ]\n",
    "    \n",
    "    # Find all requirement headers in the document\n",
    "    req_pattern = r'### (REQ-\\d+): (.*?)\\n\\n\\*\\*Description\\*\\*: \"(.*?)\"\\n\\n\\*\\*Actor\\*\\*: (.*?)\\n\\n\\*\\*Conformance\\*\\*: (.*?)(?:\\n\\n|$)'\n",
    "    req_matches = re.findall(req_pattern, content, re.DOTALL)\n",
    "    \n",
    "    print(f\"Found {len(req_matches)} potential requirements\")\n",
    "    \n",
    "    # Initialize the sections structure\n",
    "    for section_name in main_section_names:\n",
    "        section_id = section_name.lower().replace(' ', '-')\n",
    "        sections[section_name] = {\n",
    "            'id': section_id,\n",
    "            'name': section_name,\n",
    "            'content': \"\",\n",
    "            'requirements': []\n",
    "        }\n",
    "    \n",
    "    # Process each requirement\n",
    "    for req_id, req_title, req_desc, req_actor, req_conf in req_matches:\n",
    "        print(f\"Processing requirement: {req_id}\")\n",
    "        \n",
    "        # Find the full test specification for this requirement\n",
    "        test_spec_pattern = f\"# Test Specification for {req_id}(.*?)(?:---|\\n## )\"\n",
    "        test_spec_match = re.search(test_spec_pattern, content, re.DOTALL)\n",
    "        test_spec = test_spec_match.group(1).strip() if test_spec_match else \"\"\n",
    "        \n",
    "        # Determine which section this requirement belongs to\n",
    "        for section_name in main_section_names:\n",
    "            section_start = content.find(f\"## {section_name}\")\n",
    "            next_section = None\n",
    "            for other_section in main_section_names:\n",
    "                if other_section != section_name:\n",
    "                    other_start = content.find(f\"## {other_section}\")\n",
    "                    if other_start > section_start and (next_section is None or other_start < next_section):\n",
    "                        next_section = other_start\n",
    "            \n",
    "            section_end = next_section if next_section else len(content)\n",
    "            section_content = content[section_start:section_end]\n",
    "            \n",
    "            if f\"### {req_id}\" in section_content:\n",
    "                # This requirement belongs to this section\n",
    "                requirement = {\n",
    "                    'id': req_id,\n",
    "                    'title': req_title.strip(),\n",
    "                    'description': req_desc.strip(),\n",
    "                    'actor': req_actor.strip(),\n",
    "                    'conformance': req_conf.strip(),\n",
    "                    'full_content': f\"### {req_id}: {req_title}\\n\\n**Description**: \\\"{req_desc}\\\"\\n\\n**Actor**: {req_actor}\\n\\n**Conformance**: {req_conf}\",\n",
    "                    'full_spec': test_spec,\n",
    "                    'section': section_name,\n",
    "                    'testability': 'Automatic'  # Default value\n",
    "                }\n",
    "                \n",
    "                sections[section_name]['requirements'].append(requirement)\n",
    "                print(f\"Added requirement {req_id} to section {section_name}\")\n",
    "                \n",
    "    # Remove empty sections\n",
    "    sections = {k: v for k, v in sections.items() if v['requirements']}\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inferno_guidance() -> str:\n",
    "    \"\"\"\n",
    "    Load the Inferno guidance document\n",
    "    \n",
    "    Returns:\n",
    "        String containing Inferno test development guidance\n",
    "    \"\"\"\n",
    "    guidance_path = os.path.join(PROJECT_ROOT, 'inferno-guidance.md')\n",
    "    \n",
    "    # Use a default guidance if file not found\n",
    "    if not os.path.exists(guidance_path):\n",
    "        return \"\"\"# Inferno Test Development Guidance\n",
    "        \n",
    "        Inferno is a Ruby-based testing framework for FHIR implementations. Tests should follow the structure:\n",
    "        \n",
    "        ```ruby\n",
    "        module YourTestKit\n",
    "          class YourTestGroup < Inferno::TestGroup\n",
    "            id :unique_id\n",
    "            title 'Test Group Title'\n",
    "            description 'Detailed description'\n",
    "            \n",
    "            test do\n",
    "              id :test_unique_id\n",
    "              title 'Test Title'\n",
    "              description 'Test description'\n",
    "              \n",
    "              run do\n",
    "                # Test implementation\n",
    "                assert condition, 'Failure message'\n",
    "              end\n",
    "            end\n",
    "          end\n",
    "        end\n",
    "        ```\n",
    "        \"\"\"\n",
    "    \n",
    "    with open(guidance_path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_test_groups(sections: Dict[str, Dict[str, Any]], expected_actors: List[str]) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Group test specifications based on the sections and actors from the test plan\n",
    "    \n",
    "    Args:\n",
    "        sections: Dictionary of sections from the parsed test plan\n",
    "        expected_actors: List of expected actors from user input\n",
    "        \n",
    "    Returns:\n",
    "        Nested dictionary of grouped test specifications\n",
    "    \"\"\"\n",
    "    # First, group by actor\n",
    "    actor_groups = defaultdict(list)\n",
    "    \n",
    "    # Create a mapping from normalized actor names to expected actor names\n",
    "    actor_mapping = {actor.lower().replace(' ', '_'): actor for actor in expected_actors}\n",
    "    \n",
    "    # Collect all requirements across all sections\n",
    "    for section_name, section in sections.items():\n",
    "        for req in section['requirements']:\n",
    "            # Use the exact actor name from the requirement\n",
    "            actor = req['actor'].strip()\n",
    "            if not actor:\n",
    "                actor = \"Unknown Actor\"\n",
    "            else:\n",
    "                # Try to match with expected actors (case-insensitive)\n",
    "                actor_lower = actor.lower().replace(' ', '_')\n",
    "                if actor_lower in actor_mapping:\n",
    "                    actor = actor_mapping[actor_lower]\n",
    "            \n",
    "            # Store the requirement with its section information\n",
    "            req_with_section = req.copy()\n",
    "            req_with_section['section_name'] = section_name\n",
    "            req_with_section['section_id'] = section['id']\n",
    "            \n",
    "            actor_groups[actor].append(req_with_section)\n",
    "    \n",
    "    # Only include actors that have requirements\n",
    "    result_groups = {}\n",
    "    for actor, reqs in actor_groups.items():\n",
    "        if reqs:  # Only include non-empty actor groups\n",
    "            if actor not in result_groups:\n",
    "                result_groups[actor] = {}\n",
    "            \n",
    "            # Group by original section\n",
    "            section_groups = defaultdict(list)\n",
    "            for req in reqs:\n",
    "                section_groups[req['section_name']].append(req)\n",
    "            \n",
    "            # Add the section groups to the result\n",
    "            result_groups[actor] = dict(section_groups)\n",
    "    \n",
    "    return result_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests_for_section(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    section: Dict[str, Any],\n",
    "    inferno_guidance: str,\n",
    "    module_name: str,\n",
    "    rate_limit_func,\n",
    "    max_token_limit: int = 16000\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate tests for an entire section or individual requirements based on token limits\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        section: Section dictionary containing requirements\n",
    "        inferno_guidance: Inferno test development guidance\n",
    "        module_name: Name of the module for the test\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        max_token_limit: Maximum tokens for the model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping requirement IDs to generated tests\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating tests for section: {section['name']}\")\n",
    "    \n",
    "    # First, try to generate tests for the entire section\n",
    "    if len(section['requirements']) > 1:\n",
    "        # Construct a prompt for the entire section\n",
    "        section_prompt = f\"\"\"\n",
    "        Generate Inferno tests for the following section of requirements from a FHIR implementation guide.\n",
    "        \n",
    "        Section: {section['name']}\n",
    "        \n",
    "        Requirements:\n",
    "        {section['content']}\n",
    "        \n",
    "        For each requirement, create a separate Inferno test class following the naming convention:\n",
    "        - Class name: {module_name}[Actor][Resource][REQ-ID]Test\n",
    "        - File name: req_[id]_test.rb\n",
    "        \n",
    "        Module Name: {module_name}\n",
    "        \n",
    "        Follow this Inferno development guidance:\n",
    "        {inferno_guidance}\n",
    "        \n",
    "        Return the Ruby code for each test implementation, separated by clear markers indicating the requirement ID.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate token count (rough approximation)\n",
    "        estimated_tokens = len(section_prompt) / 4  # Approximate 4 chars per token\n",
    "        \n",
    "        if estimated_tokens < max_token_limit:\n",
    "            try:\n",
    "                logger.info(f\"Attempting to generate tests for entire section: {section['name']}\")\n",
    "                response = make_llm_request(\n",
    "                    client, \n",
    "                    api_type, \n",
    "                    section_prompt, \n",
    "                    INFERNO_TEST_SYSTEM_PROMPT, \n",
    "                    rate_limit_func\n",
    "                )\n",
    "                \n",
    "                # Parse the response to extract individual tests\n",
    "                tests = {}\n",
    "                current_req = None\n",
    "                current_test = []\n",
    "                \n",
    "                for line in response.split('\\n'):\n",
    "                    # Look for markers indicating requirement boundaries\n",
    "                    req_marker = re.search(r'(REQ-\\d+)', line)\n",
    "                    if req_marker and (\"Test for\" in line or \"Implementation for\" in line or \"# Requirement\" in line):\n",
    "                        if current_req and current_test:\n",
    "                            tests[current_req] = '\\n'.join(current_test)\n",
    "                            current_test = []\n",
    "                        \n",
    "                        current_req = req_marker.group(1)\n",
    "                    \n",
    "                    if current_req:\n",
    "                        current_test.append(line)\n",
    "                \n",
    "                # Don't forget the last test\n",
    "                if current_req and current_test:\n",
    "                    tests[current_req] = '\\n'.join(current_test)\n",
    "                \n",
    "                # If we successfully parsed tests for all requirements, return them\n",
    "                if len(tests) == len(section['requirements']):\n",
    "                    logger.info(f\"Successfully generated tests for all requirements in section: {section['name']}\")\n",
    "                    return tests\n",
    "                \n",
    "                logger.warning(f\"Generated tests for only {len(tests)} of {len(section['requirements'])} requirements in section: {section['name']}\")\n",
    "                # Continue to individual requirement processing\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to generate tests for entire section: {str(e)}\")\n",
    "                # Continue to individual requirement processing\n",
    "    \n",
    "    # If section-level generation failed or wasn't attempted, generate tests for individual requirements\n",
    "    tests = {}\n",
    "    for requirement in section['requirements']:\n",
    "        try:\n",
    "            logger.info(f\"Generating test for requirement: {requirement['id']}\")\n",
    "            \n",
    "            # Prepare the prompt for this requirement with full context\n",
    "            req_prompt = INFERNO_TEST_GENERATION_PROMPT.format(\n",
    "                test_specification=requirement['full_content'],\n",
    "                requirement_id=requirement['id'],\n",
    "                module_name=module_name,\n",
    "                inferno_guidance=inferno_guidance\n",
    "            )\n",
    "            \n",
    "            # Generate the test\n",
    "            test_code = make_llm_request(\n",
    "                client, \n",
    "                api_type, \n",
    "                req_prompt, \n",
    "                INFERNO_TEST_SYSTEM_PROMPT, \n",
    "                rate_limit_func\n",
    "            )\n",
    "            \n",
    "            # Clean up any markdown formatting\n",
    "            if test_code.startswith('```ruby'):\n",
    "                test_code = re.sub(r'^```ruby\\n', '', test_code)\n",
    "                test_code = re.sub(r'\\n```$', '', test_code)\n",
    "            elif test_code.startswith('```'):\n",
    "                test_code = re.sub(r'^```\\n', '', test_code)\n",
    "                test_code = re.sub(r'\\n```$', '', test_code)\n",
    "            \n",
    "            tests[requirement['id']] = test_code\n",
    "            logger.info(f\"Successfully generated test for requirement: {requirement['id']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating test for requirement {requirement['id']}: {str(e)}\")\n",
    "    \n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_structure(grouped_tests: Dict[str, Dict[str, List[Dict[str, Any]]]], module_name: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Create the file structure for the Inferno tests\n",
    "    \n",
    "    Args:\n",
    "        grouped_tests: Nested dictionary of grouped test specs\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping test specs to file paths\n",
    "    \"\"\"\n",
    "    # Create base directories\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    os.makedirs(module_dir, exist_ok=True)\n",
    "    \n",
    "    # Create test group directories\n",
    "    test_file_map = {}\n",
    "    \n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        actor_dir = os.path.join(module_dir, actor.lower())\n",
    "        os.makedirs(actor_dir, exist_ok=True)\n",
    "        \n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            resource_dir = os.path.join(actor_dir, resource.lower())\n",
    "            os.makedirs(resource_dir, exist_ok=True)\n",
    "            \n",
    "            for spec in test_specs:\n",
    "                # Create a file name based on the requirement ID\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                file_name = f\"{req_id}_test.rb\"\n",
    "                file_path = os.path.join(resource_dir, file_name)\n",
    "                \n",
    "                test_file_map[spec['id']] = file_path\n",
    "    \n",
    "    return test_file_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_module_file(module_name: str, output_dir: str, grouped_tests: Dict[str, Dict[str, List[Dict[str, Any]]]]):\n",
    "    \"\"\"\n",
    "    Generate the main module file that includes all test groups\n",
    "    \n",
    "    Args:\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        grouped_tests: Nested dictionary of grouped test specs\n",
    "        \n",
    "    Returns:\n",
    "        Path to the generated module file\n",
    "    \"\"\"\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    module_file_path = os.path.join(module_dir, f\"{module_name.lower()}.rb\")\n",
    "    \n",
    "    # Generate module file content\n",
    "    module_content = f\"# {module_name} Inferno Test Suite\\n\"\n",
    "    module_content += \"# Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\"\n",
    "    module_content += \"require 'inferno/dsl/test_suite'\\n\\n\"\n",
    "    \n",
    "    # Add requires for all test groups\n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        # Create safe versions for file paths\n",
    "        actor_safe = re.sub(r'[^a-zA-Z0-9_]', '_', actor.lower())\n",
    "        \n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            # Create safe versions for file paths\n",
    "            resource_safe = re.sub(r'[^a-zA-Z0-9_]', '_', resource.lower())\n",
    "            \n",
    "            module_content += f\"# {actor} {resource} tests\\n\"\n",
    "            for spec in test_specs:\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                module_content += f\"require_relative '{actor_safe}/{resource_safe}/{req_id}_test'\\n\"\n",
    "            module_content += \"\\n\"\n",
    "    \n",
    "    # Add module definition\n",
    "    module_content += f\"module {module_name}\\n\"\n",
    "    \n",
    "    # Add test suite class\n",
    "    class_name = f\"{module_name}TestSuite\"\n",
    "    module_content += f\"  class {class_name} < Inferno::TestSuite\\n\"\n",
    "    module_content += f\"    id :{module_name.lower()}_suite\\n\"\n",
    "    module_content += f\"    title '{module_name} Test Suite'\\n\"\n",
    "    module_content += f\"    description 'Test suite for validating {module_name} Implementation Guide conformance'\\n\\n\"\n",
    "    \n",
    "    # Add actor groups\n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        # Create safe version for Ruby identifiers\n",
    "        actor_safe = re.sub(r'[^a-zA-Z0-9_]', '_', actor.lower())\n",
    "        \n",
    "        module_content += f\"    # {actor} tests\\n\"\n",
    "        module_content += f\"    group do\\n\"\n",
    "        module_content += f\"      id :{actor_safe}_group\\n\"\n",
    "        module_content += f\"      title '{actor} Tests'\\n\\n\"\n",
    "        \n",
    "        # Add resource groups\n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            # Skip empty resource groups\n",
    "            if not test_specs:\n",
    "                continue\n",
    "                \n",
    "            # Create safe version for Ruby identifiers\n",
    "            resource_safe = re.sub(r'[^a-zA-Z0-9_]', '_', resource.lower())\n",
    "            \n",
    "            module_content += f\"      # {resource} tests\\n\"\n",
    "            module_content += f\"      group do\\n\"\n",
    "            module_content += f\"        id :{actor_safe}_{resource_safe}_group\\n\"\n",
    "            module_content += f\"        title '{resource} Tests'\\n\\n\"\n",
    "            \n",
    "            # Add references to individual tests\n",
    "            for spec in test_specs:\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                module_content += f\"        test from: :{req_id}_test\\n\"\n",
    "            \n",
    "            module_content += \"      end\\n\\n\"\n",
    "        \n",
    "        module_content += \"    end\\n\\n\"\n",
    "    \n",
    "    module_content += \"  end\\n\"\n",
    "    module_content += \"end\\n\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(module_file_path, 'w') as f:\n",
    "        f.write(module_content)\n",
    "    \n",
    "    return module_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inferno_test_kit(\n",
    "    api_type: str,\n",
    "    test_plan_file: str,\n",
    "    guidance_file: str = None,\n",
    "    module_name: str = \"PlanNet\",\n",
    "    output_dir: str = OUTPUT_DIR,\n",
    "    expected_actors: List[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a test plan and generate an Inferno test kit\n",
    "    \n",
    "    Args:\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        test_plan_file: Path to test plan markdown file\n",
    "        guidance_file: Path to Inferno guidance file (optional)\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        expected_actors: List of expected actors in the test plan\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing statistics and paths\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting Inferno test generation with {api_type} for {module_name}\")\n",
    "    \n",
    "    # Initialize API clients and rate limiters\n",
    "    clients = setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = API_CONFIGS[api_type]\n",
    "    rate_limiter = create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Module name normalization for Ruby\n",
    "    module_name = ''.join(word.capitalize() for word in module_name.split())\n",
    "    \n",
    "    # Set default actors if none provided\n",
    "    if not expected_actors:\n",
    "        expected_actors = [\"Health Plan API Actor\", \"Application Actor\"]\n",
    "    \n",
    "    try:\n",
    "        # Parse test plan into sections\n",
    "        sections = parse_test_plan(test_plan_file)\n",
    "        logger.info(f\"Parsed test plan into {len(sections)} sections\")\n",
    "        \n",
    "        # Count total requirements\n",
    "        total_requirements = sum(len(section['requirements']) for section in sections.values())\n",
    "        logger.info(f\"Found {total_requirements} total requirements\")\n",
    "        \n",
    "        # Get Inferno guidance\n",
    "        if guidance_file and os.path.exists(guidance_file):\n",
    "            with open(guidance_file, 'r') as f:\n",
    "                inferno_guidance = f.read()\n",
    "            logger.info(f\"Loaded Inferno guidance from {guidance_file}\")\n",
    "        else:\n",
    "            inferno_guidance = get_inferno_guidance()\n",
    "            logger.info(\"Using default Inferno guidance\")\n",
    "        \n",
    "        # Generate tests by section\n",
    "        all_tests = {}\n",
    "        for section_name, section in sections.items():\n",
    "            logger.info(f\"Processing section: {section_name} with {len(section['requirements'])} requirements\")\n",
    "            \n",
    "            # Skip empty sections\n",
    "            if not section['requirements']:\n",
    "                continue\n",
    "                \n",
    "            # Generate tests for this section\n",
    "            section_tests = generate_tests_for_section(\n",
    "                client, \n",
    "                api_type, \n",
    "                section, \n",
    "                inferno_guidance, \n",
    "                module_name, \n",
    "                check_limits,\n",
    "                config['max_tokens']\n",
    "            )\n",
    "            \n",
    "            # Add to our collection\n",
    "            all_tests.update(section_tests)\n",
    "            \n",
    "            # Add delay between sections\n",
    "            time.sleep(config[\"delay_between_batches\"])\n",
    "            \n",
    "        logger.info(f\"Generated tests for {len(all_tests)} requirements\")\n",
    "        \n",
    "        # Group requirements by actor and section (original groups from test plan)\n",
    "        grouped_reqs = determine_test_groups(sections, expected_actors)\n",
    "        \n",
    "        # Ensure all expected actors are represented, even if empty\n",
    "        for actor in expected_actors:\n",
    "            if actor not in grouped_reqs:\n",
    "                grouped_reqs[actor] = {}\n",
    "        \n",
    "        # Create file structure and write tests\n",
    "        os.makedirs(os.path.join(output_dir, module_name.lower()), exist_ok=True)\n",
    "        \n",
    "        file_paths = {}\n",
    "        for actor, section_groups in grouped_reqs.items():\n",
    "            actor_safe = re.sub(r'[^a-zA-Z0-9_]', '_', actor.lower())\n",
    "            actor_dir = os.path.join(output_dir, module_name.lower(), actor_safe)\n",
    "            os.makedirs(actor_dir, exist_ok=True)\n",
    "            \n",
    "            for section_name, reqs in section_groups.items():\n",
    "                section_safe = re.sub(r'[^a-zA-Z0-9_]', '_', section_name.lower())\n",
    "                section_dir = os.path.join(actor_dir, section_safe)\n",
    "                os.makedirs(section_dir, exist_ok=True)\n",
    "                \n",
    "                for req in reqs:\n",
    "                    # Skip requirements we couldn't generate tests for\n",
    "                    if req['id'] not in all_tests:\n",
    "                        continue\n",
    "                        \n",
    "                    file_name = f\"{req['id'].lower().replace('-', '_')}_test.rb\"\n",
    "                    file_path = os.path.join(section_dir, file_name)\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(all_tests[req['id']])\n",
    "                    \n",
    "                    file_paths[req['id']] = file_path\n",
    "                    logger.info(f\"Wrote test for {req['id']} to {file_path}\")\n",
    "        \n",
    "        # Generate module file\n",
    "        module_file = generate_module_file(module_name, output_dir, grouped_reqs)\n",
    "        \n",
    "        return {\n",
    "            \"total_sections\": len(sections),\n",
    "            \"total_requirements\": total_requirements,\n",
    "            \"generated_tests\": len(all_tests),\n",
    "            \"module_dir\": os.path.join(output_dir, module_name.lower()),\n",
    "            \"module_file\": module_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Inferno tests: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_module_file(module_name: str, output_dir: str, grouped_reqs) -> str:\n",
    "    \"\"\"\n",
    "    Generate the main module file that includes all test groups\n",
    "    \n",
    "    Args:\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        grouped_reqs: Dictionary of requirements grouped by actor and section\n",
    "        \n",
    "    Returns:\n",
    "        Path to the generated module file\n",
    "    \"\"\"\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    module_file_path = os.path.join(module_dir, f\"{module_name.lower()}.rb\")\n",
    "    \n",
    "    # Generate module file content\n",
    "    module_content = f\"# {module_name} Inferno Test Suite\\n\"\n",
    "    module_content += \"# Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\"\n",
    "    module_content += \"require 'inferno/dsl/test_suite'\\n\\n\"\n",
    "    \n",
    "    # Add requires for all test groups\n",
    "    for actor, section_groups in grouped_reqs.items():\n",
    "        actor_safe = actor.replace(\" \", \"_\").lower()\n",
    "        for section_name, reqs in section_groups.items():\n",
    "            section_safe = section_name.replace(\" \", \"_\").lower()\n",
    "            module_content += f\"# {actor} - {section_name} tests\\n\"\n",
    "            for req in reqs:\n",
    "                req_id_safe = req['id'].lower().replace('-', '_')\n",
    "                module_content += f\"require_relative '{actor_safe}/{section_safe}/req_{req_id_safe}_test'\\n\"\n",
    "            module_content += \"\\n\"\n",
    "    \n",
    "    # Add module definition\n",
    "    module_content += f\"module {module_name}\\n\"\n",
    "    \n",
    "    # Add test suite class\n",
    "    class_name = f\"{module_name}TestSuite\"\n",
    "    module_content += f\"  class {class_name} < Inferno::TestSuite\\n\"\n",
    "    module_content += f\"    id :{module_name.lower()}_suite\\n\"\n",
    "    module_content += f\"    title '{module_name} Test Suite'\\n\"\n",
    "    module_content += f\"    description 'Test suite for validating {module_name} Implementation Guide conformance'\\n\\n\"\n",
    "    \n",
    "    # Add actor groups\n",
    "    for actor, section_groups in grouped_reqs.items():\n",
    "        actor_safe = actor.replace(\" \", \"_\").lower()\n",
    "        module_content += f\"    # {actor} tests\\n\"\n",
    "        module_content += f\"    group do\\n\"\n",
    "        module_content += f\"      id :{actor_safe}_group\\n\"\n",
    "        module_content += f\"      title '{actor} Tests'\\n\\n\"\n",
    "        \n",
    "        # Add section groups\n",
    "        for section_name, reqs in section_groups.items():\n",
    "            section_safe = section_name.replace(\" \", \"_\").lower()\n",
    "            module_content += f\"      # {section_name} tests\\n\"\n",
    "            module_content += f\"      group do\\n\"\n",
    "            module_content += f\"        id :{actor_safe}_{section_safe}_group\\n\"\n",
    "            module_content += f\"        title '{section_name} Tests'\\n\\n\"\n",
    "            \n",
    "            # Add references to individual tests\n",
    "            for req in reqs:\n",
    "                req_id_safe = req['id'].lower().replace('-', '_')\n",
    "                module_content += f\"        test from: :req_{req_id_safe}_test\\n\"\n",
    "            \n",
    "            module_content += \"      end\\n\\n\"\n",
    "        \n",
    "        module_content += \"    end\\n\\n\"\n",
    "    \n",
    "    module_content += \"  end\\n\"\n",
    "    module_content += \"end\\n\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(module_file_path, 'w') as f:\n",
    "        f.write(module_content)\n",
    "    \n",
    "    return module_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inferno_test_generator():\n",
    "    \"\"\"\n",
    "    Run the Inferno test generator with user input\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with generation results, or None if an error occurred\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get input from user or set default values\n",
    "    print(\"\\nFHIR Inferno Test Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Let user select the API\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    # Get test plan file path\n",
    "    test_plan_file = input(\"\\nEnter path to test plan markdown file: \")\n",
    "    \n",
    "    # Check if test plan file exists\n",
    "    if not os.path.exists(test_plan_file):\n",
    "        logger.error(f\"Test plan file not found: {test_plan_file}\")\n",
    "        print(f\"Error: Test plan file not found at {test_plan_file}\")\n",
    "        return None\n",
    "    \n",
    "    # the path to the guidance file\n",
    "    guidance_file = os.path.join(CURRENT_DIR, \"inferno-guidance.md\")\n",
    "    \n",
    "    # Check if guidance file exists\n",
    "    if not os.path.exists(guidance_file):\n",
    "        logger.warning(f\"Inferno guidance file not found: {guidance_file}\")\n",
    "        print(f\"Warning: Inferno guidance file not found at {guidance_file}. Using built-in guidance.\")\n",
    "        guidance_file = None\n",
    "    else:\n",
    "        print(f\"Using guidance file: {guidance_file}\")\n",
    "    \n",
    "    # Get module name\n",
    "    module_name = input(\"\\nEnter module name (default 'PlanNet'): \") or \"PlanNet\"\n",
    "    \n",
    "    # Get actor information\n",
    "    print(\"\\nEnter the actors from the test plan (comma-separated, e.g., 'Health Plan API Actor, Application Actor'):\")\n",
    "    actors_input = input(\"Actors: \")\n",
    "    \n",
    "    if actors_input:\n",
    "        expected_actors = [actor.strip() for actor in actors_input.split(',')]\n",
    "    else:\n",
    "        # Default actors if none provided\n",
    "        expected_actors = [\"Health Plan API Actor\", \"Application Actor\"]\n",
    "    \n",
    "    # Get output directory\n",
    "    output_dir = input(f\"\\nEnter output directory (default '{OUTPUT_DIR}'): \") or OUTPUT_DIR\n",
    "    \n",
    "    print(f\"\\nGenerating Inferno tests with {api_type.capitalize()}...\")\n",
    "    print(f\"Using actors: {', '.join(expected_actors)}\")\n",
    "    if guidance_file:\n",
    "        print(f\"Using Inferno guidance from {guidance_file}\")\n",
    "    print(f\"This may take several minutes depending on the number of requirements.\")\n",
    "    \n",
    "    try:\n",
    "        # Process test plan and generate Inferno tests\n",
    "        result = generate_inferno_test_kit(\n",
    "            api_type=api_type,\n",
    "            test_plan_file=test_plan_file,\n",
    "            guidance_file=guidance_file,\n",
    "            module_name=module_name,\n",
    "            output_dir=output_dir,\n",
    "            expected_actors=expected_actors\n",
    "        )\n",
    "        \n",
    "        # Output results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Inferno test generation complete!\")\n",
    "        print(f\"Total sections: {result['total_sections']}\")\n",
    "        print(f\"Total requirements: {result['total_requirements']}\")\n",
    "        print(f\"Successfully generated tests: {result['generated_tests']}\")\n",
    "        print(f\"Module directory: {result['module_dir']}\")\n",
    "        print(f\"Main module file: {result['module_file']}\")\n",
    "        #print(f\"Generation report: {result['report_file']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FHIR Inferno Test Generator\n",
      "==================================================\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n",
      "Using guidance file: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/inferno-guidance.md\n",
      "\n",
      "Enter the actors from the test plan (comma-separated, e.g., 'Health Plan API Actor, Application Actor'):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:52:10,722 - __main__ - INFO - Starting Inferno test generation with claude for test3_claude\n",
      "2025-04-24 10:52:10,750 - __main__ - INFO - Parsed test plan into 4 sections\n",
      "2025-04-24 10:52:10,751 - __main__ - INFO - Found 4 total requirements\n",
      "2025-04-24 10:52:10,752 - __main__ - INFO - Loaded Inferno guidance from /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/inferno-guidance.md\n",
      "2025-04-24 10:52:10,752 - __main__ - INFO - Processing section: Application-Level Requirements with 1 requirements\n",
      "2025-04-24 10:52:10,752 - __main__ - INFO - Generating tests for section: Application-Level Requirements\n",
      "2025-04-24 10:52:10,752 - __main__ - INFO - Generating test for requirement: REQ-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Inferno tests with Claude...\n",
      "Using actors: Application Actor, Health Plan API\n",
      "Using Inferno guidance from /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/inferno-guidance.md\n",
      "This may take several minutes depending on the number of requirements.\n",
      "Found 4 potential requirements\n",
      "Processing requirement: REQ-08\n",
      "Added requirement REQ-08 to section Application-Level Requirements\n",
      "Processing requirement: REQ-01\n",
      "Added requirement REQ-01 to section Authentication\n",
      "Processing requirement: REQ-09\n",
      "Added requirement REQ-09 to section Base Requirements\n",
      "Processing requirement: REQ-07\n",
      "Added requirement REQ-07 to section CORE Conformance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:52:24,610 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 10:52:24,613 - __main__ - INFO - Successfully generated test for requirement: REQ-08\n",
      "2025-04-24 10:52:27,619 - __main__ - INFO - Processing section: Authentication with 1 requirements\n",
      "2025-04-24 10:52:27,621 - __main__ - INFO - Generating tests for section: Authentication\n",
      "2025-04-24 10:52:27,621 - __main__ - INFO - Generating test for requirement: REQ-01\n",
      "2025-04-24 10:52:36,562 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 10:52:36,564 - __main__ - INFO - Successfully generated test for requirement: REQ-01\n",
      "2025-04-24 10:52:39,570 - __main__ - INFO - Processing section: Base Requirements with 1 requirements\n",
      "2025-04-24 10:52:39,571 - __main__ - INFO - Generating tests for section: Base Requirements\n",
      "2025-04-24 10:52:39,572 - __main__ - INFO - Generating test for requirement: REQ-09\n",
      "2025-04-24 10:52:51,343 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 10:52:51,346 - __main__ - INFO - Successfully generated test for requirement: REQ-09\n",
      "2025-04-24 10:52:54,352 - __main__ - INFO - Processing section: CORE Conformance with 1 requirements\n",
      "2025-04-24 10:52:54,354 - __main__ - INFO - Generating tests for section: CORE Conformance\n",
      "2025-04-24 10:52:54,355 - __main__ - INFO - Generating test for requirement: REQ-07\n",
      "2025-04-24 10:53:08,239 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 10:53:08,241 - __main__ - INFO - Successfully generated test for requirement: REQ-07\n",
      "2025-04-24 10:53:11,247 - __main__ - INFO - Generated tests for 4 requirements\n",
      "2025-04-24 10:53:11,257 - __main__ - INFO - Wrote test for REQ-08 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test3_claude/application_actor/application_level_requirements/req_08_test.rb\n",
      "2025-04-24 10:53:11,259 - __main__ - INFO - Wrote test for REQ-09 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test3_claude/application_actor/base_requirements/req_09_test.rb\n",
      "2025-04-24 10:53:11,261 - __main__ - INFO - Wrote test for REQ-07 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test3_claude/application_actor/core_conformance/req_07_test.rb\n",
      "2025-04-24 10:53:11,262 - __main__ - INFO - Wrote test for REQ-01 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test3_claude/health_plan_api/authentication/req_01_test.rb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Inferno test generation complete!\n",
      "Total sections: 4\n",
      "Total requirements: 4\n",
      "Successfully generated tests: 4\n",
      "Module directory: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test3_claude\n",
      "Main module file: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test3_claude/test3_claude.rb\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_sections': 4,\n",
       " 'total_requirements': 4,\n",
       " 'generated_tests': 4,\n",
       " 'module_dir': '/Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test3_claude',\n",
       " 'module_file': '/Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test3_claude/test3_claude.rb'}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the test generator\n",
    "run_inferno_test_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
