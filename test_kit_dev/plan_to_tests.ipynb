{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Inferno Test Generator\n",
    "\n",
    "This notebook generates executable Inferno test files from FHIR Implementation Guide test specifications. The output consists of Ruby tests that can be used to validate FHIR server implementations.\n",
    "\n",
    "#### What it does\n",
    "\n",
    "- Processes test specifications from a consolidated test plan\n",
    "- Analyzes requirements and transforms them into executable test code\n",
    "- Generates comprehensive Ruby tests that follow the Inferno testing framework structure\n",
    "- Creates a complete, organized test kit with proper file structure\n",
    "\n",
    "#### How to use\n",
    "\n",
    "1. **Setup**: Individual cert setup may need to be modified in `setup_clients()` function of the llm_utils.py file before running this notebook. API keys should be in .env file. Make sure you have an API key for at least one of:\n",
    "   - Anthropic Claude (`ANTHROPIC_API_KEY`)\n",
    "   - Google Gemini (`GEMINI_API_KEY`) \n",
    "   - OpenAI GPT-4 (`OPENAI_API_KEY`)\n",
    "\n",
    "2. **Input**: A markdown file with test specifications in the following format:\n",
    "   ```markdown\n",
    "   ### REQ-ID: Requirement Title\n",
    "\n",
    "   **Description**: \"Detailed description of the requirement\"\n",
    "\n",
    "   **Actor**: System component responsible\n",
    "\n",
    "   **Conformance**: SHALL/SHOULD/MAY\n",
    "\n",
    "   # Test Specification for REQ-ID\n",
    "   ... detailed test specification ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHIR Inferno Test Generator\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import httpx\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Constants\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory\n",
    "OUTPUT_DIR = os.path.join(CURRENT_DIR, 'test_output')\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level to project root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "module_path = os.path.join(PROJECT_ROOT, 'llm_utils.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"llm_utils\", module_path)\n",
    "llm_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(llm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt utilities\n",
    "prompt_utils_path = os.path.join(PROJECT_ROOT, 'prompt_utils.py')\n",
    "spec = importlib.util.spec_from_file_location(\"prompt_utils\", prompt_utils_path)\n",
    "prompt_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(prompt_utils)\n",
    "\n",
    "# Setup the prompt environment\n",
    "prompt_env = prompt_utils.setup_prompt_environment(PROJECT_ROOT)\n",
    "PROMPT_DIR = prompt_env[\"prompt_dir\"]\n",
    "TEST_GEN_PATH = prompt_env[\"test_gen_path\"]\n",
    "\n",
    "logging.info(f\"Using prompts directory: {PROMPT_DIR}\")\n",
    "logging.info(f\"Inferno test generation prompt: {TEST_GEN_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts for test generation\n",
    "INFERNO_TEST_SYSTEM_PROMPT = \"\"\"You are a specialized FHIR testing engineer with expertise in healthcare interoperability.\n",
    "Your task is to convert test specifications from a test plan into executable Ruby tests using the Inferno testing framework.\n",
    "You will generate valid, working Ruby code that follows Inferno test patterns and best practices.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inferno_test_generation_prompt(test_specification: str, requirement_id: str, module_name: str, inferno_guidance: str) -> str:\n",
    "    \"\"\"\n",
    "    Load the Inferno test generation prompt from file and format it with test details\n",
    "    \n",
    "    Args:\n",
    "        test_specification: The test specification content\n",
    "        requirement_id: The ID of the requirement\n",
    "        module_name: The name of the module\n",
    "        inferno_guidance: The Inferno guidance\n",
    "        \n",
    "    Returns:\n",
    "        str: The formatted prompt for the LLM\n",
    "    \"\"\"\n",
    "    return prompt_utils.load_prompt(\n",
    "        TEST_GEN_PATH,\n",
    "        test_specification=test_specification,\n",
    "        requirement_id=requirement_id,\n",
    "        module_name=module_name,\n",
    "        inferno_guidance=inferno_guidance\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_plan(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a consolidated test plan into sections and requirements\n",
    "    focusing on the main requirements\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Initialize empty sections dictionary\n",
    "    sections = {}\n",
    "    \n",
    "    # First try to extract sections from the TOC\n",
    "    toc_pattern = r'## Table of Contents\\n(.*?)(?:\\n## |\\n# |\\Z)'\n",
    "    toc_match = re.search(toc_pattern, content, re.DOTALL)\n",
    "    \n",
    "    if toc_match:\n",
    "        toc_content = toc_match.group(1)\n",
    "        # Extract section names from the TOC\n",
    "        section_pattern = r'- \\[(.*?)\\]'\n",
    "        section_matches = re.findall(section_pattern, toc_content)\n",
    "        \n",
    "        # Only use top-level sections (not the REQ items)\n",
    "        top_level_sections = [s for s in section_matches if not s.startswith('REQ-')]\n",
    "        \n",
    "        # Create section entries for each section found\n",
    "        for section_name in top_level_sections:\n",
    "            clean_section_name = section_name.strip()\n",
    "            section_id = clean_section_name.lower().replace(' ', '-')\n",
    "            sections[clean_section_name] = {\n",
    "                'id': section_id,\n",
    "                'name': clean_section_name,\n",
    "                'content': \"\",\n",
    "                'requirements': []\n",
    "            }\n",
    "    \n",
    "    # If no sections found in TOC, try to find sections from the document headings\n",
    "    if not sections:\n",
    "        # Look for section headers (##) that are not part of the TOC\n",
    "        section_pattern = r'## ([^\\n#]+)(?=\\n(?!## Table of Contents))'\n",
    "        section_matches = re.findall(section_pattern, content)\n",
    "        \n",
    "        for section_name in section_matches:\n",
    "            clean_section_name = section_name.strip()\n",
    "            # Skip if this is a \"Test Specification\" section\n",
    "            if \"Test Specification\" in clean_section_name:\n",
    "                continue\n",
    "                \n",
    "            section_id = clean_section_name.lower().replace(' ', '-')\n",
    "            sections[clean_section_name] = {\n",
    "                'id': section_id,\n",
    "                'name': clean_section_name,\n",
    "                'content': \"\",\n",
    "                'requirements': []\n",
    "            }\n",
    "    \n",
    "    # Find all requirement headers in the document\n",
    "    req_pattern = r'### (REQ-\\d+): (.*?)\\n\\n\\*\\*Description\\*\\*: \"(.*?)\"\\n\\n\\*\\*Actor\\*\\*: (.*?)\\n\\n\\*\\*Conformance\\*\\*: (.*?)(?:\\n\\n|$)'\n",
    "    req_matches = re.findall(req_pattern, content, re.DOTALL)\n",
    "    \n",
    "    print(f\"Found {len(req_matches)} potential requirements\")\n",
    "    \n",
    "    # Process each requirement\n",
    "    for req_id, req_title, req_desc, req_actor, req_conf in req_matches:\n",
    "        print(f\"Processing requirement: {req_id}\")\n",
    "        \n",
    "        # Find the full test specification for this requirement\n",
    "        test_spec_pattern = f\"# Test Specification for {req_id}(.*?)(?:---|\\\\n## |$)\"\n",
    "        test_spec_match = re.search(test_spec_pattern, content, re.DOTALL)\n",
    "        test_spec = test_spec_match.group(1).strip() if test_spec_match else \"\"\n",
    "        \n",
    "        # Try to determine which section this requirement belongs to\n",
    "        assigned_to_section = False\n",
    "        \n",
    "        # First try to find the requirement in one of the identified sections\n",
    "        for section_name, section in sections.items():\n",
    "            section_start = content.find(f\"## {section_name}\")\n",
    "            if section_start == -1:\n",
    "                continue\n",
    "                \n",
    "            # Find the next section to determine where this section ends\n",
    "            next_section_start = len(content)\n",
    "            for other_section in sections.keys():\n",
    "                if other_section != section_name:\n",
    "                    other_start = content.find(f\"## {other_section}\", section_start + len(section_name))\n",
    "                    if other_start > section_start and other_start < next_section_start:\n",
    "                        next_section_start = other_start\n",
    "            \n",
    "            # Extract the content of this section\n",
    "            section_content = content[section_start:next_section_start]\n",
    "            \n",
    "            if f\"### {req_id}\" in section_content:\n",
    "                # This requirement belongs to this section\n",
    "                requirement = {\n",
    "                    'id': req_id,\n",
    "                    'title': req_title.strip(),\n",
    "                    'description': req_desc.strip(),\n",
    "                    'actor': req_actor.strip(),\n",
    "                    'conformance': req_conf.strip(),\n",
    "                    'full_content': f\"### {req_id}: {req_title}\\n\\n**Description**: \\\"{req_desc}\\\"\\n\\n**Actor**: {req_actor}\\n\\n**Conformance**: {req_conf}\",\n",
    "                    'full_spec': test_spec,\n",
    "                    'section': section_name,\n",
    "                    'testability': 'Automatic'  # Default value\n",
    "                }\n",
    "                \n",
    "                sections[section_name]['requirements'].append(requirement)\n",
    "                print(f\"Added requirement {req_id} to section {section_name}\")\n",
    "                assigned_to_section = True\n",
    "                break\n",
    "        \n",
    "        # If we couldn't find the requirement in any of our sections,\n",
    "        # create a section based on the actor if it doesn't exist already\n",
    "        if not assigned_to_section:\n",
    "            actor = req_actor.strip()\n",
    "            if not actor:\n",
    "                actor = \"Unknown Actor\"\n",
    "                \n",
    "            if actor not in sections:\n",
    "                section_id = actor.lower().replace(' ', '-')\n",
    "                sections[actor] = {\n",
    "                    'id': section_id,\n",
    "                    'name': actor,\n",
    "                    'content': \"\",\n",
    "                    'requirements': []\n",
    "                }\n",
    "            \n",
    "            requirement = {\n",
    "                'id': req_id,\n",
    "                'title': req_title.strip(),\n",
    "                'description': req_desc.strip(),\n",
    "                'actor': actor,\n",
    "                'conformance': req_conf.strip(),\n",
    "                'full_content': f\"### {req_id}: {req_title}\\n\\n**Description**: \\\"{req_desc}\\\"\\n\\n**Actor**: {req_actor}\\n\\n**Conformance**: {req_conf}\",\n",
    "                'full_spec': test_spec,\n",
    "                'section': actor,\n",
    "                'testability': 'Automatic'  # Default value\n",
    "            }\n",
    "            \n",
    "            sections[actor]['requirements'].append(requirement)\n",
    "            print(f\"Added requirement {req_id} to actor-based section {actor}\")\n",
    "    \n",
    "    # Remove empty sections\n",
    "    sections = {k: v for k, v in sections.items() if v['requirements']}\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inferno_guidance() -> str:\n",
    "    \"\"\"\n",
    "    Load the Inferno guidance document\n",
    "    \n",
    "    Returns:\n",
    "        String containing Inferno test development guidance\n",
    "    \"\"\"\n",
    "    guidance_path = os.path.join(PROJECT_ROOT, 'inferno-guidance.md')\n",
    "    \n",
    "    # Use a default guidance if file not found\n",
    "    if not os.path.exists(guidance_path):\n",
    "        return \"\"\"# Inferno Test Development Guidance\n",
    "        \n",
    "        Inferno is a Ruby-based testing framework for FHIR implementations. Tests should follow the structure:\n",
    "        \n",
    "        ```ruby\n",
    "        module YourTestKit\n",
    "          class YourTestGroup < Inferno::TestGroup\n",
    "            id :unique_id\n",
    "            title 'Test Group Title'\n",
    "            description 'Detailed description'\n",
    "            \n",
    "            test do\n",
    "              id :test_unique_id\n",
    "              title 'Test Title'\n",
    "              description 'Test description'\n",
    "              \n",
    "              run do\n",
    "                # Test implementation\n",
    "                assert condition, 'Failure message'\n",
    "              end\n",
    "            end\n",
    "          end\n",
    "        end\n",
    "        ```\n",
    "        \"\"\"\n",
    "    \n",
    "    with open(guidance_path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_test_groups(sections: Dict[str, Dict[str, Any]], expected_actors: List[str]) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Group test specifications based on the sections and actors from the test plan\n",
    "    \n",
    "    Args:\n",
    "        sections: Dictionary of sections from the parsed test plan\n",
    "        expected_actors: List of expected actors from user input\n",
    "        \n",
    "    Returns:\n",
    "        Nested dictionary of grouped test specifications\n",
    "    \"\"\"\n",
    "    # First, group by actor\n",
    "    actor_groups = defaultdict(list)\n",
    "    \n",
    "    # Create a mapping from normalized actor names to expected actor names\n",
    "    actor_mapping = {actor.lower().replace(' ', '_'): actor for actor in expected_actors}\n",
    "    \n",
    "    # Collect all requirements across all sections\n",
    "    for section_name, section in sections.items():\n",
    "        for req in section['requirements']:\n",
    "            # Use the exact actor name from the requirement\n",
    "            actor = req['actor'].strip()\n",
    "            if not actor:\n",
    "                actor = \"Unknown Actor\"\n",
    "            else:\n",
    "                # Try to match with expected actors (case-insensitive)\n",
    "                actor_lower = actor.lower().replace(' ', '_')\n",
    "                if actor_lower in actor_mapping:\n",
    "                    actor = actor_mapping[actor_lower]\n",
    "            \n",
    "            # Store the requirement with its section information\n",
    "            req_with_section = req.copy()\n",
    "            req_with_section['section_name'] = section_name\n",
    "            req_with_section['section_id'] = section['id']\n",
    "            \n",
    "            actor_groups[actor].append(req_with_section)\n",
    "    \n",
    "    # Only include actors that have requirements\n",
    "    result_groups = {}\n",
    "    for actor, reqs in actor_groups.items():\n",
    "        if reqs:  # Only include non-empty actor groups\n",
    "            if actor not in result_groups:\n",
    "                result_groups[actor] = {}\n",
    "            \n",
    "            # Group by original section\n",
    "            section_groups = defaultdict(list)\n",
    "            for req in reqs:\n",
    "                section_groups[req['section_name']].append(req)\n",
    "            \n",
    "            # Add the section groups to the result\n",
    "            result_groups[actor] = dict(section_groups)\n",
    "    \n",
    "    return result_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests_for_section(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    section: Dict[str, Any],\n",
    "    inferno_guidance: str,\n",
    "    module_name: str,\n",
    "    rate_limit_func,\n",
    "    max_token_limit: int = 16000\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate tests for an entire section or individual requirements based on token limits\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        section: Section dictionary containing requirements\n",
    "        inferno_guidance: Inferno test development guidance\n",
    "        module_name: Name of the module for the test\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        max_token_limit: Maximum tokens for the model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping requirement IDs to generated tests\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating tests for section: {section['name']}\")\n",
    "    \n",
    "    # First, try to generate tests for the entire section\n",
    "    if len(section['requirements']) > 1:\n",
    "        # Construct a prompt for the entire section\n",
    "        section_prompt = f\"\"\"\n",
    "        Generate Inferno tests for the following section of requirements from a FHIR implementation guide.\n",
    "        \n",
    "        Section: {section['name']}\n",
    "        \n",
    "        Requirements:\n",
    "        {section['content']}\n",
    "        \n",
    "        For each requirement, create a separate Inferno test class following the naming convention:\n",
    "        - Class name: {module_name}[Actor][Resource][REQ-ID]Test\n",
    "        - File name: req_[id]_test.rb\n",
    "        \n",
    "        Module Name: {module_name}\n",
    "        \n",
    "        Follow this Inferno development guidance:\n",
    "        {inferno_guidance}\n",
    "        \n",
    "        Return the Ruby code for each test implementation, separated by clear markers indicating the requirement ID.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate token count (rough approximation)\n",
    "        estimated_tokens = len(section_prompt) / 4  # Approximate 4 chars per token\n",
    "        \n",
    "        if estimated_tokens < max_token_limit:\n",
    "            try:\n",
    "                logger.info(f\"Attempting to generate tests for entire section: {section['name']}\")\n",
    "                response = llm_utils.make_llm_request(\n",
    "                    client, \n",
    "                    api_type, \n",
    "                    section_prompt, \n",
    "                    INFERNO_TEST_SYSTEM_PROMPT, \n",
    "                    rate_limit_func\n",
    "                )\n",
    "                \n",
    "                # Parse the response to extract individual tests\n",
    "                tests = {}\n",
    "                current_req = None\n",
    "                current_test = []\n",
    "                \n",
    "                for line in response.split('\\n'):\n",
    "                    # Look for markers indicating requirement boundaries\n",
    "                    req_marker = re.search(r'(REQ-\\d+)', line)\n",
    "                    if req_marker and (\"Test for\" in line or \"Implementation for\" in line or \"# Requirement\" in line):\n",
    "                        if current_req and current_test:\n",
    "                            tests[current_req] = '\\n'.join(current_test)\n",
    "                            current_test = []\n",
    "                        \n",
    "                        current_req = req_marker.group(1)\n",
    "                    \n",
    "                    if current_req:\n",
    "                        current_test.append(line)\n",
    "                \n",
    "                # Don't forget the last test\n",
    "                if current_req and current_test:\n",
    "                    tests[current_req] = '\\n'.join(current_test)\n",
    "                \n",
    "                # If we successfully parsed tests for all requirements, return them\n",
    "                if len(tests) == len(section['requirements']):\n",
    "                    logger.info(f\"Successfully generated tests for all requirements in section: {section['name']}\")\n",
    "                    return tests\n",
    "                \n",
    "                logger.warning(f\"Generated tests for only {len(tests)} of {len(section['requirements'])} requirements in section: {section['name']}\")\n",
    "                # Continue to individual requirement processing\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to generate tests for entire section: {str(e)}\")\n",
    "                # Continue to individual requirement processing\n",
    "    \n",
    "    # If section-level generation failed or wasn't attempted, generate tests for individual requirements\n",
    "    tests = {}\n",
    "    for requirement in section['requirements']:\n",
    "        try:\n",
    "            logger.info(f\"Generating test for requirement: {requirement['id']}\")\n",
    "            \n",
    "            # Prepare the prompt for this requirement with full context\n",
    "            req_prompt = get_inferno_test_generation_prompt(\n",
    "                test_specification=requirement['full_content'],\n",
    "                requirement_id=requirement['id'],\n",
    "                module_name=module_name,\n",
    "                inferno_guidance=inferno_guidance\n",
    "            )\n",
    "            \n",
    "            # Generate the test\n",
    "            test_code = llm_utils.make_llm_request(\n",
    "                client, \n",
    "                api_type, \n",
    "                req_prompt, \n",
    "                INFERNO_TEST_SYSTEM_PROMPT, \n",
    "                rate_limit_func\n",
    "            )\n",
    "            \n",
    "            # Clean up any markdown formatting\n",
    "            if test_code.startswith('```ruby'):\n",
    "                test_code = re.sub(r'^```ruby\\n', '', test_code)\n",
    "                test_code = re.sub(r'\\n```$', '', test_code)\n",
    "            elif test_code.startswith('```'):\n",
    "                test_code = re.sub(r'^```\\n', '', test_code)\n",
    "                test_code = re.sub(r'\\n```$', '', test_code)\n",
    "            \n",
    "            tests[requirement['id']] = test_code\n",
    "            logger.info(f\"Successfully generated test for requirement: {requirement['id']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating test for requirement {requirement['id']}: {str(e)}\")\n",
    "    \n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_structure(grouped_tests: Dict[str, Dict[str, List[Dict[str, Any]]]], module_name: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Create the file structure for the Inferno tests\n",
    "    \n",
    "    Args:\n",
    "        grouped_tests: Nested dictionary of grouped test specs\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping test specs to file paths\n",
    "    \"\"\"\n",
    "    # Create base directories\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    os.makedirs(module_dir, exist_ok=True)\n",
    "    \n",
    "    # Create test group directories\n",
    "    test_file_map = {}\n",
    "    \n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        actor_dir = os.path.join(module_dir, actor.lower())\n",
    "        os.makedirs(actor_dir, exist_ok=True)\n",
    "        \n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            resource_dir = os.path.join(actor_dir, resource.lower())\n",
    "            os.makedirs(resource_dir, exist_ok=True)\n",
    "            \n",
    "            for spec in test_specs:\n",
    "                # Create a file name based on the requirement ID\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                file_name = f\"{req_id}_test.rb\"\n",
    "                file_path = os.path.join(resource_dir, file_name)\n",
    "                \n",
    "                test_file_map[spec['id']] = file_path\n",
    "    \n",
    "    return test_file_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_module_file(module_name: str, output_dir: str, grouped_tests: Dict[str, Dict[str, List[Dict[str, Any]]]]):\n",
    "    \"\"\"\n",
    "    Generate the main module file that includes all test groups\n",
    "    \n",
    "    Args:\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        grouped_tests: Nested dictionary of grouped test specs\n",
    "        \n",
    "    Returns:\n",
    "        Path to the generated module file\n",
    "    \"\"\"\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    module_file_path = os.path.join(module_dir, f\"{module_name.lower()}.rb\")\n",
    "    \n",
    "    # Generate module file content\n",
    "    module_content = f\"# {module_name} Inferno Test Suite\\n\"\n",
    "    module_content += \"# Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\"\n",
    "    module_content += \"require 'inferno/dsl/test_suite'\\n\\n\"\n",
    "    \n",
    "    # Add requires for all test groups\n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        # Create safe versions for file paths\n",
    "        actor_safe = re.sub(r'[^a-zA-Z0-9_]', '_', actor.lower())\n",
    "        \n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            # Create safe versions for file paths\n",
    "            resource_safe = re.sub(r'[^a-zA-Z0-9_]', '_', resource.lower())\n",
    "            \n",
    "            module_content += f\"# {actor} {resource} tests\\n\"\n",
    "            for spec in test_specs:\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                module_content += f\"require_relative '{actor_safe}/{resource_safe}/{req_id}_test'\\n\"\n",
    "            module_content += \"\\n\"\n",
    "    \n",
    "    # Add module definition\n",
    "    module_content += f\"module {module_name}\\n\"\n",
    "    \n",
    "    # Add test suite class\n",
    "    class_name = f\"{module_name}TestSuite\"\n",
    "    module_content += f\"  class {class_name} < Inferno::TestSuite\\n\"\n",
    "    module_content += f\"    id :{module_name.lower()}_suite\\n\"\n",
    "    module_content += f\"    title '{module_name} Test Suite'\\n\"\n",
    "    module_content += f\"    description 'Test suite for validating {module_name} Implementation Guide conformance'\\n\\n\"\n",
    "    \n",
    "    # Add actor groups\n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        # Create safe version for Ruby identifiers\n",
    "        actor_safe = re.sub(r'[^a-zA-Z0-9_]', '_', actor.lower())\n",
    "        \n",
    "        module_content += f\"    # {actor} tests\\n\"\n",
    "        module_content += f\"    group do\\n\"\n",
    "        module_content += f\"      id :{actor_safe}_group\\n\"\n",
    "        module_content += f\"      title '{actor} Tests'\\n\\n\"\n",
    "        \n",
    "        # Add resource groups\n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            # Skip empty resource groups\n",
    "            if not test_specs:\n",
    "                continue\n",
    "                \n",
    "            # Create safe version for Ruby identifiers\n",
    "            resource_safe = re.sub(r'[^a-zA-Z0-9_]', '_', resource.lower())\n",
    "            \n",
    "            module_content += f\"      # {resource} tests\\n\"\n",
    "            module_content += f\"      group do\\n\"\n",
    "            module_content += f\"        id :{actor_safe}_{resource_safe}_group\\n\"\n",
    "            module_content += f\"        title '{resource} Tests'\\n\\n\"\n",
    "            \n",
    "            # Add references to individual tests\n",
    "            for spec in test_specs:\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                module_content += f\"        test from: :{req_id}_test\\n\"\n",
    "            \n",
    "            module_content += \"      end\\n\\n\"\n",
    "        \n",
    "        module_content += \"    end\\n\\n\"\n",
    "    \n",
    "    module_content += \"  end\\n\"\n",
    "    module_content += \"end\\n\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(module_file_path, 'w') as f:\n",
    "        f.write(module_content)\n",
    "    \n",
    "    return module_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inferno_test_kit(\n",
    "    api_type: str,\n",
    "    test_plan_file: str,\n",
    "    guidance_file: str = None,\n",
    "    module_name: str = \"PlanNet\",\n",
    "    output_dir: str = OUTPUT_DIR,\n",
    "    expected_actors: List[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a test plan and generate an Inferno test kit\n",
    "    \n",
    "    Args:\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        test_plan_file: Path to test plan markdown file\n",
    "        guidance_file: Path to Inferno guidance file (optional)\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        expected_actors: List of expected actors in the test plan\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing statistics and paths\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting Inferno test generation with {api_type} for {module_name}\")\n",
    "    \n",
    "    # Initialize API clients and rate limiters\n",
    "    clients = llm_utils.setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = llm_utils.API_CONFIGS[api_type]\n",
    "    rate_limiter = llm_utils.create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        llm_utils.check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Module name normalization for Ruby\n",
    "    module_name = ''.join(word.capitalize() for word in module_name.split())\n",
    "    \n",
    "    # Set default actors if none provided\n",
    "    if not expected_actors:\n",
    "        expected_actors = [\"Health Plan API Actor\", \"Application Actor\"]\n",
    "    \n",
    "    try:\n",
    "        # Parse test plan into sections\n",
    "        sections = parse_test_plan(test_plan_file)\n",
    "        logger.info(f\"Parsed test plan into {len(sections)} sections\")\n",
    "        \n",
    "        # Count total requirements\n",
    "        total_requirements = sum(len(section['requirements']) for section in sections.values())\n",
    "        logger.info(f\"Found {total_requirements} total requirements\")\n",
    "        \n",
    "        # Get Inferno guidance\n",
    "        if guidance_file and os.path.exists(guidance_file):\n",
    "            with open(guidance_file, 'r') as f:\n",
    "                inferno_guidance = f.read()\n",
    "            logger.info(f\"Loaded Inferno guidance from {guidance_file}\")\n",
    "        else:\n",
    "            inferno_guidance = get_inferno_guidance()\n",
    "            logger.info(\"Using default Inferno guidance\")\n",
    "        \n",
    "        # Generate tests by section\n",
    "        all_tests = {}\n",
    "        for section_name, section in sections.items():\n",
    "            logger.info(f\"Processing section: {section_name} with {len(section['requirements'])} requirements\")\n",
    "            \n",
    "            # Skip empty sections\n",
    "            if not section['requirements']:\n",
    "                continue\n",
    "                \n",
    "            # Generate tests for this section\n",
    "            section_tests = generate_tests_for_section(\n",
    "                client, \n",
    "                api_type, \n",
    "                section, \n",
    "                inferno_guidance, \n",
    "                module_name, \n",
    "                check_limits,\n",
    "                config['max_tokens']\n",
    "            )\n",
    "            \n",
    "            # Add to our collection\n",
    "            all_tests.update(section_tests)\n",
    "            \n",
    "            # Add delay between sections\n",
    "            time.sleep(config[\"delay_between_batches\"])\n",
    "            \n",
    "        logger.info(f\"Generated tests for {len(all_tests)} requirements\")\n",
    "        \n",
    "        # Group requirements by actor and section (original groups from test plan)\n",
    "        grouped_reqs = determine_test_groups(sections, expected_actors)\n",
    "        \n",
    "        # Ensure all expected actors are represented, even if empty\n",
    "        for actor in expected_actors:\n",
    "            if actor not in grouped_reqs:\n",
    "                grouped_reqs[actor] = {}\n",
    "        \n",
    "        # Create file structure and write tests\n",
    "        os.makedirs(os.path.join(output_dir, module_name.lower()), exist_ok=True)\n",
    "        \n",
    "        file_paths = {}\n",
    "        for actor, section_groups in grouped_reqs.items():\n",
    "            actor_safe = re.sub(r'[^a-zA-Z0-9_]', '_', actor.lower())\n",
    "            actor_dir = os.path.join(output_dir, module_name.lower(), actor_safe)\n",
    "            os.makedirs(actor_dir, exist_ok=True)\n",
    "            \n",
    "            for section_name, reqs in section_groups.items():\n",
    "                section_safe = re.sub(r'[^a-zA-Z0-9_]', '_', section_name.lower())\n",
    "                section_dir = os.path.join(actor_dir, section_safe)\n",
    "                os.makedirs(section_dir, exist_ok=True)\n",
    "                \n",
    "                for req in reqs:\n",
    "                    # Skip requirements we couldn't generate tests for\n",
    "                    if req['id'] not in all_tests:\n",
    "                        continue\n",
    "                        \n",
    "                    file_name = f\"{req['id'].lower().replace('-', '_')}_test.rb\"\n",
    "                    file_path = os.path.join(section_dir, file_name)\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(all_tests[req['id']])\n",
    "                    \n",
    "                    file_paths[req['id']] = file_path\n",
    "                    logger.info(f\"Wrote test for {req['id']} to {file_path}\")\n",
    "        \n",
    "        # Generate module file\n",
    "        module_file = generate_module_file(module_name, output_dir, grouped_reqs)\n",
    "        \n",
    "        return {\n",
    "            \"total_sections\": len(sections),\n",
    "            \"total_requirements\": total_requirements,\n",
    "            \"generated_tests\": len(all_tests),\n",
    "            \"module_dir\": os.path.join(output_dir, module_name.lower()),\n",
    "            \"module_file\": module_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Inferno tests: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_module_file(module_name: str, output_dir: str, grouped_reqs) -> str:\n",
    "    \"\"\"\n",
    "    Generate the main module file that includes all test groups\n",
    "    \n",
    "    Args:\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        grouped_reqs: Dictionary of requirements grouped by actor and section\n",
    "        \n",
    "    Returns:\n",
    "        Path to the generated module file\n",
    "    \"\"\"\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    module_file_path = os.path.join(module_dir, f\"{module_name.lower()}.rb\")\n",
    "    \n",
    "    # Generate module file content\n",
    "    module_content = f\"# {module_name} Inferno Test Suite\\n\"\n",
    "    module_content += \"# Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\"\n",
    "    module_content += \"require 'inferno/dsl/test_suite'\\n\\n\"\n",
    "    \n",
    "    # Add requires for all test groups\n",
    "    for actor, section_groups in grouped_reqs.items():\n",
    "        actor_safe = actor.replace(\" \", \"_\").lower()\n",
    "        for section_name, reqs in section_groups.items():\n",
    "            section_safe = section_name.replace(\" \", \"_\").lower()\n",
    "            module_content += f\"# {actor} - {section_name} tests\\n\"\n",
    "            for req in reqs:\n",
    "                req_id_safe = req['id'].lower().replace('-', '_')\n",
    "                module_content += f\"require_relative '{actor_safe}/{section_safe}/req_{req_id_safe}_test'\\n\"\n",
    "            module_content += \"\\n\"\n",
    "    \n",
    "    # Add module definition\n",
    "    module_content += f\"module {module_name}\\n\"\n",
    "    \n",
    "    # Add test suite class\n",
    "    class_name = f\"{module_name}TestSuite\"\n",
    "    module_content += f\"  class {class_name} < Inferno::TestSuite\\n\"\n",
    "    module_content += f\"    id :{module_name.lower()}_suite\\n\"\n",
    "    module_content += f\"    title '{module_name} Test Suite'\\n\"\n",
    "    module_content += f\"    description 'Test suite for validating {module_name} Implementation Guide conformance'\\n\\n\"\n",
    "    \n",
    "    # Add actor groups\n",
    "    for actor, section_groups in grouped_reqs.items():\n",
    "        actor_safe = actor.replace(\" \", \"_\").lower()\n",
    "        module_content += f\"    # {actor} tests\\n\"\n",
    "        module_content += f\"    group do\\n\"\n",
    "        module_content += f\"      id :{actor_safe}_group\\n\"\n",
    "        module_content += f\"      title '{actor} Tests'\\n\\n\"\n",
    "        \n",
    "        # Add section groups\n",
    "        for section_name, reqs in section_groups.items():\n",
    "            section_safe = section_name.replace(\" \", \"_\").lower()\n",
    "            module_content += f\"      # {section_name} tests\\n\"\n",
    "            module_content += f\"      group do\\n\"\n",
    "            module_content += f\"        id :{actor_safe}_{section_safe}_group\\n\"\n",
    "            module_content += f\"        title '{section_name} Tests'\\n\\n\"\n",
    "            \n",
    "            # Add references to individual tests\n",
    "            for req in reqs:\n",
    "                req_id_safe = req['id'].lower().replace('-', '_')\n",
    "                module_content += f\"        test from: :req_{req_id_safe}_test\\n\"\n",
    "            \n",
    "            module_content += \"      end\\n\\n\"\n",
    "        \n",
    "        module_content += \"    end\\n\\n\"\n",
    "    \n",
    "    module_content += \"  end\\n\"\n",
    "    module_content += \"end\\n\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(module_file_path, 'w') as f:\n",
    "        f.write(module_content)\n",
    "    \n",
    "    return module_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inferno_test_generator():\n",
    "    \"\"\"\n",
    "    Run the Inferno test generator with user input\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with generation results, or None if an error occurred\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get input from user or set default values\n",
    "    print(\"\\nFHIR Inferno Test Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Let user select the API\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    # Get test plan file path\n",
    "    test_plan_file = input(\"\\nEnter path to test plan markdown file: \")\n",
    "    \n",
    "    # Check if test plan file exists\n",
    "    if not os.path.exists(test_plan_file):\n",
    "        logger.error(f\"Test plan file not found: {test_plan_file}\")\n",
    "        print(f\"Error: Test plan file not found at {test_plan_file}\")\n",
    "        return None\n",
    "    \n",
    "    # the path to the guidance file\n",
    "    guidance_file = os.path.join(CURRENT_DIR, \"inferno-guidance.md\")\n",
    "    \n",
    "    # Check if guidance file exists\n",
    "    if not os.path.exists(guidance_file):\n",
    "        logger.warning(f\"Inferno guidance file not found: {guidance_file}\")\n",
    "        print(f\"Warning: Inferno guidance file not found at {guidance_file}. Using built-in guidance.\")\n",
    "        guidance_file = None\n",
    "    else:\n",
    "        print(f\"Using guidance file: {guidance_file}\")\n",
    "    \n",
    "    # Get module name\n",
    "    module_name = input(\"\\nEnter module name (default 'PlanNet'): \") or \"PlanNet\"\n",
    "    \n",
    "    # Get actor information\n",
    "    print(\"\\nEnter the actors from the test plan (comma-separated, e.g., 'Health Plan API Actor, Application Actor'):\")\n",
    "    actors_input = input(\"Actors: \")\n",
    "    \n",
    "    if actors_input:\n",
    "        expected_actors = [actor.strip() for actor in actors_input.split(',')]\n",
    "    else:\n",
    "        # Default actors if none provided\n",
    "        expected_actors = [\"Health Plan API Actor\", \"Application Actor\"]\n",
    "    \n",
    "    # Get output directory\n",
    "    output_dir = input(f\"\\nEnter output directory (default '{OUTPUT_DIR}'): \") or OUTPUT_DIR\n",
    "    \n",
    "    print(f\"\\nGenerating Inferno tests with {api_type.capitalize()}...\")\n",
    "    print(f\"Using actors: {', '.join(expected_actors)}\")\n",
    "    if guidance_file:\n",
    "        print(f\"Using Inferno guidance from {guidance_file}\")\n",
    "    print(f\"This may take several minutes depending on the number of requirements.\")\n",
    "    \n",
    "    try:\n",
    "        # Process test plan and generate Inferno tests\n",
    "        result = generate_inferno_test_kit(\n",
    "            api_type=api_type,\n",
    "            test_plan_file=test_plan_file,\n",
    "            guidance_file=guidance_file,\n",
    "            module_name=module_name,\n",
    "            output_dir=output_dir,\n",
    "            expected_actors=expected_actors\n",
    "        )\n",
    "        \n",
    "        # Output results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Inferno test generation complete!\")\n",
    "        print(f\"Total sections: {result['total_sections']}\")\n",
    "        print(f\"Total requirements: {result['total_requirements']}\")\n",
    "        print(f\"Successfully generated tests: {result['generated_tests']}\")\n",
    "        print(f\"Module directory: {result['module_dir']}\")\n",
    "        print(f\"Main module file: {result['module_file']}\")\n",
    "        #print(f\"Generation report: {result['report_file']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FHIR Inferno Test Generator\n",
      "==================================================\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n",
      "Using guidance file: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/inferno-guidance.md\n",
      "\n",
      "Enter the actors from the test plan (comma-separated, e.g., 'Health Plan API Actor, Application Actor'):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:34:41,605 - __main__ - INFO - Starting Inferno test generation with gemini for PlanNet\n",
      "2025-04-30 12:34:41,640 - __main__ - INFO - Parsed test plan into 11 sections\n",
      "2025-04-30 12:34:41,641 - __main__ - INFO - Found 11 total requirements\n",
      "2025-04-30 12:34:41,642 - __main__ - INFO - Loaded Inferno guidance from /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/inferno-guidance.md\n",
      "2025-04-30 12:34:41,642 - __main__ - INFO - Processing section: Application-Level Requirements with 1 requirements\n",
      "2025-04-30 12:34:41,642 - __main__ - INFO - Generating tests for section: Application-Level Requirements\n",
      "2025-04-30 12:34:41,643 - __main__ - INFO - Generating test for requirement: REQ-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Inferno tests with Gemini...\n",
      "Using actors: Health Plan API, Application Actor\n",
      "Using Inferno guidance from /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/inferno-guidance.md\n",
      "This may take several minutes depending on the number of requirements.\n",
      "Found 11 potential requirements\n",
      "Processing requirement: REQ-08\n",
      "Added requirement REQ-08 to section Application-Level Requirements\n",
      "Processing requirement: REQ-01\n",
      "Added requirement REQ-01 to section Authentication\n",
      "Processing requirement: REQ-09\n",
      "Added requirement REQ-09 to section Base Requirements\n",
      "Processing requirement: REQ-07\n",
      "Added requirement REQ-07 to section CORE Conformance\n",
      "Processing requirement: REQ-06\n",
      "Added requirement REQ-06 to section Cross-Resource\n",
      "Processing requirement: REQ-04\n",
      "Added requirement REQ-04 to section General Requirements\n",
      "Processing requirement: REQ-05\n",
      "Added requirement REQ-05 to section Global\n",
      "Processing requirement: REQ-11\n",
      "Added requirement REQ-11 to section OrganizationAffiliation\n",
      "Processing requirement: REQ-02\n",
      "Added requirement REQ-02 to section Plan-Net API Security\n",
      "Processing requirement: REQ-10\n",
      "Added requirement REQ-10 to section PractitionerRole\n",
      "Processing requirement: REQ-03\n",
      "Added requirement REQ-03 to section Security\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:35:38,419 - __main__ - INFO - Successfully generated test for requirement: REQ-08\n",
      "2025-04-30 12:35:43,429 - __main__ - INFO - Processing section: Authentication with 1 requirements\n",
      "2025-04-30 12:35:43,430 - __main__ - INFO - Generating tests for section: Authentication\n",
      "2025-04-30 12:35:43,432 - __main__ - INFO - Generating test for requirement: REQ-01\n",
      "2025-04-30 12:36:06,273 - __main__ - INFO - Successfully generated test for requirement: REQ-01\n",
      "2025-04-30 12:36:11,278 - __main__ - INFO - Processing section: Base Requirements with 1 requirements\n",
      "2025-04-30 12:36:11,278 - __main__ - INFO - Generating tests for section: Base Requirements\n",
      "2025-04-30 12:36:11,279 - __main__ - INFO - Generating test for requirement: REQ-09\n",
      "2025-04-30 12:37:18,694 - __main__ - INFO - Successfully generated test for requirement: REQ-09\n",
      "2025-04-30 12:37:23,694 - __main__ - INFO - Processing section: CORE Conformance with 1 requirements\n",
      "2025-04-30 12:37:23,695 - __main__ - INFO - Generating tests for section: CORE Conformance\n",
      "2025-04-30 12:37:23,695 - __main__ - INFO - Generating test for requirement: REQ-07\n",
      "2025-04-30 12:38:13,416 - __main__ - INFO - Successfully generated test for requirement: REQ-07\n",
      "2025-04-30 12:38:18,418 - __main__ - INFO - Processing section: Cross-Resource with 1 requirements\n",
      "2025-04-30 12:38:18,419 - __main__ - INFO - Generating tests for section: Cross-Resource\n",
      "2025-04-30 12:38:18,419 - __main__ - INFO - Generating test for requirement: REQ-06\n",
      "2025-04-30 12:39:24,732 - __main__ - INFO - Successfully generated test for requirement: REQ-06\n",
      "2025-04-30 12:39:29,736 - __main__ - INFO - Processing section: General Requirements with 1 requirements\n",
      "2025-04-30 12:39:29,738 - __main__ - INFO - Generating tests for section: General Requirements\n",
      "2025-04-30 12:39:29,741 - __main__ - INFO - Generating test for requirement: REQ-04\n",
      "2025-04-30 12:40:47,208 - __main__ - INFO - Successfully generated test for requirement: REQ-04\n",
      "2025-04-30 12:40:52,212 - __main__ - INFO - Processing section: Global with 1 requirements\n",
      "2025-04-30 12:40:52,214 - __main__ - INFO - Generating tests for section: Global\n",
      "2025-04-30 12:40:52,215 - __main__ - INFO - Generating test for requirement: REQ-05\n",
      "2025-04-30 12:42:18,073 - __main__ - INFO - Successfully generated test for requirement: REQ-05\n",
      "2025-04-30 12:42:23,078 - __main__ - INFO - Processing section: OrganizationAffiliation with 1 requirements\n",
      "2025-04-30 12:42:23,080 - __main__ - INFO - Generating tests for section: OrganizationAffiliation\n",
      "2025-04-30 12:42:23,081 - __main__ - INFO - Generating test for requirement: REQ-11\n",
      "2025-04-30 12:43:00,932 - __main__ - INFO - Successfully generated test for requirement: REQ-11\n",
      "2025-04-30 12:43:05,940 - __main__ - INFO - Processing section: Plan-Net API Security with 1 requirements\n",
      "2025-04-30 12:43:05,940 - __main__ - INFO - Generating tests for section: Plan-Net API Security\n",
      "2025-04-30 12:43:05,941 - __main__ - INFO - Generating test for requirement: REQ-02\n",
      "2025-04-30 12:43:22,094 - __main__ - INFO - Successfully generated test for requirement: REQ-02\n",
      "2025-04-30 12:43:27,096 - __main__ - INFO - Processing section: PractitionerRole with 1 requirements\n",
      "2025-04-30 12:43:27,096 - __main__ - INFO - Generating tests for section: PractitionerRole\n",
      "2025-04-30 12:43:27,097 - __main__ - INFO - Generating test for requirement: REQ-10\n",
      "2025-04-30 12:43:58,070 - __main__ - INFO - Successfully generated test for requirement: REQ-10\n",
      "2025-04-30 12:44:03,075 - __main__ - INFO - Processing section: Security with 1 requirements\n",
      "2025-04-30 12:44:03,078 - __main__ - INFO - Generating tests for section: Security\n",
      "2025-04-30 12:44:03,079 - __main__ - INFO - Generating test for requirement: REQ-03\n",
      "2025-04-30 12:44:28,412 - __main__ - INFO - Successfully generated test for requirement: REQ-03\n",
      "2025-04-30 12:44:33,417 - __main__ - INFO - Generated tests for 11 requirements\n",
      "2025-04-30 12:44:33,423 - __main__ - INFO - Wrote test for REQ-08 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/application_actor/application_level_requirements/req_08_test.rb\n",
      "2025-04-30 12:44:33,425 - __main__ - INFO - Wrote test for REQ-09 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/application_actor/base_requirements/req_09_test.rb\n",
      "2025-04-30 12:44:33,426 - __main__ - INFO - Wrote test for REQ-07 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/application_actor/core_conformance/req_07_test.rb\n",
      "2025-04-30 12:44:33,427 - __main__ - INFO - Wrote test for REQ-03 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/application_actor/security/req_03_test.rb\n",
      "2025-04-30 12:44:33,428 - __main__ - INFO - Wrote test for REQ-01 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/health_plan_api/authentication/req_01_test.rb\n",
      "2025-04-30 12:44:33,430 - __main__ - INFO - Wrote test for REQ-06 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/health_plan_api/cross_resource/req_06_test.rb\n",
      "2025-04-30 12:44:33,431 - __main__ - INFO - Wrote test for REQ-04 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/health_plan_api/general_requirements/req_04_test.rb\n",
      "2025-04-30 12:44:33,432 - __main__ - INFO - Wrote test for REQ-05 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/health_plan_api/global/req_05_test.rb\n",
      "2025-04-30 12:44:33,433 - __main__ - INFO - Wrote test for REQ-11 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/health_plan_api/organizationaffiliation/req_11_test.rb\n",
      "2025-04-30 12:44:33,434 - __main__ - INFO - Wrote test for REQ-02 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/health_plan_api/plan_net_api_security/req_02_test.rb\n",
      "2025-04-30 12:44:33,435 - __main__ - INFO - Wrote test for REQ-10 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/health_plan_api/practitionerrole/req_10_test.rb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Inferno test generation complete!\n",
      "Total sections: 11\n",
      "Total requirements: 11\n",
      "Successfully generated tests: 11\n",
      "Module directory: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet\n",
      "Main module file: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/plannet.rb\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_sections': 11,\n",
       " 'total_requirements': 11,\n",
       " 'generated_tests': 11,\n",
       " 'module_dir': '/Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet',\n",
       " 'module_file': '/Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/plannet/plannet.rb'}"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the test generator\n",
    "run_inferno_test_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
