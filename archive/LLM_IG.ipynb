{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring FHIR Implementation Guides (IGs) + LLMs\n",
    "\n",
    "In this notebook, we aim to explore how much LLMs understand about FHIR Implementation Guides (IGs) and investigate ways to upload IG content for deeper analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have langchain-community and beautifulsoup4 installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U json_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as gemini\n",
    "from anthropic import Anthropic\n",
    "from openai import OpenAI\n",
    "import io, threading, time, re, json\n",
    "import pandas as pd\n",
    "from json_repair import repair_json\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in US Core IG JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Be sure that you have downloaded the US Core IG files from https://www.hl7.org/fhir/us/core/package.tgz and placed them in your current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'full-ig/site'\n",
    "destination_folder = 'full-ig/html_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store only .json files\n",
    "# html_files = []\n",
    "json_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(source_folder):\n",
    "    # Check if the file ends with .html but not with compound extensions\n",
    "    if file_name.endswith('.json'):\n",
    "                                    # and not (file_name.endswith('.ttl.html') or \n",
    "                                            #  file_name.endswith('.json.html') or \n",
    "                                            #  file_name.endswith('.xml.html') or \n",
    "                                            #  file_name.endswith('.change.history.html')):\n",
    "        json_files.append(file_name)\n",
    "        # Move the file to the destination folder\n",
    "        shutil.copy(os.path.join(source_folder, file_name), destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading HTML with BeautifulSoup4\n",
    "\n",
    "This is archived code to load HTML files with BeautifulSoup4. We are now using JSON files so this step is skipped. \n",
    "\n",
    "\n",
    "TODO: Rewrite HTML loader as a function for potential future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_only_folder = 'full-ig/html_only'\n",
    "json_only_folder = 'package/json_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new folder named \"plain_text\" inside the current directory\n",
    "# processed_files_path = os.path.join(html_only_folder, 'plain_txt')\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "# if not os.path.exists(processed_files_path):\n",
    "#     os.makedirs(processed_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the files processed\n",
    "# processed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through the files in the HTML folder\n",
    "# for file_name in os.listdir(html_only_folder):\n",
    "#     # Full path to the .html file\n",
    "#     html_file_path = os.path.join(html_only_folder, file_name)\n",
    "    \n",
    "#     # Check if it's a file (not a directory)\n",
    "#     if os.path.isfile(html_file_path):\n",
    "#         # Use BSHTMLLoader to load the HTML content\n",
    "#         loader = BSHTMLLoader(html_file_path, bs_kwargs={'features': 'html.parser'})\n",
    "#         data = loader.load()\n",
    "#         # Extract the plain text from the loaded data\n",
    "#         plain_text = '\\n'.join([doc.page_content for doc in data])\n",
    "        \n",
    "#         # Create the output file path with .txt extension\n",
    "#         txt_file_name = file_name.replace('.html', '.txt')\n",
    "#         txt_file_path = os.path.join(processed_files_path, txt_file_name)\n",
    "        \n",
    "#         # Write the extracted plain text to the new .txt file\n",
    "#         with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "#             txt_file.write(plain_text)\n",
    "        \n",
    "#         # Append to processed files list\n",
    "#         processed_files.append(txt_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_files_by_base_name(directory_path, delimiter='-'):\n",
    "    \"\"\"\n",
    "    Group files in the directory by their base name (portion before a delimiter).\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    delimiter (str): The delimiter to split the file name on (default is '-').\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are base names and values are lists of files that share the same base name.\n",
    "    \"\"\"\n",
    "    grouped_files = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):  # Only process .txt files\n",
    "            if delimiter in filename:  # Only consider files with the delimiter\n",
    "                # Get the base name (before the first delimiter)\n",
    "                base_name = filename.split(delimiter)[0]\n",
    "                \n",
    "                # Append the file to the group corresponding to its base name\n",
    "                grouped_files[base_name].append(filename)\n",
    "    \n",
    "    return grouped_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'package/json_only'\n",
    "# 'full-ig/html_only/plain_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_files = group_files_by_base_name(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base name: StructureDefinition (Total files: 63)\n",
      "Base name: ValueSet (Total files: 29)\n",
      "Base name: SearchParameter (Total files: 110)\n",
      "Base name: CapabilityStatement (Total files: 2)\n",
      "Base name: CodeSystem (Total files: 5)\n",
      "Base name: OperationDefinition (Total files: 1)\n",
      "Base name: ImplementationGuide (Total files: 1)\n"
     ]
    }
   ],
   "source": [
    "for base_name, files in grouped_files.items():\n",
    "    print(f\"Base name: {base_name} (Total files: {len(files)})\")\n",
    "    # for file in files:\n",
    "    #     print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_to_folders(directory_path, grouped_files):\n",
    "    \"\"\"\n",
    "    Copy files to folders if the base name group has more than 1 file, and remove them from the original directory.\n",
    "    \n",
    "    Args:\n",
    "    directory_path (str): Path to the directory containing files.\n",
    "    grouped_files (dict): Dictionary of grouped files by base name.\n",
    "    \"\"\"\n",
    "    for base_name, files in grouped_files.items():\n",
    "        if len(files) >= 1:  # Only process groups with more than 1 file\n",
    "            # Create a folder for the base name in the same directory\n",
    "            base_folder = os.path.join(directory_path, base_name)\n",
    "            if not os.path.exists(base_folder):\n",
    "                os.makedirs(base_folder)  # Create the folder if it doesn't exist\n",
    "            print(f\"Created folder: {base_folder}\")\n",
    "            \n",
    "            # Copy each file in the group to the new folder\n",
    "            for file in files:\n",
    "                source_file = os.path.join(directory_path, file)\n",
    "                destination_file = os.path.join(base_folder, file)\n",
    "                shutil.copy(source_file, destination_file)  # Copy the file\n",
    "                # print(f\"Copied {file} to {base_folder}\")\n",
    "                \n",
    "                # Remove the file from the original directory\n",
    "                # os.remove(source_file)\n",
    "                # print(f\"Removed {file} from original directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder: package/json_only/StructureDefinition\n",
      "Created folder: package/json_only/ValueSet\n",
      "Created folder: package/json_only/SearchParameter\n",
      "Created folder: package/json_only/CapabilityStatement\n",
      "Created folder: package/json_only/CodeSystem\n",
      "Created folder: package/json_only/OperationDefinition\n",
      "Created folder: package/json_only/ImplementationGuide\n"
     ]
    }
   ],
   "source": [
    "copy_files_to_folders(directory_path, grouped_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files categories match the files that we had identified to keep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing files\n",
    "\n",
    "In this step, we're preparing our loaded files by combining JSONs in each directory into a singular JSON, structured with top level items: \"resource_Type\", \"total\", and \"entry\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_json_files(folder_path):\n",
    "    \"\"\"\n",
    "    Combines all JSON files in a folder into a single array of JSON objects.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of JSON objects from all files\n",
    "    \"\"\"\n",
    "    combined_json = []\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    json_content = json.load(file)\n",
    "                    combined_json.append(json_content)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                \n",
    "    return combined_json\n",
    "\n",
    "def create_consolidated_jsons(base_directory='package/json_only'):\n",
    "    \"\"\"\n",
    "    Creates consolidated JSON files for each subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        base_directory (str): Base directory containing the categorized folders\n",
    "    \"\"\"\n",
    "    # Get all subdirectories\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "              if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    # Process each subdirectory\n",
    "    for subdir in subdirs:\n",
    "        folder_path = os.path.join(base_directory, subdir)\n",
    "        print(f\"Processing {subdir}...\")\n",
    "        \n",
    "        # Combine all JSON files in this folder\n",
    "        combined_data = combine_json_files(folder_path)\n",
    "        \n",
    "        if combined_data:\n",
    "            # Create output filename\n",
    "            output_filename = f\"{subdir}_combined.json\"\n",
    "            output_path = os.path.join(base_directory, output_filename)\n",
    "            \n",
    "            # Write the combined JSON to a file\n",
    "            try:\n",
    "                with open(output_path, 'w') as outfile:\n",
    "                    json.dump({\n",
    "                        \"resourceType\": subdir,\n",
    "                        \"total\": len(combined_data),\n",
    "                        \"entry\": combined_data\n",
    "                    }, outfile, indent=2)\n",
    "                print(f\"Created {output_filename} with {len(combined_data)} entries\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing {output_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing StructureDefinition...\n",
      "Created StructureDefinition_combined.json with 63 entries\n",
      "Processing CapabilityStatement...\n",
      "Created CapabilityStatement_combined.json with 2 entries\n",
      "Processing CodeSystem...\n",
      "Created CodeSystem_combined.json with 5 entries\n",
      "Processing ValueSet...\n",
      "Created ValueSet_combined.json with 29 entries\n",
      "Processing SearchParameter...\n",
      "Created SearchParameter_combined.json with 110 entries\n",
      "Processing ImplementationGuide...\n",
      "Created ImplementationGuide_combined.json with 1 entries\n",
      "Processing OperationDefinition...\n",
      "Created OperationDefinition_combined.json with 1 entries\n"
     ]
    }
   ],
   "source": [
    "# Create the consolidated JSON files\n",
    "create_consolidated_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to identify narrative files and use that content to provide context to the LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archived functions/code to combine txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file\n",
    "with open('/Users/amathur/Documents/ONCLAIVE/onclaive-aanchalwip/package/json_only/ImplementationGuide/ImplementationGuide-hl7.fhir.us.core.json', 'r') as file:\n",
    "    implementation_guide = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending IG through LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in API keys for Claude, Gemini, and GPT from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "claude_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "OpenAI.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = Anthropic(api_key = claude_api_key)\n",
    "claude_version = \"claude-3-5-sonnet-20240620\"  # \"claude-3-opus-20240229\"   \"claude-3-5-sonnet-20240620\" \"claude-3-sonnet-20240229\" \"claude-3-haiku-20240307\"\n",
    "claude_max_output_tokens = 8192  # claude 3 opus is only 4096 tokens, sonnet is 8192\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to send IG content Claude and request analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CERT_PATH = '/Users/amathur/ca-certificates.crt'\n",
    "CERT_PATH = '/opt/homebrew/etc/openssl@3/cert.pem'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anthropic_client():\n",
    "    \"\"\"Create Anthropic client with proper certificate verification\"\"\"\n",
    "    verify_path = CERT_PATH if os.path.exists(CERT_PATH) else True\n",
    "    http_client = httpx.Client(\n",
    "        verify=verify_path,\n",
    "        timeout=30.0\n",
    "    )\n",
    "    return Anthropic(\n",
    "        api_key=claude_api_key,\n",
    "        http_client=http_client\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add sleep pauses\n",
    "def heartbeat(stop_event, start_time):\n",
    "    \"\"\"Prints elapsed time periodically until stopped.\"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"... still processing ({elapsed:.1f}s elapsed)\")\n",
    "        time.sleep(5)\n",
    "\n",
    "#send message request to claude letting it know an IG is being shared and providing it the action prompt\n",
    "def message_claude(claude_client, user_prompt, content_text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Sends a message to Claude API with the provided prompt and content.\n",
    "    \"\"\"\n",
    "    # Construct the full prompt\n",
    "    full_prompt = f\"\"\"Here is the content of an HL7 FHIR Implementation Guide:\n",
    "\n",
    "{content_text}\n",
    "\n",
    "{user_prompt}\"\"\"\n",
    "    \n",
    "    # Set up heartbeat\n",
    "    start_time = time.time()\n",
    "    stop_event = threading.Event()\n",
    "    heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "    heartbeat_thread.start()\n",
    "    \n",
    "    retry_count = 0\n",
    "    last_error = None\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            print(f\"Sending request to Claude API (attempt {retry_count + 1}/{max_retries})...\")\n",
    "            \n",
    "            response = claude_client.messages.create(\n",
    "                model=claude_version,\n",
    "                max_tokens=claude_max_output_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            print(\"Successfully received response from Claude API\")\n",
    "            response_text = response.content[0].text\n",
    "            return response, response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            retry_count += 1\n",
    "            if retry_count < max_retries:\n",
    "                wait_time = 2 ** retry_count\n",
    "                print(f\"Error occurred: {str(e)}\")\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {max_retries} attempts. Last error: {str(e)}\")\n",
    "                raise\n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            heartbeat_thread.join()\n",
    "\n",
    "#analyze content of IG\n",
    "def analyze_ig(content, prompt):\n",
    "    \"\"\"\n",
    "    Main function to process IG files and get Claude's analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Claude client\n",
    "        claude_client = create_anthropic_client()\n",
    "        \n",
    "        #confirm combined text object has been created\n",
    "        if not content:\n",
    "            raise ValueError(\"No content found in text files\")\n",
    "        \n",
    "        #print characters of combined text object\n",
    "        print(f\"Combined content length: {len(content)} characters\")\n",
    "        \n",
    "        print(\"Sending to Claude API...\")\n",
    "        response, response_text = message_claude(claude_client, prompt, content)\n",
    "        \n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_ig: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the implementation guide\n",
    "with open(\"full-ig/site/ImplementationGuide-hl7.fhir.us.davinci-pdex-plan-net.json\", \"r\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: sending IG JSON from PDEX Plan Net IG to Claude to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined content length: 93955 characters\n",
      "Sending to Claude API...\n",
      "... still processing (0.0s elapsed)\n",
      "Sending request to Claude API (attempt 1/3)...\n",
      "... still processing (5.0s elapsed)\n",
      "... still processing (10.0s elapsed)\n",
      "Successfully received response from Claude API\n",
      "This JSON content represents an HL7 FHIR Implementation Guide (IG) for the DaVinci PDEX Plan Net project. Here's a summary of the key information and purpose:\n",
      "\n",
      "1. Resource Type: ImplementationGuide\n",
      "\n",
      "2. ID: hl7.fhir.us.davinci-pdex-plan-net\n",
      "\n",
      "3. Name and Title: DaVinciPDEXPlanNet (DaVinci PDEX Plan Net)\n",
      "\n",
      "4. Version: 1.1.0\n",
      "\n",
      "5. Status: Active\n",
      "\n",
      "6. Date: April 4, 2022\n",
      "\n",
      "7. Publisher: HL7 Financial Management Working Group\n",
      "\n",
      "8. Description: Davinci PDEX Plan Net\n",
      "\n",
      "9. Jurisdiction: United States (US)\n",
      "\n",
      "10. FHIR Version: 4.0.1\n",
      "\n",
      "11. Purpose: This implementation guide defines profiles, extensions, and search parameters for various FHIR resources related to healthcare provider directories, including:\n",
      "    - Endpoints\n",
      "    - HealthcareServices\n",
      "    - InsurancePlans\n",
      "    - Locations\n",
      "    - Networks\n",
      "    - Organizations\n",
      "    - OrganizationAffiliations\n",
      "    - Practitioners\n",
      "    - PractitionerRoles\n",
      "\n",
      "12. The IG also includes value sets, code systems, and example resources for these profiles.\n",
      "\n",
      "13. It defines numerous search parameters to enable querying of the defined resources.\n",
      "\n",
      "14. The IG is designed to meet the CMS Final Rule requirement for provider directory access.\n",
      "\n",
      "15. It includes a CapabilityStatement describing the expected capabilities of Plan-Net Servers and Clients.\n",
      "\n",
      "The overall purpose of this implementation guide is to standardize the representation and exchange of healthcare provider directory information, including insurance plans, organizations, practitioners, and services, using FHIR resources. This standardization aims to improve interoperability and data sharing in the healthcare domain, particularly for provider directories and insurance networks.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "The the content below is a JSON file in <JSON_CONTENT></JSON_CONTENT> tags. \n",
    "Can you summarize the key information and purpose of this data? \n",
    "\n",
    "<JSON_CONTENT>\n",
    "{content}\n",
    "</JSON_CONTENT>\n",
    "\"\"\"\n",
    "result = analyze_ig(content, prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_version = \"models/gemini-1.5-pro-001\" \n",
    "gemini_max_output_tokens = 8192\n",
    "temp = 0.75  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini maintains history in a chat session object. Messages are sent to a specific chat session.\n",
    "def message_gemini(prompt, chat_session=None):\n",
    "    global gemini_version, temp, gemini_max_output_tokens\n",
    "    if chat_session is None:\n",
    "        model = gemini.GenerativeModel(model_name=gemini_version, generation_config={\"max_output_tokens\": gemini_max_output_tokens, \"response_mime_type\": \"application/json\"})\n",
    "        chat_session = model.start_chat()\n",
    "    start_time = time.time()\n",
    "    stop_event = threading.Event()\n",
    "    heartbeat_thread = threading.Thread(target=heartbeat, args=(stop_event, start_time))\n",
    "    heartbeat_thread.start()\n",
    "    try:\n",
    "        resoponse = chat_session.send_message(prompt, stream=False)\n",
    "    finally:\n",
    "        stop_event.set()  # Signal the heartbeat to stop\n",
    "        heartbeat_thread.join()  # Wait for the heartbeat thread to finish\n",
    "    return resoponse.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "The the content below is a JSON file in <JSON_CONTENT></JSON_CONTENT> tags. \n",
    "Can you summarize the key information and purpose of this data? \n",
    "\n",
    "<JSON_CONTENT>\n",
    "{content}\n",
    "</JSON_CONTENT>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gemini.GenerativeModel(model_name='gemini-1.5-flash-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 8192)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = gemini.get_model('models/gemini-1.5-flash-latest')\n",
    "(model_info.input_token_limit, model_info.output_token_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_tokens: 72174"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_tokens(prompt[0:model_info.input_token_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This JSON file represents an FHIR Implementation Guide (IG) named \"US Core Implementation Guide\". It defines the minimum conformance requirements for accessing patient data in the US healthcare system. \n",
      "\n",
      "Here's a breakdown of the key information:\n",
      "\n",
      "* **Purpose:** To establish a standard for interoperability between healthcare systems in the US by defining a set of FHIR profiles, extensions, search parameters, and operations that all systems must adhere to.\n",
      "* **Version:**  7.0.0\n",
      "* **FHIR Version:** 4.0.1\n",
      "* **Based On:** The Argonaut pilot implementations, ONC 2015 Edition Common Clinical Data Set (CCDS), and ONC U.S. Core Data for Interoperability (USCDI) v1.\n",
      "* **Scope:**  Defines minimum expectations for accessing patient data for various resources including Patient, Encounter, Condition, Observation, Medication, Immunization, and many more.\n",
      "* **Content:**\n",
      "    * **Profiles:** Specifies the structure and content of different FHIR resources. \n",
      "    * **Extensions:** Defines custom elements to extend the standard FHIR resources.\n",
      "    * **Search Parameters:** Defines standardized search criteria for retrieving specific data.\n",
      "    * **Operations:** Defines additional operations beyond the standard RESTful interactions.\n",
      "    * **Examples:** Provides sample data to illustrate the usage of profiles and extensions.\n",
      "    * **Terminology:**  Specifies the use of value sets and code systems for defining the data values. \n",
      "    * **Guidance:**  Provides instructions and recommendations for implementing the IG.\n",
      "    * **Security:** Includes information about security requirements for data exchange.\n",
      "\n",
      "This IG is crucial for fostering interoperability in US healthcare, enabling systems to exchange patient information securely and efficiently. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(prompt[0:model_info.input_token_limit])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This JSON file represents a FHIR Implementation Guide (IG) called \"US Core Implementation Guide\". It's designed to define the minimum conformance requirements for accessing patient data within the US healthcare system. \n",
    "\n",
    "Here's a breakdown of the key information and purpose:\n",
    "\n",
    "**Key Information:**\n",
    "\n",
    "* **ID:** hl7.fhir.us.core\n",
    "* **URL:** http://hl7.org/fhir/us/core/ImplementationGuide/hl7.fhir.us.core\n",
    "* **Version:** 7.0.0\n",
    "* **Name:** USCore\n",
    "* **Title:** US Core Implementation Guide\n",
    "* **Status:** active\n",
    "* **Publisher:** HL7 International / Cross-Group Projects\n",
    "* **Description:**  The IG builds upon FHIR R4 and incorporates requirements from past initiatives like Argonaut, ONC CCDS, and USCDI. It aims to facilitate interoperability by establishing minimum standards for accessing patient data. \n",
    "* **Jurisdiction:** US\n",
    "* **FHIR Version:** 4.0.1\n",
    "* **Dependencies:**  Relies on several other FHIR IGs and packages, such as HL7 Terminology, Smart App Launch, VSAC, and others.\n",
    "* **Profiles and Extensions:** Defines numerous FHIR profiles and extensions for various resource types (e.g., Patient, Encounter, Observation, MedicationRequest). These profiles detail specific requirements for data elements, codes, and value sets.\n",
    "* **Search Parameters and Operations:** Includes search parameters and operations, such as \"$docref\", for retrieving patient data in a standardized way. \n",
    "* **Terminology:**  Uses standard terminologies like LOINC and SNOMED CT for data elements.\n",
    "* **Examples:** Provides numerous example resources that demonstrate the use of the profiles and extensions.\n",
    "* **Guidance:** Offers general guidance, clinical notes guidance, medication list guidance, and specific guidance on USCDI requirements. \n",
    "* **Future Directions:**  Outlines plans for future expansion and updates to the US Core IG.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "The main purpose of the US Core Implementation Guide is to:\n",
    "\n",
    "* **Promote interoperability:** By defining common standards for accessing patient data, it allows different healthcare systems to exchange information more effectively.\n",
    "* **Support data exchange:**  It provides a framework for implementing FHIR within the US healthcare landscape, ensuring that systems can share essential patient data.\n",
    "* **Enable data access:**  It establishes the minimum requirements for accessing and using patient data, promoting patient-centered care.\n",
    "* **Facilitate certification:**  It serves as a foundation for ONC Health IT certification, ensuring that systems meet the necessary standards for interoperability. \n",
    "\n",
    "In essence, the US Core Implementation Guide acts as a roadmap for implementing FHIR in the US, promoting interoperability and enhancing the use of electronic health information. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft prompt to ask LLM to summarize IG text\n",
    "prompt = \"\"\"Please analyze this Implementation Guide and provide:\n",
    "1. A high-level summary of what this IG is about\n",
    "2. Key profiles and extensions defined\n",
    "3. Main requirements and constraints\n",
    "4. Notable usage patterns or guidance\n",
    "\n",
    "In as much detail as possible, please organize the information clearly and highlight particularly important aspects.\"\"\"\n",
    "\n",
    "# analyze partial combined text (text currently too large to all be ingested)\n",
    "# result = analyze_ig(combined_content[1:15000], prompt)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with sending the IG in smaller chunks\n",
    "Note: see prompt chaining notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Images to LLM (Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in ./.venv/lib/python3.12/site-packages (0.37.1)\n",
      "Requirement already satisfied: IPython in ./.venv/lib/python3.12/site-packages (8.29.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in ./.venv/lib/python3.12/site-packages (from anthropic) (0.20.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.12/site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from IPython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from IPython) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from IPython) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from IPython) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.12/site-packages (from IPython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./.venv/lib/python3.12/site-packages (from IPython) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from IPython) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->IPython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->IPython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.23.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers>=0.13.0->anthropic) (0.26.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack-data->IPython) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack-data->IPython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack-data->IPython) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->IPython) (1.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#ensure you have installed IPython\n",
    "#%pip install anthropic IPython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from IPython.display import Image\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new claude instance and define a function to decode an encoded image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up claude instance\n",
    "client = create_anthropic_client()\n",
    "MODEL_NAME = \"claude-3-opus-20240229\"\n",
    "\n",
    "#function to decode base64 encoded image\n",
    "def get_base64_encoded_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        binary_data = image_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode('utf-8')\n",
    "        return base64_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test the Claude vision capability on a diagram from the PDEX Plan Net IG and ask the LLM to summarize the diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This diagram appears to be an entity-relationship (ER) diagram representing a data model related to healthcare organizations.\n",
      "\n",
      "The main entity is \"Organization\", which has relationships with several other entities:\n",
      "\n",
      "1. An Organization is partOf another Organization (recursive relationship)\n",
      "2. An Organization ownedBy a Network\n",
      "3. An Organization administeredBy an InsurancePlan, which covers a certain coverageArea\n",
      "4. An Organization participatesIn an OrganizationAffiliation\n",
      "5. An Organization is partOf another Organization (another recursive relationship)\n",
      "\n",
      "The \"Practitioner\" entity has two relationships:\n",
      "1. A PractitionerRole is associated with an Organization\n",
      "2. A Practitioner works at a Location\n",
      "\n",
      "Other entities include:\n",
      "- Endpoint: Likely represents connection points within the Network\n",
      "- HealthcareService: Services provided by the Organization, available at certain Locations\n",
      "\n",
      "The numbers on the relationship lines (e.g., 0..1, 1..1) represent cardinality - the minimum and maximum number of instances of one entity that can be associated with a single instance of the related entity.\n",
      "\n",
      "Overall, this data model captures the complex relationships between healthcare organizations, networks, practitioners, insurance plans, and service locations. It would allow tracking which organizations belong to which networks, what services they provide, their affiliations, and the practitioners working at each location.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set path to example image\n",
    "image_path='full-ig/site/Figures/Slide1.png'\n",
    "\n",
    "#set message and prompt to Claude API\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": 'user',\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": get_base64_encoded_image(image_path)}},\n",
    "            {\"type\": \"text\", \"text\": \"Explain the diagram\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# generate response\n",
    "response = client.messages.create(\n",
    "    model=MODEL_NAME,\n",
    "    max_tokens=2048,\n",
    "    messages=message_list\n",
    ")\n",
    "\n",
    "#print the text of claude's response\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Read in relevant context files \n",
    "- IG_golden_rules\n",
    "- IG_example\n",
    "- IG_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
