{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Narrative from HTML to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores extracting narrative elements from HTML files. It relies on having a downloaded IG folder structured full-ig/site containing input files. It outputs new markdown files in full-ig/markdown_output. Input and output directory paths need to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from bs4.element import Tag\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.document_transformers import MarkdownifyTransformer\n",
    "from urllib.parse import urlparse\n",
    "from langchain.schema import Document\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain tool (Markdownify) to convert HTML to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to store Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where you want to save the markdown files\n",
    "output_dir = '/Users/ceadams/Documents/onclaive/onclaive/us-core/markdown_output1'\n",
    "# Create output directory if it doesn't exist\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary libraries if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  markdownify\n",
    "# %pip install -U lxml\n",
    "# %pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_local_html_to_markdown(input_dir, output_dir=\"markdown_output\", exclude_patterns=None):\n",
    "    \"\"\"\n",
    "    Convert HTML files from a local directory to markdown, excluding files matching specific patterns.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to the directory containing HTML files\n",
    "        output_dir (str): Path to save the markdown files\n",
    "        exclude_patterns (list): List of regex patterns to exclude\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Compile regex patterns for exclusion if provided\n",
    "    if exclude_patterns:\n",
    "        compiled_patterns = [re.compile(pattern) for pattern in exclude_patterns]\n",
    "    else:\n",
    "        compiled_patterns = [re.compile(r'\\.ttl\\.html$'), re.compile(r'\\.xml\\.html$')]\n",
    "    \n",
    "    # Get all HTML files in the directory\n",
    "    html_files = []\n",
    "    for file in Path(input_dir).glob('**/*.html'):\n",
    "        file_str = str(file)\n",
    "        \n",
    "        # Check if the file should be excluded\n",
    "        exclude = False\n",
    "        for pattern in compiled_patterns:\n",
    "            if pattern.search(file_str):\n",
    "                exclude = True\n",
    "                break\n",
    "        \n",
    "        if not exclude:\n",
    "            html_files.append(file)\n",
    "    \n",
    "    print(f\"Found {len(html_files)} HTML files to process\")\n",
    "    \n",
    "    # Process each HTML file\n",
    "    processed = 0\n",
    "    errors = 0\n",
    "    md_transformer = MarkdownifyTransformer()\n",
    "    \n",
    "    for i, html_file in enumerate(html_files):\n",
    "        try:\n",
    "            # Create relative path to preserve directory structure\n",
    "            rel_path = html_file.relative_to(input_dir)\n",
    "            output_path = Path(output_dir) / rel_path.with_suffix('.md')\n",
    "            \n",
    "            # Create parent directories if they don't exist\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Load HTML content\n",
    "            with open(html_file, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "            \n",
    "            # === NEW HEADER PROCESSING LOGIC ===\n",
    "            try:\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                \n",
    "                # Find the style tag with CSS heading prefix information\n",
    "                style_tag = soup.find(\"style\", attrs={'type': 'text/css'})\n",
    "                \n",
    "                if style_tag and style_tag.text:\n",
    "                    # Look for heading prefix pattern in CSS\n",
    "                    h_prefix_match = re.search(r'(h[0-9])\\s*\\{\\s*--heading-prefix\\s*:\\s*\"([0-9]+(?:\\.[0-9]+)*)\"', style_tag.text)\n",
    "                    \n",
    "                    if h_prefix_match:\n",
    "                        # Extract the starting header level and numbering\n",
    "                        starting_header_level = int(h_prefix_match.group(1)[1:])\n",
    "                        prev_level = starting_header_level - 1\n",
    "                        header_list = [int(x) for x in h_prefix_match.group(2).split('.')]\n",
    "                        \n",
    "                        # Process all headers in the document\n",
    "                        for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "                            header_level = int(header.name[1:])\n",
    "                            \n",
    "                            # Adjust header numbering based on level\n",
    "                            if header_level > prev_level:\n",
    "                                # Going deeper - extend the list with zeros if needed\n",
    "                                while len(header_list) < header_level:\n",
    "                                    header_list.append(0)\n",
    "                            elif header_level < prev_level:\n",
    "                                # Going shallower - trim the list\n",
    "                                header_list = header_list[:header_level]\n",
    "                            \n",
    "                            # Increment the counter at current level\n",
    "                            if len(header_list) >= header_level:\n",
    "                                header_list[header_level - 1] += 1\n",
    "                            else:\n",
    "                                header_list.append(1)\n",
    "                            \n",
    "                            # Create the numbered header text\n",
    "                            header_number = \".\".join([str(x) for x in header_list[:header_level]])\n",
    "                            markdown_header = \" \".join([\n",
    "                                \"#\" * header_level, \n",
    "                                header_number, \n",
    "                                header.get_text(strip=True)\n",
    "                            ])\n",
    "                            \n",
    "                            # Replace the original header with numbered version\n",
    "                            header.replace_with(markdown_header)\n",
    "                            prev_level = header_level\n",
    "                \n",
    "                # Use the processed soup content\n",
    "                processed_content = str(soup)\n",
    "                \n",
    "            except Exception as header_error:\n",
    "                print(f\"Warning: Header processing failed for {html_file}: {str(header_error)}\")\n",
    "                print(\"Falling back to original HTML content...\")\n",
    "                processed_content = html_content\n",
    "            \n",
    "            # === END HEADER PROCESSING LOGIC ===\n",
    "            \n",
    "            # Create a LangChain Document object with the processed HTML content\n",
    "            doc = Document(page_content=processed_content)\n",
    "            \n",
    "            # Transform to Markdown\n",
    "            converted_docs = md_transformer.transform_documents([doc])\n",
    "            \n",
    "            # Write to output file\n",
    "            if converted_docs and len(converted_docs) > 0:\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(converted_docs[0].page_content)\n",
    "                processed += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0 or i == len(html_files) - 1:\n",
    "                print(f\"Processed {i + 1}/{len(html_files)} files\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {html_file}: {str(e)}\")\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"Conversion complete. Successfully processed {processed} files. Encountered {errors} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2476 HTML files to process\n",
      "Processed 10/2476 files\n",
      "Processed 20/2476 files\n",
      "Processed 30/2476 files\n",
      "Processed 40/2476 files\n",
      "Processed 50/2476 files\n",
      "Processed 60/2476 files\n",
      "Processed 70/2476 files\n",
      "Processed 80/2476 files\n",
      "Processed 90/2476 files\n",
      "Processed 100/2476 files\n",
      "Processed 110/2476 files\n",
      "Processed 120/2476 files\n",
      "Processed 130/2476 files\n",
      "Processed 140/2476 files\n",
      "Processed 150/2476 files\n",
      "Processed 160/2476 files\n",
      "Processed 170/2476 files\n",
      "Processed 180/2476 files\n",
      "Processed 190/2476 files\n",
      "Processed 200/2476 files\n",
      "Processed 210/2476 files\n",
      "Processed 220/2476 files\n",
      "Processed 230/2476 files\n",
      "Processed 240/2476 files\n",
      "Processed 250/2476 files\n",
      "Processed 260/2476 files\n",
      "Processed 270/2476 files\n",
      "Processed 280/2476 files\n",
      "Processed 290/2476 files\n",
      "Processed 300/2476 files\n",
      "Processed 310/2476 files\n",
      "Processed 320/2476 files\n",
      "Processed 330/2476 files\n",
      "Processed 340/2476 files\n",
      "Processed 350/2476 files\n",
      "Processed 360/2476 files\n",
      "Processed 370/2476 files\n",
      "Processed 380/2476 files\n",
      "Processed 390/2476 files\n",
      "Processed 400/2476 files\n",
      "Processed 410/2476 files\n",
      "Processed 420/2476 files\n",
      "Processed 430/2476 files\n",
      "Processed 440/2476 files\n",
      "Processed 450/2476 files\n",
      "Processed 460/2476 files\n",
      "Processed 470/2476 files\n",
      "Processed 480/2476 files\n",
      "Processed 490/2476 files\n",
      "Processed 500/2476 files\n",
      "Processed 510/2476 files\n",
      "Processed 520/2476 files\n",
      "Processed 530/2476 files\n",
      "Processed 540/2476 files\n",
      "Processed 550/2476 files\n",
      "Processed 560/2476 files\n",
      "Processed 570/2476 files\n",
      "Processed 580/2476 files\n",
      "Processed 590/2476 files\n",
      "Processed 600/2476 files\n",
      "Processed 610/2476 files\n",
      "Processed 620/2476 files\n",
      "Processed 630/2476 files\n",
      "Processed 640/2476 files\n",
      "Processed 650/2476 files\n",
      "Processed 660/2476 files\n",
      "Processed 670/2476 files\n",
      "Processed 680/2476 files\n",
      "Processed 690/2476 files\n",
      "Processed 700/2476 files\n",
      "Processed 710/2476 files\n",
      "Processed 720/2476 files\n",
      "Processed 730/2476 files\n",
      "Processed 740/2476 files\n",
      "Processed 750/2476 files\n",
      "Processed 760/2476 files\n",
      "Processed 770/2476 files\n",
      "Processed 780/2476 files\n",
      "Processed 790/2476 files\n",
      "Processed 800/2476 files\n",
      "Processed 810/2476 files\n",
      "Processed 820/2476 files\n",
      "Processed 830/2476 files\n",
      "Processed 840/2476 files\n",
      "Processed 850/2476 files\n",
      "Processed 860/2476 files\n",
      "Processed 870/2476 files\n",
      "Processed 880/2476 files\n",
      "Processed 890/2476 files\n",
      "Processed 900/2476 files\n",
      "Processed 910/2476 files\n",
      "Processed 920/2476 files\n",
      "Processed 930/2476 files\n",
      "Processed 940/2476 files\n",
      "Processed 950/2476 files\n",
      "Processed 960/2476 files\n",
      "Processed 970/2476 files\n",
      "Processed 980/2476 files\n",
      "Processed 990/2476 files\n",
      "Processed 1000/2476 files\n",
      "Processed 1010/2476 files\n",
      "Processed 1020/2476 files\n",
      "Processed 1030/2476 files\n",
      "Processed 1040/2476 files\n",
      "Processed 1050/2476 files\n",
      "Processed 1060/2476 files\n",
      "Processed 1070/2476 files\n",
      "Processed 1080/2476 files\n",
      "Processed 1090/2476 files\n",
      "Processed 1100/2476 files\n",
      "Processed 1110/2476 files\n",
      "Processed 1120/2476 files\n",
      "Processed 1130/2476 files\n",
      "Processed 1140/2476 files\n",
      "Processed 1150/2476 files\n",
      "Processed 1160/2476 files\n",
      "Processed 1170/2476 files\n",
      "Processed 1180/2476 files\n",
      "Processed 1190/2476 files\n",
      "Processed 1200/2476 files\n",
      "Processed 1210/2476 files\n",
      "Processed 1220/2476 files\n",
      "Processed 1230/2476 files\n",
      "Processed 1240/2476 files\n",
      "Processed 1250/2476 files\n",
      "Processed 1260/2476 files\n",
      "Processed 1270/2476 files\n",
      "Processed 1280/2476 files\n",
      "Processed 1290/2476 files\n",
      "Processed 1300/2476 files\n",
      "Processed 1310/2476 files\n",
      "Processed 1320/2476 files\n",
      "Processed 1330/2476 files\n",
      "Processed 1340/2476 files\n",
      "Processed 1350/2476 files\n",
      "Processed 1360/2476 files\n",
      "Processed 1370/2476 files\n",
      "Processed 1380/2476 files\n",
      "Processed 1390/2476 files\n",
      "Processed 1400/2476 files\n",
      "Processed 1410/2476 files\n",
      "Processed 1420/2476 files\n",
      "Processed 1430/2476 files\n",
      "Processed 1440/2476 files\n",
      "Processed 1450/2476 files\n",
      "Processed 1460/2476 files\n",
      "Processed 1470/2476 files\n",
      "Processed 1480/2476 files\n",
      "Processed 1490/2476 files\n",
      "Processed 1500/2476 files\n",
      "Processed 1510/2476 files\n",
      "Processed 1520/2476 files\n",
      "Processed 1530/2476 files\n",
      "Processed 1540/2476 files\n",
      "Processed 1550/2476 files\n",
      "Processed 1560/2476 files\n",
      "Processed 1570/2476 files\n",
      "Processed 1580/2476 files\n",
      "Processed 1590/2476 files\n",
      "Processed 1600/2476 files\n",
      "Processed 1610/2476 files\n",
      "Processed 1620/2476 files\n",
      "Processed 1630/2476 files\n",
      "Processed 1640/2476 files\n",
      "Processed 1650/2476 files\n",
      "Processed 1660/2476 files\n",
      "Processed 1670/2476 files\n",
      "Processed 1680/2476 files\n",
      "Processed 1690/2476 files\n",
      "Processed 1700/2476 files\n",
      "Processed 1710/2476 files\n",
      "Processed 1720/2476 files\n",
      "Processed 1730/2476 files\n",
      "Processed 1740/2476 files\n",
      "Processed 1750/2476 files\n",
      "Processed 1760/2476 files\n",
      "Processed 1770/2476 files\n",
      "Processed 1780/2476 files\n",
      "Processed 1790/2476 files\n",
      "Processed 1800/2476 files\n",
      "Processed 1810/2476 files\n",
      "Processed 1820/2476 files\n",
      "Processed 1830/2476 files\n",
      "Processed 1840/2476 files\n",
      "Processed 1850/2476 files\n",
      "Processed 1860/2476 files\n",
      "Processed 1870/2476 files\n",
      "Processed 1880/2476 files\n",
      "Processed 1890/2476 files\n",
      "Processed 1900/2476 files\n",
      "Processed 1910/2476 files\n",
      "Processed 1920/2476 files\n",
      "Processed 1930/2476 files\n",
      "Processed 1940/2476 files\n",
      "Processed 1950/2476 files\n",
      "Processed 1960/2476 files\n",
      "Processed 1970/2476 files\n",
      "Processed 1980/2476 files\n",
      "Processed 1990/2476 files\n",
      "Processed 2000/2476 files\n",
      "Processed 2010/2476 files\n",
      "Processed 2020/2476 files\n",
      "Processed 2030/2476 files\n",
      "Processed 2040/2476 files\n",
      "Processed 2050/2476 files\n",
      "Processed 2060/2476 files\n",
      "Processed 2070/2476 files\n",
      "Processed 2080/2476 files\n",
      "Processed 2090/2476 files\n",
      "Processed 2100/2476 files\n",
      "Processed 2110/2476 files\n",
      "Processed 2120/2476 files\n",
      "Processed 2130/2476 files\n",
      "Processed 2140/2476 files\n",
      "Processed 2150/2476 files\n",
      "Processed 2160/2476 files\n",
      "Processed 2170/2476 files\n",
      "Processed 2180/2476 files\n",
      "Processed 2190/2476 files\n",
      "Processed 2200/2476 files\n",
      "Processed 2210/2476 files\n",
      "Processed 2220/2476 files\n",
      "Processed 2230/2476 files\n",
      "Processed 2240/2476 files\n",
      "Processed 2250/2476 files\n",
      "Processed 2260/2476 files\n",
      "Processed 2270/2476 files\n",
      "Processed 2280/2476 files\n",
      "Processed 2290/2476 files\n",
      "Processed 2300/2476 files\n",
      "Processed 2310/2476 files\n",
      "Processed 2320/2476 files\n",
      "Processed 2330/2476 files\n",
      "Processed 2340/2476 files\n",
      "Processed 2350/2476 files\n",
      "Processed 2360/2476 files\n",
      "Processed 2370/2476 files\n",
      "Processed 2380/2476 files\n",
      "Processed 2390/2476 files\n",
      "Processed 2400/2476 files\n",
      "Processed 2410/2476 files\n",
      "Processed 2420/2476 files\n",
      "Processed 2430/2476 files\n",
      "Processed 2440/2476 files\n",
      "Processed 2450/2476 files\n",
      "Processed 2460/2476 files\n",
      "Processed 2470/2476 files\n",
      "Processed 2476/2476 files\n",
      "Conversion complete. Successfully processed 2476 files. Encountered 0 errors.\n"
     ]
    }
   ],
   "source": [
    "#set input and output directories\n",
    "input_directory = \"/Users/ceadams/Documents/onclaive/onclaive/us-core/site\"\n",
    "output_directory = \"/Users/ceadams/Documents/onclaive/onclaive/us-core/markdown_output1\"\n",
    "\n",
    "# Define patterns to exclude\n",
    "exclude_patterns = [\n",
    "    r'\\.ttl\\.html$',  # Exclude files ending with .ttl.html\n",
    "    r'\\.xml\\.html$',  # Exclude files ending with .xml.html\n",
    "    r'\\.json\\.html$',  # Also exclude .json.html files\n",
    "    r'\\.change\\.history\\.html$',\n",
    "    r'\\.profile\\.history\\.html$',\n",
    "    r'\\-example\\.html$',\n",
    "    r'\\-examples\\.html$'\n",
    "]\n",
    "\n",
    "convert_local_html_to_markdown(\n",
    "    input_dir=input_directory,\n",
    "    output_dir=output_directory,\n",
    "    exclude_patterns=exclude_patterns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files should be stored in the `PlanNet/site/markdown_output` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
