{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON only experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict  # Add this import\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import httpx\n",
    "from anthropic import RateLimitError, Anthropic\n",
    "from openai import OpenAI\n",
    "import google.generativeai as gemini\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Basic setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "CERT_PATH = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "OUTPUT_DIR = 'analysis_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update token limits and batch configuration\n",
    "CLAUDE_CONFIG = {\n",
    "    \"model_name\": \"claude-3-5-sonnet-20240620\",\n",
    "    \"max_tokens\": 4096,  # Reduced from 8192\n",
    "    \"max_input_tokens\": 8000,  # Reduced from 12000\n",
    "    \"requests_per_minute\": 25,\n",
    "    \"delay_between_chunks\": 2,\n",
    "    \"delay_between_batches\": 10,\n",
    "    \"default_batch_size\": 3,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "GPT_CONFIG = {\n",
    "    \"model_name\": \"gpt-3.5-turbo\",\n",
    "    \"max_tokens\": 2048,  # Significantly reduced from 4000\n",
    "    \"max_input_tokens\": 2048,  # Reduced from 3000\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "GEMINI_CONFIG = {\n",
    "    \"model_name\": \"models/gemini-1.5-pro-001\",\n",
    "    \"max_tokens\": 4096,  # Reduced from 8192\n",
    "    \"max_input_tokens\": 8000,  # Reduced from 12000\n",
    "    \"temperature\": 0.7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate number of tokens in text (rough approximation)\"\"\"\n",
    "    # Rough approximation: 1 token â‰ˆ 4 characters for English text\n",
    "    return len(text) // 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in API keys for Claude, Gemini, and GPT from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# System prompts focusing on test kit development\n",
    "SYSTEM_PROMPT = \"\"\"You are a seasoned Healthcare Integration Test Engineer with extensive FHIR experience.\n",
    "Your task is to analyze technical documentation with a focus on creating a comprehensive FHIR Test Kit.\n",
    "Pay special attention to:\n",
    "1. Resource definitions and constraints\n",
    "2. Required and optional elements\n",
    "3. Search parameters and operations\n",
    "4. Technical validation requirements\n",
    "5. Conformance rules and expectations\n",
    "6. Integration points and dependencies\n",
    "7. Test scenarios and edge cases\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Rate Limit Configurations\n",
    "RATE_LIMITS = {\n",
    "    \"claude\": {\n",
    "        \"requests_per_minute\": 25,\n",
    "        \"max_requests_per_day\": 5000,\n",
    "        \"delay_between_requests\": 2\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"requests_per_minute\": 60,\n",
    "        \"max_requests_per_day\": 60000,\n",
    "        \"delay_between_requests\": 1\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"requests_per_minute\": 200,\n",
    "        \"max_requests_per_day\": 10000,\n",
    "        \"delay_between_requests\": 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in RATE_LIMITS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    limits = RATE_LIMITS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= limits['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= limits['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:]\n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < limits['delay_between_requests']:\n",
    "        time.sleep(limits['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service with proper authentication and timeout\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        verify_path = CERT_PATH if os.path.exists(CERT_PATH) else True\n",
    "        http_client = httpx.Client(verify=verify_path, timeout=60.0)  # Increased timeout\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found in environment variables\")\n",
    "        gemini.configure(api_key=gemini_api_key)\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=GEMINI_CONFIG[\"model_name\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": GEMINI_CONFIG[\"max_tokens\"],\n",
    "                \"temperature\": GEMINI_CONFIG[\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "        openai_client = OpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            organization=os.getenv('OPENAI_ORG_ID'),\n",
    "            timeout=60.0  # Add timeout\n",
    "        )\n",
    "        \n",
    "        # Simple ping test instead of full API test\n",
    "        return claude_client, gemini_client, openai_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_json_files(source_folder='full-ig/site', destination_folder='full-ig/json_only'):\n",
    "    \"\"\"\n",
    "    Copy JSON files from source to destination directory,\n",
    "    excluding compound extensions and creating the directory if needed.\n",
    "    \"\"\"\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    json_files = []\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        # Check if the file ends with .json but not with compound extensions\n",
    "        if (file_name.endswith('.json') and \n",
    "            not any(file_name.endswith(ext) for ext in [\n",
    "                '.ttl.json', \n",
    "                '.jsonld.json', \n",
    "                '.xml.json', \n",
    "                '.change.history.json'\n",
    "            ])):\n",
    "            json_files.append(file_name)\n",
    "            # Copy the file to the destination folder\n",
    "            shutil.copy(os.path.join(source_folder, file_name), destination_folder)\n",
    "            \n",
    "    logging.info(f\"Copied {len(json_files)} JSON files to {destination_folder}\")\n",
    "    return json_files\n",
    "\n",
    "def group_files_by_base_name(directory_path: str, delimiter: str = '-') -> dict:\n",
    "    \"\"\"Group files in the directory by their base name.\"\"\"\n",
    "    grouped_files = defaultdict(list)\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json') and delimiter in filename:\n",
    "            base_name = filename.split(delimiter)[0]\n",
    "            grouped_files[base_name].append(filename)\n",
    "    \n",
    "    return grouped_files\n",
    "\n",
    "def copy_files_to_folders(directory_path: str, grouped_files: dict):\n",
    "    \"\"\"Create folders for each base name and copy related files into them.\"\"\"\n",
    "    for base_name, files in grouped_files.items():\n",
    "        if len(files) >= 1:\n",
    "            # Create base name folder\n",
    "            base_folder = os.path.join(directory_path, base_name)\n",
    "            if not os.path.exists(base_folder):\n",
    "                os.makedirs(base_folder)\n",
    "            logging.info(f\"Created folder: {base_folder}\")\n",
    "            \n",
    "            # Copy files to their respective folders\n",
    "            for file in files:\n",
    "                source_file = os.path.join(directory_path, file)\n",
    "                destination_file = os.path.join(base_folder, file)\n",
    "                shutil.copy(source_file, destination_file)\n",
    "\n",
    "def consolidate_jsons(base_directory: str = 'full-ig/json_only'):\n",
    "    \"\"\"\n",
    "    Consolidate related JSON files while maintaining object integrity.\n",
    "    Creates combined files for each resource type.\n",
    "    \"\"\"\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "              if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        folder_path = os.path.join(base_directory, subdir)\n",
    "        combined_data = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        json_content = json.load(f)\n",
    "                        if isinstance(json_content, dict) and 'entry' in json_content:\n",
    "                            combined_data.extend(json_content['entry'])\n",
    "                        else:\n",
    "                            combined_data.append(json_content)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.error(f\"Error decoding JSON from {filename}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if combined_data:\n",
    "            output_filename = f\"{subdir}_combined.json\"\n",
    "            output_path = os.path.join(base_directory, output_filename)\n",
    "            \n",
    "            try:\n",
    "                with open(output_path, 'w') as outfile:\n",
    "                    json.dump({\n",
    "                        \"resourceType\": subdir,\n",
    "                        \"total\": len(combined_data),\n",
    "                        \"entry\": combined_data\n",
    "                    }, outfile, indent=2)\n",
    "                logging.info(f\"Created {output_filename} with {len(combined_data)} entries\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error writing {output_filename}: {e}\")\n",
    "\n",
    "def prepare_json_directory():\n",
    "    \"\"\"Prepare JSON directory with all necessary files.\"\"\"\n",
    "    # Add import if not already present\n",
    "    import shutil\n",
    "    \n",
    "    # Define directories\n",
    "    source_dir = 'full-ig/site'\n",
    "    json_dir = 'full-ig/json_only'\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs('full-ig', exist_ok=True)\n",
    "    os.makedirs(source_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy and organize JSON files\n",
    "    copy_json_files(source_dir, json_dir)\n",
    "    grouped_files = group_files_by_base_name(json_dir)\n",
    "    copy_files_to_folders(json_dir, grouped_files)\n",
    "    consolidate_jsons(json_dir)\n",
    "    \n",
    "    return json_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_analysis_prompt(json_data: Union[dict, list], chunk_num: int, total_chunks: int) -> str:\n",
    "    \"\"\"Create prompt focusing on technical details for test kit development\"\"\"\n",
    "    return f\"\"\"Analyze this portion ({chunk_num} of {total_chunks}) of a FHIR Implementation Guide JSON resource bundle.\n",
    "    Focus on extracting technical details needed for creating a FHIR Test Kit.\n",
    "    \n",
    "    JSON Content:\n",
    "    {json.dumps(json_data, indent=2)}\n",
    "    \n",
    "    Please provide detailed technical analysis of:\n",
    "    1. Resource Types and Profiles\n",
    "        - Required elements and cardinality\n",
    "        - Must Support elements\n",
    "        - Extensions and custom data types\n",
    "    2. Search Parameters\n",
    "        - Parameter names and types\n",
    "        - Required and optional parameters\n",
    "        - Chaining and reverse chaining capabilities\n",
    "    3. Technical Operations\n",
    "        - Supported CRUD operations\n",
    "        - Custom operations\n",
    "        - Required headers and parameters\n",
    "    4. Validation Requirements\n",
    "        - Resource validation rules\n",
    "        - Business logic constraints\n",
    "        - Required terminology bindings\n",
    "    5. Integration Points\n",
    "        - Dependencies between resources\n",
    "        - Required external systems or services\n",
    "        - Authentication and authorization requirements\n",
    "    6. Conformance Requirements\n",
    "        - Server capabilities\n",
    "        - Required profiles and extensions\n",
    "        - Version compatibility\n",
    "\n",
    "    Maintain technical precision and include specific details that would be needed for test case development.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError, Exception))\n",
    ")\n",
    "def split_json_for_llm(json_data: Union[dict, list], llm_config: dict) -> List[dict]:\n",
    "    \"\"\"Split JSON into smaller chunks to avoid token limits\"\"\"\n",
    "    if isinstance(json_data, dict):\n",
    "        json_data = [json_data]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    max_chunk_tokens = llm_config.get('max_input_tokens', 2048)  # Default to smaller chunks\n",
    "    \n",
    "    for item in json_data:\n",
    "        item_json = json.dumps(item)\n",
    "        item_tokens = estimate_tokens(item_json)\n",
    "        \n",
    "        # Handle large individual items by splitting them\n",
    "        if item_tokens > max_chunk_tokens:\n",
    "            split_chunks = split_large_item(item, max_chunk_tokens)\n",
    "            chunks.extend(split_chunks)\n",
    "            continue\n",
    "            \n",
    "        if current_tokens + item_tokens > max_chunk_tokens and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_tokens = 0\n",
    "        \n",
    "        current_chunk.append(item)\n",
    "        current_tokens += item_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def split_large_item(item: dict, max_tokens: int) -> List[List[dict]]:\n",
    "    \"\"\"Split a large item into smaller chunks\"\"\"\n",
    "    chunks = []\n",
    "    if isinstance(item, dict):\n",
    "        current_chunk = {}\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for key, value in item.items():\n",
    "            value_json = json.dumps({key: value})\n",
    "            value_tokens = estimate_tokens(value_json)\n",
    "            \n",
    "            if current_tokens + value_tokens > max_tokens and current_chunk:\n",
    "                chunks.append([current_chunk.copy()])\n",
    "                current_chunk = {}\n",
    "                current_tokens = 0\n",
    "            \n",
    "            current_chunk[key] = value\n",
    "            current_tokens += value_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append([current_chunk])\n",
    "    else:\n",
    "        # If we can't split it, just return it as a single chunk\n",
    "        chunks.append([item])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_with_llm_with_retries(client: Any, content: str, llm_type: str, rate_limiter: dict) -> str:\n",
    "    \"\"\"Process content with specified LLM using rate limiting, retries, and token management\"\"\"\n",
    "    try:\n",
    "        # Apply rate limiting\n",
    "        check_rate_limits(rate_limiter, llm_type)\n",
    "        \n",
    "        # Estimate tokens in content\n",
    "        content_tokens = estimate_tokens(content)\n",
    "        llm_config = globals()[f\"{llm_type.upper()}_CONFIG\"]\n",
    "        max_input_tokens = llm_config.get('max_input_tokens')\n",
    "        \n",
    "        if content_tokens > max_input_tokens:\n",
    "            raise ValueError(f\"Content exceeds maximum input tokens for {llm_type} \"\n",
    "                           f\"({content_tokens} > {max_input_tokens})\")\n",
    "        \n",
    "        if llm_type == \"claude\":\n",
    "            try:\n",
    "                response = client.messages.create(\n",
    "                    model=CLAUDE_CONFIG[\"model_name\"],\n",
    "                    messages=[{\"role\": \"user\", \"content\": content}],\n",
    "                    system=SYSTEM_PROMPT,\n",
    "                    max_tokens=CLAUDE_CONFIG[\"max_tokens\"]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "            except Exception as e:\n",
    "                if \"overloaded\" in str(e).lower():\n",
    "                    logging.warning(f\"Claude API overloaded, waiting before retry...\")\n",
    "                    time.sleep(30)\n",
    "                    raise\n",
    "                raise\n",
    "\n",
    "        elif llm_type == \"gemini\":\n",
    "            try:\n",
    "                response = client.generate_content(content)\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                if \"length\" in str(e).lower():\n",
    "                    logging.error(f\"Content too long for Gemini: {str(e)}\")\n",
    "                    raise ValueError(f\"Content exceeds Gemini limits: {str(e)}\")\n",
    "                logging.warning(f\"Gemini API error: {str(e)}\")\n",
    "                time.sleep(5)\n",
    "                raise\n",
    "\n",
    "        elif llm_type == \"gpt\":\n",
    "            try:\n",
    "                # Calculate total tokens including system prompt\n",
    "                system_tokens = estimate_tokens(SYSTEM_PROMPT)\n",
    "                total_tokens = content_tokens + system_tokens + GPT_CONFIG[\"max_tokens\"]\n",
    "                \n",
    "                if total_tokens > 8192:  # GPT's total context limit\n",
    "                    raise ValueError(f\"Total tokens ({total_tokens}) would exceed GPT context limit\")\n",
    "                \n",
    "                response = client.chat.completions.create(\n",
    "                    model=GPT_CONFIG[\"model_name\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\", \"content\": content}\n",
    "                    ],\n",
    "                    max_tokens=GPT_CONFIG[\"max_tokens\"],\n",
    "                    temperature=GPT_CONFIG[\"temperature\"]\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                if \"context_length_exceeded\" in str(e):\n",
    "                    logging.error(f\"GPT context length exceeded: {str(e)}\")\n",
    "                    raise ValueError(f\"Content exceeds GPT limits: {str(e)}\")\n",
    "                logging.warning(f\"GPT API error: {str(e)}\")\n",
    "                time.sleep(5)\n",
    "                raise\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing with {llm_type}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_chunks_for_llm(json_file_path: str, llm_config: dict) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Prepare JSON file for LLM processing with improved chunking and validation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read and prepare JSON with better error handling\n",
    "        json_data = prepare_json_for_processing(json_file_path)\n",
    "        if json_data is None:\n",
    "            logging.warning(f\"Could not process {json_file_path} - invalid or empty data\")\n",
    "            return []\n",
    "        \n",
    "        # Ensure data is in list format\n",
    "        if isinstance(json_data, dict):\n",
    "            if 'entry' in json_data:\n",
    "                json_data = json_data['entry']\n",
    "            else:\n",
    "                json_data = [json_data]\n",
    "                \n",
    "        # Get max tokens for this LLM\n",
    "        max_tokens = llm_config.get('max_input_tokens', 2048)\n",
    "        \n",
    "        # Calculate overhead tokens (system prompt, template, etc)\n",
    "        overhead_tokens = estimate_tokens(SYSTEM_PROMPT) + 500  # Extra padding for safety\n",
    "        available_tokens = max_tokens - overhead_tokens\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for item in json_data:\n",
    "            # Add resourceType if missing\n",
    "            if isinstance(item, dict) and 'resourceType' not in item:\n",
    "                file_name = os.path.basename(json_file_path)\n",
    "                base_name = file_name.split('_')[0]  # Get resource type from filename\n",
    "                item['resourceType'] = base_name\n",
    "            \n",
    "            # Estimate tokens for this item\n",
    "            item_json = json.dumps(item)\n",
    "            item_tokens = estimate_tokens(item_json)\n",
    "            \n",
    "            # If item is too large, split it\n",
    "            if item_tokens > available_tokens:\n",
    "                split_items = split_large_resource(item, available_tokens)\n",
    "                for split_item in split_items:\n",
    "                    if validate_chunk(split_item):\n",
    "                        chunks.append([split_item])\n",
    "                continue\n",
    "            \n",
    "            # If adding item would exceed limit, start new chunk\n",
    "            if current_tokens + item_tokens > available_tokens and current_chunk:\n",
    "                if validate_chunk(current_chunk):\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = []\n",
    "                current_tokens = 0\n",
    "            \n",
    "            current_chunk.append(item)\n",
    "            current_tokens += item_tokens\n",
    "        \n",
    "        # Add final chunk if it exists\n",
    "        if current_chunk and validate_chunk(current_chunk):\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error preparing chunks from {json_file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def validate_chunk(chunk: Union[dict, list]) -> bool:\n",
    "    \"\"\"\n",
    "    Enhanced validation of FHIR resource chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(chunk, list):\n",
    "            for item in chunk:\n",
    "                if not validate_resource(item):\n",
    "                    return False\n",
    "            return len(chunk) > 0\n",
    "        else:\n",
    "            return validate_resource(chunk)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error validating chunk: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def validate_resource(resource: Any) -> bool:\n",
    "    \"\"\"\n",
    "    Validate individual FHIR resource.\n",
    "    \"\"\"\n",
    "    if not isinstance(resource, dict):\n",
    "        return False\n",
    "        \n",
    "    # Check for required FHIR elements\n",
    "    if 'resourceType' not in resource:\n",
    "        return False\n",
    "        \n",
    "    # Ensure resourceType is not empty\n",
    "    if not resource['resourceType']:\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def validate_json_structure(chunk: Union[dict, list]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that a JSON chunk maintains proper FHIR resource structure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(chunk, list):\n",
    "            for item in chunk:\n",
    "                if not isinstance(item, dict):\n",
    "                    return False\n",
    "                if 'resourceType' not in item:\n",
    "                    return False\n",
    "        elif isinstance(chunk, dict):\n",
    "            if 'resourceType' not in chunk:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error validating JSON structure: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_resource_stats(chunks: List[dict]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get statistics about resources in the chunks for logging and verification.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of JSON chunks\n",
    "    Returns:\n",
    "        Dictionary with resource type counts\n",
    "    \"\"\"\n",
    "    stats = defaultdict(int)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if isinstance(chunk, list):\n",
    "            for item in chunk:\n",
    "                if isinstance(item, dict) and 'resourceType' in item:\n",
    "                    stats[item['resourceType']] += 1\n",
    "        elif isinstance(chunk, dict) and 'resourceType' in chunk:\n",
    "            stats[chunk['resourceType']] += 1\n",
    "            \n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(results: Dict[str, Any], llm_type: str):\n",
    "    \"\"\"Save analysis results with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = os.path.join(OUTPUT_DIR, llm_type, f\"analysis_results_{timestamp}.json\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": timestamp,\n",
    "                \"llm_type\": llm_type,\n",
    "                \"config\": globals()[f\"{llm_type.upper()}_CONFIG\"],\n",
    "                \"generation_date\": datetime.now().isoformat()\n",
    "            },\n",
    "            \"results\": results\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_summary_prompt(summaries: Dict[str, str]) -> str:\n",
    "    \"\"\"Create prompt for technical meta-summary\"\"\"\n",
    "    return f\"\"\"Synthesize these technical analyses into a comprehensive summary focused on FHIR Test Kit development requirements:\n",
    "\n",
    "    {json.dumps(summaries, indent=2)}\n",
    "\n",
    "    Create a detailed technical analysis that preserves specific implementation details:\n",
    "    1. Resource Specifications\n",
    "       - Complete list of resources with their constraints\n",
    "       - Detailed element requirements and cardinalities\n",
    "       - Specific extensions and their usage\n",
    "\n",
    "    2. Interaction Requirements\n",
    "       - Detailed search parameter specifications\n",
    "       - Operation definitions and requirements\n",
    "       - Required and optional capabilities\n",
    "\n",
    "    3. Technical Validation Rules\n",
    "       - Specific validation requirements for each resource\n",
    "       - Business rule constraints\n",
    "       - Required terminologies and value sets\n",
    "\n",
    "    4. Integration Requirements\n",
    "       - Detailed API specifications\n",
    "       - Authentication and authorization requirements\n",
    "       - System dependencies and prerequisites\n",
    "\n",
    "    5. Test Coverage Requirements\n",
    "       - Required test scenarios\n",
    "       - Edge cases and boundary conditions\n",
    "       - Error handling requirements\n",
    "\n",
    "    Focus on maintaining technical precision and specific details needed for test case development.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_json_with_llm_safe(\n",
    "    client: Any,\n",
    "    llm_type: str,\n",
    "    json_file_path: str,\n",
    "    rate_limiter: dict\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Analyze a JSON file with specified LLM using rate limiting and safe error handling\"\"\"\n",
    "    max_retries = 3\n",
    "    retry_delay = 30  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Get appropriate config for LLM\n",
    "            llm_config = globals()[f\"{llm_type.upper()}_CONFIG\"]\n",
    "            \n",
    "            # Prepare and validate chunks\n",
    "            chunks = prepare_chunks_for_llm(json_file_path, llm_config)\n",
    "            if not chunks:\n",
    "                raise ValueError(f\"No valid chunks produced from {json_file_path}\")\n",
    "            \n",
    "            # Log resource statistics\n",
    "            stats = get_resource_stats(chunks)\n",
    "            logging.info(f\"Processing {json_file_path} with {llm_type}. \"\n",
    "                        f\"Resource counts: {json.dumps(stats, indent=2)}\")\n",
    "            \n",
    "            chunk_analyses = []\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                # Create technical analysis prompt\n",
    "                prompt = create_technical_analysis_prompt(chunk, i, len(chunks))\n",
    "                \n",
    "                # Process with rate limiting and retries\n",
    "                analysis = process_with_llm_with_retries(client, prompt, llm_type, rate_limiter)\n",
    "                chunk_analyses.append(analysis)\n",
    "                \n",
    "                # Add delay between chunks\n",
    "                time.sleep(CLAUDE_CONFIG[\"delay_between_chunks\"])\n",
    "            \n",
    "            # Create meta-summary\n",
    "            meta_prompt = create_meta_summary_prompt({\n",
    "                \"chunk_analyses\": chunk_analyses,\n",
    "                \"resource_stats\": stats\n",
    "            })\n",
    "            \n",
    "            meta_summary = process_with_llm_with_retries(client, meta_prompt, llm_type, rate_limiter)\n",
    "            \n",
    "            return {\n",
    "                \"file_path\": json_file_path,\n",
    "                \"resource_stats\": stats,\n",
    "                \"chunk_analyses\": chunk_analyses,\n",
    "                \"meta_summary\": meta_summary\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.warning(f\"Attempt {attempt + 1} failed for {llm_type}, retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(f\"All retries failed for {llm_type} on {json_file_path}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "def run_multi_llm_analysis(json_directory: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Run analysis using multiple LLMs with improved error handling\"\"\"\n",
    "    try:\n",
    "        # Setup clients\n",
    "        claude_client, gemini_client, openai_client = setup_clients()\n",
    "        \n",
    "        # Create single rate limiter for all APIs\n",
    "        rate_limiter = create_rate_limiter()\n",
    "        \n",
    "        # Get JSON files\n",
    "        json_files = [\n",
    "            os.path.join(json_directory, f) \n",
    "            for f in os.listdir(json_directory) \n",
    "            if f.endswith('_combined.json')\n",
    "        ]\n",
    "        \n",
    "        results = {\n",
    "            \"claude\": {},\n",
    "            \"gemini\": {},\n",
    "            \"gpt\": {}\n",
    "        }\n",
    "        \n",
    "        # Process with each LLM\n",
    "        for json_file in json_files:\n",
    "            logging.info(f\"Processing {json_file}\")\n",
    "            \n",
    "            # Process with each LLM using the same rate limiter\n",
    "            for llm_type, client in [\n",
    "                (\"claude\", claude_client),\n",
    "                (\"gemini\", gemini_client),\n",
    "                (\"gpt\", openai_client)\n",
    "            ]:\n",
    "                try:\n",
    "                    results[llm_type][json_file] = analyze_json_with_llm_safe(\n",
    "                        client, llm_type, json_file, rate_limiter\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to process {json_file} with {llm_type}: {str(e)}\")\n",
    "                    results[llm_type][json_file] = {\n",
    "                        \"error\": str(e),\n",
    "                        \"file_path\": json_file\n",
    "                    }\n",
    "                \n",
    "                # Add delay between different LLMs\n",
    "                time.sleep(5)\n",
    "            \n",
    "            # Add delay between files\n",
    "            time.sleep(CLAUDE_CONFIG[\"delay_between_batches\"])\n",
    "        \n",
    "        # Save results\n",
    "        for llm_type, llm_results in results.items():\n",
    "            save_analysis_results(llm_results, llm_type)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in multi-LLM analysis: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_json_for_processing(json_file_path: str) -> Union[dict, list]:\n",
    "    \"\"\"\n",
    "    Read and prepare JSON file for processing, handling encoding issues and validation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try UTF-8-sig to handle BOM\n",
    "        try:\n",
    "            with open(json_file_path, 'r', encoding='utf-8-sig') as f:\n",
    "                data = json.load(f)\n",
    "        except UnicodeDecodeError:\n",
    "            # Fallback to regular UTF-8\n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        \n",
    "        # Handle empty or invalid data\n",
    "        if not data:\n",
    "            logging.warning(f\"Empty data in {json_file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # Extract entries if present in a bundle\n",
    "        if isinstance(data, dict):\n",
    "            if 'entry' in data:\n",
    "                return data['entry']\n",
    "            if 'resourceType' in data:\n",
    "                return [data]  # Single resource\n",
    "            \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading {json_file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def adaptive_chunk_sizing(content: Union[dict, list], max_tokens: int) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Adaptively size chunks based on content length and token limits.\n",
    "    \"\"\"\n",
    "    if isinstance(content, dict):\n",
    "        content = [content]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    # Estimate tokens needed for prompts and system message\n",
    "    overhead_tokens = estimate_tokens(SYSTEM_PROMPT) + 200  # 200 for prompt template\n",
    "    available_tokens = max_tokens - overhead_tokens\n",
    "    \n",
    "    for item in content:\n",
    "        item_json = json.dumps(item)\n",
    "        item_tokens = estimate_tokens(item_json)\n",
    "        \n",
    "        # If single item is too large, try to split it\n",
    "        if item_tokens > available_tokens:\n",
    "            split_chunks = split_large_resource(item, available_tokens)\n",
    "            chunks.extend(split_chunks)\n",
    "            continue\n",
    "            \n",
    "        # Check if adding item would exceed limit\n",
    "        if current_tokens + item_tokens > available_tokens and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_tokens = 0\n",
    "        \n",
    "        current_chunk.append(item)\n",
    "        current_tokens += item_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def split_large_resource(resource: dict, max_tokens: int) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Split large FHIR resources while maintaining valid resource structure.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    try:\n",
    "        # Keep essential FHIR elements\n",
    "        base_resource = {\n",
    "            'resourceType': resource['resourceType'],\n",
    "            'id': resource.get('id', 'split-resource'),\n",
    "            'meta': resource.get('meta', {})\n",
    "        }\n",
    "        base_tokens = estimate_tokens(json.dumps(base_resource))\n",
    "        \n",
    "        current_chunk = base_resource.copy()\n",
    "        current_tokens = base_tokens\n",
    "        \n",
    "        # Process remaining elements\n",
    "        for key, value in resource.items():\n",
    "            if key in base_resource:\n",
    "                continue\n",
    "                \n",
    "            element_json = json.dumps({key: value})\n",
    "            element_tokens = estimate_tokens(element_json)\n",
    "            \n",
    "            if current_tokens + element_tokens > max_tokens:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = base_resource.copy()\n",
    "                current_tokens = base_tokens\n",
    "            \n",
    "            current_chunk[key] = value\n",
    "            current_tokens += element_tokens\n",
    "        \n",
    "        if len(current_chunk) > len(base_resource):\n",
    "            chunks.append(current_chunk)\n",
    "            \n",
    "        return chunks if chunks else [resource]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error splitting resource: {str(e)}\")\n",
    "        return [resource]  # Return original if splitting fails\n",
    "\n",
    "def process_with_llm_with_retries(client: Any, content: str, llm_type: str, rate_limiter: dict) -> str:\n",
    "    \"\"\"Process content with specified LLM using improved error handling.\"\"\"\n",
    "    try:\n",
    "        # Apply rate limiting\n",
    "        check_rate_limits(rate_limiter, llm_type)\n",
    "        \n",
    "        # Get appropriate config\n",
    "        llm_config = globals()[f\"{llm_type.upper()}_CONFIG\"]\n",
    "        \n",
    "        # Estimate tokens and check limits\n",
    "        content_tokens = estimate_tokens(content)\n",
    "        max_input_tokens = llm_config.get('max_input_tokens')\n",
    "        \n",
    "        if content_tokens > max_input_tokens:\n",
    "            # Try to reduce content size for GPT\n",
    "            if llm_type == \"gpt\":\n",
    "                content = truncate_content_for_gpt(content, max_input_tokens)\n",
    "            else:\n",
    "                raise ValueError(f\"Content exceeds maximum input tokens for {llm_type} \"\n",
    "                               f\"({content_tokens} > {max_input_tokens})\")\n",
    "        \n",
    "        # Process with appropriate LLM\n",
    "        if llm_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=CLAUDE_CONFIG[\"model_name\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": content}],\n",
    "                system=SYSTEM_PROMPT,\n",
    "                max_tokens=CLAUDE_CONFIG[\"max_tokens\"]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif llm_type == \"gemini\":\n",
    "            response = client.generate_content(content)\n",
    "            return response.text\n",
    "            \n",
    "        elif llm_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=GPT_CONFIG[\"model_name\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": content}\n",
    "                ],\n",
    "                max_tokens=GPT_CONFIG[\"max_tokens\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing with {llm_type}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def truncate_content_for_gpt(content: str, max_tokens: int) -> str:\n",
    "    \"\"\"Truncate content to fit GPT token limits while maintaining JSON validity.\"\"\"\n",
    "    try:\n",
    "        # Parse content to find JSON portion\n",
    "        import re\n",
    "        json_match = re.search(r'(\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\])', content)\n",
    "        if not json_match:\n",
    "            return content[:max_tokens * 4]  # Rough char estimate\n",
    "            \n",
    "        json_str = json_match.group(0)\n",
    "        json_data = json.loads(json_str)\n",
    "        \n",
    "        # Truncate arrays or objects while maintaining structure\n",
    "        if isinstance(json_data, list):\n",
    "            while estimate_tokens(json.dumps(json_data)) > max_tokens:\n",
    "                json_data = json_data[:-1]\n",
    "        elif isinstance(json_data, dict):\n",
    "            keys = list(json_data.keys())\n",
    "            while estimate_tokens(json.dumps(json_data)) > max_tokens:\n",
    "                if keys:\n",
    "                    del json_data[keys.pop()]\n",
    "                    \n",
    "        # Reconstruct content\n",
    "        before_json = content[:json_match.start()]\n",
    "        after_json = content[json_match.end():]\n",
    "        return before_json + json.dumps(json_data) + after_json\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error truncating content: {str(e)}\")\n",
    "        return content[:max_tokens * 4]  # Fallback to simple truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Copied 166 JSON files to full-ig/json_only\n",
      "INFO:root:Created folder: full-ig/json_only/Location\n",
      "INFO:root:Created folder: full-ig/json_only/StructureDefinition\n",
      "INFO:root:Created folder: full-ig/json_only/ValueSet\n",
      "INFO:root:Created folder: full-ig/json_only/CodeSystem\n",
      "INFO:root:Created folder: full-ig/json_only/OrganizationAffiliation\n",
      "INFO:root:Created folder: full-ig/json_only/SearchParameter\n",
      "INFO:root:Created folder: full-ig/json_only/HealthcareService\n",
      "INFO:root:Created folder: full-ig/json_only/usage\n",
      "INFO:root:Created folder: full-ig/json_only/Organization\n",
      "INFO:root:Created folder: full-ig/json_only/CapabilityStatement\n",
      "INFO:root:Created folder: full-ig/json_only/PractitionerRole\n",
      "INFO:root:Created folder: full-ig/json_only/ImplementationGuide\n",
      "INFO:root:Created folder: full-ig/json_only/InsurancePlan\n",
      "INFO:root:Created folder: full-ig/json_only/Practitioner\n",
      "INFO:root:Created folder: full-ig/json_only/Endpoint\n",
      "INFO:root:Created folder: full-ig/json_only/plan\n",
      "INFO:root:Created Organization_combined.json with 11 entries\n",
      "INFO:root:Created StructureDefinition_combined.json with 21 entries\n",
      "INFO:root:Created CapabilityStatement_combined.json with 1 entries\n",
      "INFO:root:Created Practitioner_combined.json with 3 entries\n",
      "ERROR:root:Error decoding JSON from plan-net.openapi.json: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)\n",
      "INFO:root:Created Location_combined.json with 9 entries\n",
      "INFO:root:Created CodeSystem_combined.json with 14 entries\n",
      "INFO:root:Created usage_combined.json with 1 entries\n",
      "INFO:root:Created ValueSet_combined.json with 24 entries\n",
      "INFO:root:Created HealthcareService_combined.json with 10 entries\n",
      "INFO:root:Created SearchParameter_combined.json with 51 entries\n",
      "INFO:root:Created Endpoint_combined.json with 1 entries\n",
      "INFO:root:Created InsurancePlan_combined.json with 2 entries\n",
      "INFO:root:Created ImplementationGuide_combined.json with 1 entries\n",
      "INFO:root:Created PractitionerRole_combined.json with 6 entries\n",
      "INFO:root:Created OrganizationAffiliation_combined.json with 7 entries\n",
      "INFO:root:Processing full-ig/json_only/Practitioner_combined.json\n",
      "INFO:root:Processing full-ig/json_only/Practitioner_combined.json with claude. Resource counts: {\n",
      "  \"Practitioner\": 3\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.411655 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing full-ig/json_only/Practitioner_combined.json with gemini. Resource counts: {\n",
      "  \"Practitioner\": 3\n",
      "}\n",
      "INFO:root:Processing full-ig/json_only/Practitioner_combined.json with gpt. Resource counts: {\n",
      "  \"Practitioner\": 3\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing full-ig/json_only/Endpoint_combined.json\n",
      "INFO:root:Processing full-ig/json_only/Endpoint_combined.json with claude. Resource counts: {\n",
      "  \"Endpoint\": 1\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing full-ig/json_only/Endpoint_combined.json with gemini. Resource counts: {\n",
      "  \"Endpoint\": 1\n",
      "}\n",
      "INFO:root:Processing full-ig/json_only/Endpoint_combined.json with gpt. Resource counts: {\n",
      "  \"Endpoint\": 1\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing full-ig/json_only/usage_combined.json\n",
      "INFO:root:Processing full-ig/json_only/usage_combined.json with claude. Resource counts: {\n",
      "  \"usage\": 1\n",
      "}\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.444194 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.478398 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing full-ig/json_only/usage_combined.json with gemini. Resource counts: {\n",
      "  \"usage\": 1\n",
      "}\n",
      "INFO:root:Processing full-ig/json_only/usage_combined.json with gpt. Resource counts: {\n",
      "  \"usage\": 3\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing full-ig/json_only/ImplementationGuide_combined.json\n",
      "INFO:root:Processing full-ig/json_only/ImplementationGuide_combined.json with claude. Resource counts: {\n",
      "  \"ImplementationGuide\": 2\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "ERROR:root:Error processing with claude: Content exceeds maximum input tokens for claude (24770 > 8000)\n",
      "WARNING:root:Attempt 1 failed for claude, retrying in 30 seconds...\n",
      "INFO:root:Processing full-ig/json_only/ImplementationGuide_combined.json with claude. Resource counts: {\n",
      "  \"ImplementationGuide\": 2\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "ERROR:root:Error processing with claude: Content exceeds maximum input tokens for claude (24770 > 8000)\n",
      "WARNING:root:Attempt 2 failed for claude, retrying in 30 seconds...\n",
      "INFO:root:Processing full-ig/json_only/ImplementationGuide_combined.json with claude. Resource counts: {\n",
      "  \"ImplementationGuide\": 2\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "ERROR:root:Error processing with claude: Content exceeds maximum input tokens for claude (24770 > 8000)\n",
      "ERROR:root:All retries failed for claude on full-ig/json_only/ImplementationGuide_combined.json: Content exceeds maximum input tokens for claude (24770 > 8000)\n",
      "ERROR:root:Failed to process full-ig/json_only/ImplementationGuide_combined.json with claude: Content exceeds maximum input tokens for claude (24770 > 8000)\n",
      "INFO:root:Processing full-ig/json_only/ImplementationGuide_combined.json with gemini. Resource counts: {\n",
      "  \"ImplementationGuide\": 2\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m json_dir \u001b[38;5;241m=\u001b[39m prepare_json_directory()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Run analysis\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multi_llm_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[106], line 95\u001b[0m, in \u001b[0;36mrun_multi_llm_analysis\u001b[0;34m(json_directory)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m llm_type, client \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     90\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude\u001b[39m\u001b[38;5;124m\"\u001b[39m, claude_client),\n\u001b[1;32m     91\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini\u001b[39m\u001b[38;5;124m\"\u001b[39m, gemini_client),\n\u001b[1;32m     92\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, openai_client)\n\u001b[1;32m     93\u001b[0m ]:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m         results[llm_type][json_file] \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_json_with_llm_safe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_limiter\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     99\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[106], line 32\u001b[0m, in \u001b[0;36manalyze_json_with_llm_safe\u001b[0;34m(client, llm_type, json_file_path, rate_limiter)\u001b[0m\n\u001b[1;32m     29\u001b[0m prompt \u001b[38;5;241m=\u001b[39m create_technical_analysis_prompt(chunk, i, \u001b[38;5;28mlen\u001b[39m(chunks))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Process with rate limiting and retries\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_with_llm_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m chunk_analyses\u001b[38;5;241m.\u001b[39mappend(analysis)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Add delay between chunks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[107], line 147\u001b[0m, in \u001b[0;36mprocess_with_llm_with_retries\u001b[0;34m(client, content, llm_type, rate_limiter)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m llm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m llm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/grpc/_channel.py:1178\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1175\u001b[0m     (\n\u001b[1;32m   1176\u001b[0m         state,\n\u001b[1;32m   1177\u001b[0m         call,\n\u001b[0;32m-> 1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare JSON directory\n",
    "json_dir = prepare_json_directory()\n",
    "        \n",
    "\n",
    "# Run analysis\n",
    "results = run_multi_llm_analysis(json_dir)\n",
    "\n",
    "print(\"\\nAnalysis Complete!\")\n",
    "print(f\"Results saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive query session\n",
    "session = InteractiveQuerySession(client, 'summarized_output/technical_summary_json_only.md')\n",
    "\n",
    "# Ask questions\n",
    "session.ask_question(\"What specific test cases should be created for the Location resource?\")\n",
    "session.ask_question(\"How should we validate the search parameters?\")\n",
    "\n",
    "# Save the conversation\n",
    "session.save_conversation('summarized_output/query_session.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full analysis\n",
    "results = run_full_analysis()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nAnalysis Complete!\")\n",
    "print(f\"Technical analysis saved to: {results['analysis_file']}\")\n",
    "print(f\"Verification results available in: {results['output_directory']}\")\n",
    "print(\"\\nYou can now use the query session for additional questions:\")\n",
    "print(\"query_session = results['query_session']\")\n",
    "print(\"answer = query_session.ask_question('Your question here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the interactive session\n",
    "session = results['query_session']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask additional questions\n",
    "answer = session.ask_question(\"What are the key test scenarios for the Location resource?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the conversation\n",
    "session.save_conversation('summarized_output/interactive_session.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_version = \"models/gemini-1.5-pro-001\" #gemini-1.5-flash-latest\n",
    "gemini_max_output_tokens = 8192\n",
    "temp = 0.75 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
