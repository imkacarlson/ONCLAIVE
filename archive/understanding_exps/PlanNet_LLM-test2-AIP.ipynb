{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from huggingface_hub import InferenceClient\n",
    "import re\n",
    "\n",
    "# Basic setup\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for AIP models\n",
    "API_CONFIGS = {\n",
    "    \"mixtral\": {\n",
    "        \"url\": \"https://mixtral-8x22b.k8s.aip.mitre.org\",\n",
    "        \"max_tokens\": 60000,\n",
    "        \"batch_size\": 3,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 10\n",
    "    },\n",
    "    \"llama\": {\n",
    "        \"url\": \"https://llama3-70b.k8s.aip.mitre.org\",\n",
    "        \"max_tokens\": 4192,\n",
    "        \"batch_size\": 3,\n",
    "        \"delay_between_chunks\": 3,\n",
    "        \"delay_between_batches\": 15\n",
    "    },\n",
    "    \"codestral\": {\n",
    "        \"url\": \"https://codestral-22b.k8s.aip.mitre.org\",\n",
    "        \"max_tokens\": 4192,\n",
    "        \"batch_size\": 3,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 10\n",
    "    },\n",
    "    \"cohere\": {\n",
    "        \"url\": \"https://command-rplus.k8s.aip.mitre.org\",\n",
    "        \"max_tokens\": 4192,\n",
    "        \"batch_size\": 3,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 10\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_markdown(text: str) -> str:\n",
    "    \"\"\"Clean markdown content\"\"\"\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\\\(.)', r'\\1', text)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'[-\\s]*\\n[-\\s]*', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_markdown(content: str, max_size: int = 2000) -> List[str]:\n",
    "    \"\"\"Split markdown into manageable chunks\"\"\"\n",
    "    chunks = []\n",
    "    lines = content.split('\\n')\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line_size = len(line)\n",
    "        if current_size + line_size > max_size:\n",
    "            if current_chunk:\n",
    "                chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [line]\n",
    "            current_size = line_size\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_size += line_size\n",
    "            \n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def create_client(model_type: str) -> InferenceClient:\n",
    "    \"\"\"Create a client for the specified model\"\"\"\n",
    "    return InferenceClient(model=API_CONFIGS[model_type][\"url\"])\n",
    "\n",
    "def process_markdown_chunk(client: InferenceClient, chunk: str, model_type: str) -> str:\n",
    "    \"\"\"Process a single markdown chunk with comprehensive requirements analysis\"\"\"\n",
    "    prompt = f\"\"\"Analyze this FHIR Implementation Guide markdown content. Focus on requirements that would be included in a test kit to verify the behaviors of the system under test for this IG. A requirement defines how a system or user may or must behave.\n",
    "\n",
    "Content to analyze:\n",
    "{chunk}\n",
    "\n",
    "The following information is needed about each requirement for the test kit:\n",
    "1. Requirement statement text. Clarifications and context may be included within brackets to clearly distinguish source quotes from added detail\n",
    "2. Sub-requirements: A reference to a list of requirements from this or another set that this requirement points to.\n",
    "3. Conformance: The conformance level (SHALL, SHOULD, MAY, SHOULD NOT, SHALL NOT) indicating whether systems implementing the requirement can choose to meet it or not. The conformance level helps drive decisions about which requirements to test and which to prioritize creating tests for during test kit planning and development.\n",
    "4. Actor(s): The actor, or actors, that this requirement applies to. Inferno test suites verify the behavior of specific systems that play the role of actors defined in the requirements. Only requirements that apply to the actor(s) played by the system under test need to be verified. Thus, pulling out the actor discretely enables the requirements coverage analysis to determine whether a requirement is in scope for a suite based on the actor(s) it verifies. It also assists with planning by indicating which requirements Inferno will need to verify and which it will need to simulate so it can interact with a given actor to gather the information needed to perform verification of other requirements.\n",
    "5. Conditionality: Optional indicator of whether the requirement is conditional on another decision made by the system under test, such as whether to implement another optional requirement.\n",
    "\n",
    "Extract precise requirements following these guidelines, focusing on new information not covered in previous chunks.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat_completion(messages, max_tokens=API_CONFIGS[model_type][\"max_tokens\"])\n",
    "    return response\n",
    "\n",
    "def process_markdown_content(markdown_dir: str, model_type: str) -> Dict:\n",
    "    \"\"\"Process all markdown content using specified model\"\"\"\n",
    "    client = create_client(model_type)\n",
    "    results = {\n",
    "        \"metadata\": {\n",
    "            \"model\": model_type,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"processed_files\": []\n",
    "        },\n",
    "        \"summaries\": []\n",
    "    }\n",
    "    \n",
    "    for filename in os.listdir(markdown_dir):\n",
    "        if filename.endswith('.md'):\n",
    "            file_path = os.path.join(markdown_dir, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = clean_markdown(f.read())\n",
    "                \n",
    "            chunks = split_markdown(content)\n",
    "            chunk_summaries = []\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                summary = process_markdown_chunk(client, chunk, model_type)\n",
    "                chunk_summaries.append(summary)\n",
    "                time.sleep(API_CONFIGS[model_type][\"delay_between_chunks\"])\n",
    "                \n",
    "            results[\"metadata\"][\"processed_files\"].append(filename)\n",
    "            results[\"summaries\"].extend(chunk_summaries)\n",
    "            time.sleep(API_CONFIGS[model_type][\"delay_between_batches\"])\n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_summary(results: Dict, model_type: str) -> str:\n",
    "    \"\"\"Create a meta-summary from all processed content with enhanced focus on testing requirements\"\"\"\n",
    "    client = create_client(model_type)\n",
    "    \n",
    "    meta_prompt = f\"\"\"Synthesize these content summaries into a comprehensive list outlining testing requirements, including information about conformance, actors, and conditionality:\n",
    "\n",
    "{json.dumps(results['summaries'], indent=2)}\n",
    "\n",
    "Create a comprehensive analysis that:\n",
    "1. Eliminates redundant information\n",
    "2. Maintains technical accuracy\n",
    "3. Includes specific technical information about:\n",
    "   3.1 Requirement statement text. Clarifications and context may be included within brackets to clearly distinguish source quotes from added detail\n",
    "   3.2 Sub-requirements: A reference to a list of requirements from this or another set that this requirement points to.\n",
    "   3.3 Conformance: The conformance level (SHALL, SHOULD, MAY, SHOULD NOT, SHALL NOT) indicating whether systems implementing the requirement can choose to meet it or not. The conformance level helps drive decisions about which requirements to test and which to prioritize creating tests for during test kit planning and development.\n",
    "   3.4 Actor(s): The actor, or actors, that this requirement applies to. Inferno test suites verify the behavior of specific systems that play the role of actors defined in the requirements. Only requirements that apply to the actor(s) played by the system under test need to be verified.\n",
    "   3.5 Conditionality: Optional indicator of whether the requirement is conditional on another decision made by the system under test, such as whether to implement another optional requirement.\n",
    "\n",
    "Focus on extracting precise, testable requirements that would be needed for test kit development.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": meta_prompt}]\n",
    "    return client.chat_completion(messages, max_tokens=API_CONFIGS[model_type][\"max_tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "markdown_dir = \"full-ig/markdown\"\n",
    "output_dir = \"analysis_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process with each model\n",
    "models = [\"mixtral\", \"llama\", \"codestral\", \"cohere\"]\n",
    "\n",
    "for model in models:\n",
    "    try:\n",
    "        logging.info(f\"Processing with {model}...\")\n",
    "        \n",
    "        # Process all markdown content\n",
    "        results = process_markdown_content(markdown_dir, model)\n",
    "        \n",
    "        # Generate meta-summary\n",
    "        meta_summary = create_meta_summary(results, model)\n",
    "        results[\"meta_summary\"] = meta_summary\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = os.path.join(output_dir, f\"{model}_analysis_{timestamp}.json\")\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        logging.info(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {model}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
