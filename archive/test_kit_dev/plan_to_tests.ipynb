{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Inferno Test Generator\n",
    "\n",
    "This notebook generates executable Inferno test files from FHIR Implementation Guide test specifications. The output consists of Ruby tests that can be used to validate FHIR server implementations.\n",
    "\n",
    "#### What it does\n",
    "\n",
    "- Processes test specifications from a consolidated test plan\n",
    "- Analyzes requirements and transforms them into executable test code\n",
    "- Generates comprehensive Ruby tests that follow the Inferno testing framework structure\n",
    "- Creates a complete, organized test kit with proper file structure\n",
    "\n",
    "#### How to use\n",
    "\n",
    "1. **Setup**: Individual cert setup may need to be modified in `setup_clients()` function of the llm_utils.py file before running this notebook. API keys should be in .env file. Make sure you have an API key for at least one of:\n",
    "   - Anthropic Claude (`ANTHROPIC_API_KEY`)\n",
    "   - Google Gemini (`GEMINI_API_KEY`) \n",
    "   - OpenAI GPT-4 (`OPENAI_API_KEY`)\n",
    "\n",
    "2. **Input**: A markdown file with test specifications in the following format:\n",
    "   ```markdown\n",
    "   ### REQ-ID: Requirement Title\n",
    "\n",
    "   **Description**: \"Detailed description of the requirement\"\n",
    "\n",
    "   **Actor**: System component responsible\n",
    "\n",
    "   **Conformance**: SHALL/SHOULD/MAY\n",
    "\n",
    "   # Test Specification for REQ-ID\n",
    "   ... detailed test specification ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHIR Inferno Test Generator\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import httpx\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Constants\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory\n",
    "OUTPUT_DIR = os.path.join(CURRENT_DIR, 'test_output')\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level to project root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "module_path = os.path.join(PROJECT_ROOT, 'llm_utils.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"llm_utils\", module_path)\n",
    "llm_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(llm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 09:21:29,109 - root - INFO - Prompt environment set up at: /Users/ceadams/Documents/onclaive/onclaive/prompts\n",
      "2025-06-16 09:21:29,110 - root - INFO - Using prompts directory: /Users/ceadams/Documents/onclaive/onclaive/prompts\n",
      "2025-06-16 09:21:29,110 - root - INFO - Inferno test generation prompt: /Users/ceadams/Documents/onclaive/onclaive/prompts/test_gen.md\n"
     ]
    }
   ],
   "source": [
    "# Import prompt utilities\n",
    "prompt_utils_path = os.path.join(PROJECT_ROOT, 'prompt_utils.py')\n",
    "spec = importlib.util.spec_from_file_location(\"prompt_utils\", prompt_utils_path)\n",
    "prompt_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(prompt_utils)\n",
    "\n",
    "# Setup the prompt environment\n",
    "prompt_env = prompt_utils.setup_prompt_environment(PROJECT_ROOT)\n",
    "PROMPT_DIR = prompt_env[\"prompt_dir\"]\n",
    "TEST_GEN_PATH = prompt_env[\"test_gen_path\"]\n",
    "\n",
    "logging.info(f\"Using prompts directory: {PROMPT_DIR}\")\n",
    "logging.info(f\"Inferno test generation prompt: {TEST_GEN_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts for test generation\n",
    "INFERNO_TEST_SYSTEM_PROMPT = \"\"\"You are a specialized FHIR testing engineer with expertise in healthcare interoperability.\n",
    "Your task is to convert test specifications from a test plan into executable Ruby tests using the Inferno testing framework.\n",
    "You will generate valid, working Ruby code that follows Inferno test patterns and best practices.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inferno_test_generation_prompt(test_specification: str, requirement_id: str, module_name: str, dsl_guidance: str) -> str:\n",
    "    \"\"\"\n",
    "    Load the Inferno test generation prompt from file and format it with test details\n",
    "    \n",
    "    Args:\n",
    "        test_specification: The test specification content\n",
    "        requirement_id: The ID of the requirement\n",
    "        module_name: The name of the module\n",
    "        inferno_guidance: The Inferno guidance\n",
    "        \n",
    "    Returns:\n",
    "        str: The formatted prompt for the LLM\n",
    "    \"\"\"\n",
    "    return prompt_utils.load_prompt(\n",
    "        TEST_GEN_PATH,\n",
    "        test_specification=test_specification,\n",
    "        requirement_id=requirement_id,\n",
    "        module_name=module_name,\n",
    "        dsl_guidance=dsl_guidance\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_plan(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a consolidated test plan into sections and requirements\n",
    "    focusing on the main requirements\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Initialize empty sections dictionary\n",
    "    sections = {}\n",
    "    \n",
    "    # First try to extract sections from the TOC\n",
    "    toc_pattern = r'## Table of Contents\\n(.*?)(?:\\n## |\\n# |\\Z)'\n",
    "    toc_match = re.search(toc_pattern, content, re.DOTALL)\n",
    "    \n",
    "    if toc_match:\n",
    "        toc_content = toc_match.group(1)\n",
    "        # Extract section names from the TOC\n",
    "        section_pattern = r'- \\[(.*?)\\]'\n",
    "        section_matches = re.findall(section_pattern, toc_content)\n",
    "        \n",
    "        # Only use top-level sections (not the REQ items)\n",
    "        top_level_sections = [s for s in section_matches if not s.startswith('REQ-')]\n",
    "        \n",
    "        # Create section entries for each section found\n",
    "        for section_name in top_level_sections:\n",
    "            clean_section_name = section_name.strip()\n",
    "            section_id = clean_section_name.lower().replace(' ', '-')\n",
    "            sections[clean_section_name] = {\n",
    "                'id': section_id,\n",
    "                'name': clean_section_name,\n",
    "                'content': \"\",\n",
    "                'requirements': []\n",
    "            }\n",
    "    \n",
    "    # If no sections found in TOC, try to find sections from the document headings\n",
    "    if not sections:\n",
    "        # Look for section headers (##) that are not part of the TOC\n",
    "        section_pattern = r'## ([^\\n#]+)(?=\\n(?!## Table of Contents))'\n",
    "        section_matches = re.findall(section_pattern, content)\n",
    "        \n",
    "        for section_name in section_matches:\n",
    "            clean_section_name = section_name.strip()\n",
    "            # Skip if this is a \"Test Specification\" section\n",
    "            if \"Test Specification\" in clean_section_name:\n",
    "                continue\n",
    "                \n",
    "            section_id = clean_section_name.lower().replace(' ', '-')\n",
    "            sections[clean_section_name] = {\n",
    "                'id': section_id,\n",
    "                'name': clean_section_name,\n",
    "                'content': \"\",\n",
    "                'requirements': []\n",
    "            }\n",
    "    \n",
    "    # Find all requirement headers in the document\n",
    "    req_pattern = r'### (REQ-\\d+): (.*?)\\n\\n\\*\\*Description\\*\\*: \"(.*?)\"\\n\\n\\*\\*Actor\\*\\*: (.*?)\\n\\n\\*\\*Conformance\\*\\*: (.*?)(?:\\n\\n|$)'\n",
    "    req_matches = re.findall(req_pattern, content, re.DOTALL)\n",
    "    \n",
    "    print(f\"Found {len(req_matches)} potential requirements\")\n",
    "    \n",
    "    # Process each requirement\n",
    "    for req_id, req_title, req_desc, req_actor, req_conf in req_matches:\n",
    "        print(f\"Processing requirement: {req_id}\")\n",
    "        \n",
    "        # Find the full test specification for this requirement\n",
    "        test_spec_pattern = f\"# Test Specification for {req_id}(.*?)(?:---|\\\\n## |$)\"\n",
    "        test_spec_match = re.search(test_spec_pattern, content, re.DOTALL)\n",
    "        test_spec = test_spec_match.group(1).strip() if test_spec_match else \"\"\n",
    "        \n",
    "        # Try to determine which section this requirement belongs to\n",
    "        assigned_to_section = False\n",
    "        \n",
    "        # First try to find the requirement in one of the identified sections\n",
    "        for section_name, section in sections.items():\n",
    "            section_start = content.find(f\"## {section_name}\")\n",
    "            if section_start == -1:\n",
    "                continue\n",
    "                \n",
    "            # Find the next section to determine where this section ends\n",
    "            next_section_start = len(content)\n",
    "            for other_section in sections.keys():\n",
    "                if other_section != section_name:\n",
    "                    other_start = content.find(f\"## {other_section}\", section_start + len(section_name))\n",
    "                    if other_start > section_start and other_start < next_section_start:\n",
    "                        next_section_start = other_start\n",
    "            \n",
    "            # Extract the content of this section\n",
    "            section_content = content[section_start:next_section_start]\n",
    "            \n",
    "            if f\"### {req_id}\" in section_content:\n",
    "                # This requirement belongs to this section\n",
    "                requirement = {\n",
    "                    'id': req_id,\n",
    "                    'title': req_title.strip(),\n",
    "                    'description': req_desc.strip(),\n",
    "                    'actor': req_actor.strip(),\n",
    "                    'conformance': req_conf.strip(),\n",
    "                    'full_content': f\"### {req_id}: {req_title}\\n\\n**Description**: \\\"{req_desc}\\\"\\n\\n**Actor**: {req_actor}\\n\\n**Conformance**: {req_conf}\",\n",
    "                    'full_spec': test_spec,\n",
    "                    'section': section_name,\n",
    "                    'testability': 'Automatic'  # Default value\n",
    "                }\n",
    "                \n",
    "                sections[section_name]['requirements'].append(requirement)\n",
    "                print(f\"Added requirement {req_id} to section {section_name}\")\n",
    "                assigned_to_section = True\n",
    "                break\n",
    "        \n",
    "        # If we couldn't find the requirement in any of our sections,\n",
    "        # create a section based on the actor if it doesn't exist already\n",
    "        if not assigned_to_section:\n",
    "            actor = req_actor.strip()\n",
    "            if not actor:\n",
    "                actor = \"Unknown Actor\"\n",
    "                \n",
    "            if actor not in sections:\n",
    "                section_id = actor.lower().replace(' ', '-')\n",
    "                sections[actor] = {\n",
    "                    'id': section_id,\n",
    "                    'name': actor,\n",
    "                    'content': \"\",\n",
    "                    'requirements': []\n",
    "                }\n",
    "            \n",
    "            requirement = {\n",
    "                'id': req_id,\n",
    "                'title': req_title.strip(),\n",
    "                'description': req_desc.strip(),\n",
    "                'actor': actor,\n",
    "                'conformance': req_conf.strip(),\n",
    "                'full_content': f\"### {req_id}: {req_title}\\n\\n**Description**: \\\"{req_desc}\\\"\\n\\n**Actor**: {req_actor}\\n\\n**Conformance**: {req_conf}\",\n",
    "                'full_spec': test_spec,\n",
    "                'section': actor,\n",
    "                'testability': 'Automatic'  # Default value\n",
    "            }\n",
    "            \n",
    "            sections[actor]['requirements'].append(requirement)\n",
    "            print(f\"Added requirement {req_id} to actor-based section {actor}\")\n",
    "    \n",
    "    # Remove empty sections\n",
    "    sections = {k: v for k, v in sections.items() if v['requirements']}\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inferno_guidance() -> str:\n",
    "    \"\"\"\n",
    "    Load the Inferno guidance document\n",
    "    \n",
    "    Returns:\n",
    "        String containing Inferno test development guidance\n",
    "    \"\"\"\n",
    "    guidance_path = os.path.join(PROJECT_ROOT, 'dsl-guidance.md')\n",
    "    \n",
    "    # Use a default guidance if file not found\n",
    "    if not os.path.exists(guidance_path):\n",
    "        return \"\"\"# Inferno Test Development Guidance\n",
    "        \n",
    "        Inferno is a Ruby-based testing framework for FHIR implementations. Tests should follow the structure:\n",
    "        \n",
    "        ```ruby\n",
    "        module YourTestKit\n",
    "          class YourTestGroup < Inferno::TestGroup\n",
    "            id :unique_id\n",
    "            title 'Test Group Title'\n",
    "            description 'Detailed description'\n",
    "            \n",
    "            test do\n",
    "              id :test_unique_id\n",
    "              title 'Test Title'\n",
    "              description 'Test description'\n",
    "              \n",
    "              run do\n",
    "                # Test implementation\n",
    "                assert condition, 'Failure message'\n",
    "              end\n",
    "            end\n",
    "          end\n",
    "        end\n",
    "        ```\n",
    "        \"\"\"\n",
    "    \n",
    "    with open(guidance_path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_test_groups(sections: Dict[str, Dict[str, Any]], expected_actors: List[str]) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Group test specifications based on the sections and actors from the test plan\n",
    "    \n",
    "    Args:\n",
    "        sections: Dictionary of sections from the parsed test plan\n",
    "        expected_actors: List of expected actors from user input\n",
    "        \n",
    "    Returns:\n",
    "        Nested dictionary of grouped test specifications\n",
    "    \"\"\"\n",
    "    # First, group by actor\n",
    "    actor_groups = defaultdict(list)\n",
    "    \n",
    "    # Create a mapping from normalized actor names to expected actor names\n",
    "    actor_mapping = {actor.lower().replace(' ', '_'): actor for actor in expected_actors}\n",
    "    \n",
    "    # Collect all requirements across all sections\n",
    "    for section_name, section in sections.items():\n",
    "        for req in section['requirements']:\n",
    "            # Use the exact actor name from the requirement\n",
    "            actor = req['actor'].strip()\n",
    "            if not actor:\n",
    "                actor = \"Unknown Actor\"\n",
    "            else:\n",
    "                # Try to match with expected actors (case-insensitive)\n",
    "                actor_lower = actor.lower().replace(' ', '_')\n",
    "                if actor_lower in actor_mapping:\n",
    "                    actor = actor_mapping[actor_lower]\n",
    "            \n",
    "            # Store the requirement with its section information\n",
    "            req_with_section = req.copy()\n",
    "            req_with_section['section_name'] = section_name\n",
    "            req_with_section['section_id'] = section['id']\n",
    "            \n",
    "            actor_groups[actor].append(req_with_section)\n",
    "    \n",
    "    # Only include actors that have requirements\n",
    "    result_groups = {}\n",
    "    for actor, reqs in actor_groups.items():\n",
    "        if reqs:  # Only include non-empty actor groups\n",
    "            if actor not in result_groups:\n",
    "                result_groups[actor] = {}\n",
    "            \n",
    "            # Group by original section\n",
    "            section_groups = defaultdict(list)\n",
    "            for req in reqs:\n",
    "                section_groups[req['section_name']].append(req)\n",
    "            \n",
    "            # Add the section groups to the result\n",
    "            result_groups[actor] = dict(section_groups)\n",
    "    \n",
    "    return result_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_test_with_llm(client, api_type, test_code, dsl_guidance, rate_limit_func):\n",
    "    \"\"\"Use LLM to check and correct common issues in generated test code.\"\"\"\n",
    "    \n",
    "    validation_prompt = f\"\"\"\n",
    "    Review this Inferno test code for common issues and fix them:\n",
    "    \n",
    "    Property usage:\n",
    "       - The 'links' property can ONLY be used in a TestSuite, not in TestGroups or individual Tests\n",
    "       - Check proper use of 'test from:' vs 'group from:':\n",
    "         * Use 'test from:' only for individual Test objects\n",
    "         * Use 'group from:' for TestGroup objects\n",
    "         \n",
    "    Input declarations:\n",
    "       - Ensure each test explicitly declares all inputs it uses with the `input` method\n",
    "       - Don't use server_url or access_token without declaring them as inputs\n",
    "       - Use valid input types (see guidance below; e.g., text, textarea, radio, checkbox, oauth_credentials)     \n",
    "   \n",
    "   FHIR client usage:\n",
    "       - Never create FHIR clients manually with 'FHIR::Client.new()' \n",
    "       - Always use the Inferno DSL's 'fhir_client' block\n",
    "       - Do not use non-existent methods like 'use_additional_headers='\n",
    "    \n",
    "    HTTP and FHIR requests:\n",
    "       - Never use direct HTTP methods like 'get', 'post', etc. - use the Inferno DSL methods instead\n",
    "       - Use proper FHIR client methods - 'fhir_get_capability_statement' instead of 'get capability_statement_url'\n",
    "       - NEVER redefine the 'response' variable which is automatically created by Inferno\n",
    "       \n",
    "    ID references:\n",
    "    - Ensure test/group IDs match those in the referenced files\n",
    "    - Don't reference filename-based IDs, but use the actual defined ID in the referenced file\n",
    "    - Example: If 'req_XX_test.rb' contains a group with ID `:example_text`, \n",
    "        reference that ID, not `:req_XX_test`\n",
    "\n",
    "    Response handling:\n",
    "       - Use hash syntax for response access: response[:code], response[:body], etc.\n",
    "       - Don't use dot notation like response.code or response.body\n",
    "       - For headers, use response[:headers] and iterate through the array of headers\n",
    "       - Fix patterns like 'response.headers['www-authenticate']' to use proper header access\n",
    "    \n",
    "    Assertion patterns:\n",
    "       - Use assertions instead of exceptions for failures\n",
    "       - Do not use rescue blocks for error handling in tests\n",
    "       - Never rescue StandardError in test implementations\n",
    "    \n",
    "    Request management:\n",
    "        - When using 'uses_request', ensure a previous test has declared 'makes_request' with the same name\n",
    "        - Ensure proper request naming and reference\n",
    "        \n",
    "    Resources: \n",
    "        - Only test resources specified in the test plan - don't add tests for resources not mentioned\n",
    "    \n",
    "    Here's the test code:\n",
    "    ```ruby\n",
    "    {test_code}\n",
    "    ```\n",
    "    Please ensure the code is valid Ruby and follows the below Inferno best practices, using the Inferno domain specific language.\n",
    "    {dsl_guidance}\n",
    "    \n",
    "    Return ONLY the corrected code without explanations.\n",
    "    \"\"\"\n",
    "    # Add token counting\n",
    "    actual_tokens = llm_utils.count_tokens(validation_prompt, api_type)\n",
    "    logger.info(f\"Validation for test: Sending {actual_tokens} tokens to {api_type} API\")\n",
    "    \n",
    "    # Check if we need to truncate the guidance to fit within token limits\n",
    "    if actual_tokens > 70000:  # Assuming a high limit for Claude\n",
    "        logger.warning(f\"Validation prompt exceeds token limit ({actual_tokens})\")\n",
    "        \n",
    "    \n",
    "    corrected_code = llm_utils.make_llm_request(\n",
    "        client, \n",
    "        api_type, \n",
    "        validation_prompt, \n",
    "        \"You are an expert Ruby developer specialized in FHIR Inferno testing.\", \n",
    "        rate_limit_func\n",
    "    )\n",
    "    \n",
    "    # Clean up any markdown formatting\n",
    "    if corrected_code.startswith('```ruby'):\n",
    "        corrected_code = re.sub(r'^```ruby\\n', '', corrected_code)\n",
    "        corrected_code = re.sub(r'\\n```$', '', corrected_code)\n",
    "    elif corrected_code.startswith('```'):\n",
    "        corrected_code = re.sub(r'^```\\n', '', corrected_code)\n",
    "        corrected_code = re.sub(r'\\n```$', '', corrected_code)\n",
    "    \n",
    "    return corrected_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests_for_section(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    section: Dict[str, Any],\n",
    "    dsl_guidance: str,\n",
    "    module_name: str,\n",
    "    rate_limit_func,\n",
    "    max_input_token_limit: int = 16000\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate tests for an entire section or individual requirements based on token limits\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        section: Section dictionary containing requirements\n",
    "        dsl_guidance: Inferno test development guidance\n",
    "        module_name: Name of the module for the test\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        max_input_token_limit: Maximum tokens for the model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping requirement IDs to generated tests\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating tests for section: {section['name']}\")\n",
    "\n",
    "    # First, try to generate tests for the entire section\n",
    "    if len(section['requirements']) > 1:\n",
    "        # Construct a prompt for the entire section\n",
    "        section_prompt = f\"\"\"\n",
    "        Generate Inferno tests for the following section of requirements from a FHIR implementation guide.\n",
    "        \n",
    "        Section: {section['name']}\n",
    "        \n",
    "        Requirements:\n",
    "        {section['content']}\n",
    "        \n",
    "        For each requirement, create a separate Inferno test class following the naming convention:\n",
    "        - Class name: {module_name}[Actor][Resource][REQ-ID]Test\n",
    "        - File name: req_[id]_test.rb\n",
    "        \n",
    "        Module Name: {module_name}\n",
    "        \n",
    "        Follow this Inferno development guidance:\n",
    "        {dsl_guidance}\n",
    "        \n",
    "        Return the Ruby code for each test implementation, separated by clear markers indicating the requirement ID.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate token count (rough approximation)\n",
    "        estimated_tokens = len(section_prompt) / 4  # Approximate 4 chars per token\n",
    "        actual_tokens = llm_utils.count_tokens(section_prompt, api_type)\n",
    "        logger.info(f\"Sending {actual_tokens} tokens to {api_type} API (limit: {max_input_token_limit})\")\n",
    "        \n",
    "        if estimated_tokens < max_input_token_limit:\n",
    "            try:\n",
    "                logger.info(f\"Attempting to generate tests for entire section: {section['name']}\")\n",
    "                response = llm_utils.make_llm_request(\n",
    "                    client, \n",
    "                    api_type, \n",
    "                    section_prompt, \n",
    "                    INFERNO_TEST_SYSTEM_PROMPT, \n",
    "                    rate_limit_func\n",
    "                )\n",
    "                \n",
    "                # Parse the response to extract individual tests\n",
    "                tests = {}\n",
    "                current_req = None\n",
    "                current_test = []\n",
    "                \n",
    "                for line in response.split('\\n'):\n",
    "                    # Look for markers indicating requirement boundaries\n",
    "                    req_marker = re.search(r'(REQ-\\d+)', line)\n",
    "                    if req_marker and (\"Test for\" in line or \"Implementation for\" in line or \"# Requirement\" in line):\n",
    "                        if current_req and current_test:\n",
    "                            tests[current_req] = '\\n'.join(current_test)\n",
    "                            current_test = []\n",
    "                        \n",
    "                        current_req = req_marker.group(1)\n",
    "                    \n",
    "                    if current_req:\n",
    "                        current_test.append(line)\n",
    "            \n",
    "                if current_req and current_test:\n",
    "                    tests[current_req] = '\\n'.join(current_test)\n",
    "                \n",
    "                # If successfully parsed tests for all requirements, return them\n",
    "                if len(tests) == len(section['requirements']):\n",
    "                    logger.info(f\"Successfully generated tests for all requirements in section: {section['name']}\")\n",
    "                    return tests\n",
    "                \n",
    "                logger.warning(f\"Generated tests for only {len(tests)} of {len(section['requirements'])} requirements in section: {section['name']}\")\n",
    "                # Continue to individual requirement processing\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to generate tests for entire section: {str(e)}\")\n",
    "                # Continue to individual requirement processing\n",
    "    \n",
    "    # If section-level generation failed or wasn't attempted, generate tests for individual requirements\n",
    "    tests = {}\n",
    "    for requirement in section['requirements']:\n",
    "        try:\n",
    "            logger.info(f\"Generating test for requirement: {requirement['id']}\")\n",
    "            \n",
    "            # Prepare the prompt for this requirement with full context\n",
    "            req_prompt = get_inferno_test_generation_prompt(\n",
    "                test_specification=requirement['full_content'],\n",
    "                requirement_id=requirement['id'],\n",
    "                module_name=module_name,\n",
    "                dsl_guidance=dsl_guidance\n",
    "            )\n",
    "            actual_tokens = llm_utils.count_tokens(req_prompt, api_type)\n",
    "            logger.info(f\"Requirement {requirement['id']}: Sending {actual_tokens} tokens to {api_type} API (limit: {max_input_token_limit})\")\n",
    "        \n",
    "            # Generate the test\n",
    "            test_code = llm_utils.make_llm_request(\n",
    "                client, \n",
    "                api_type, \n",
    "                req_prompt, \n",
    "                INFERNO_TEST_SYSTEM_PROMPT, \n",
    "                rate_limit_func\n",
    "            )\n",
    "            \n",
    "            # Clean up any markdown formatting\n",
    "            if test_code.startswith('```ruby'):\n",
    "                test_code = re.sub(r'^```ruby\\n', '', test_code)\n",
    "                test_code = re.sub(r'\\n```$', '', test_code)\n",
    "            elif test_code.startswith('```'):\n",
    "                test_code = re.sub(r'^```\\n', '', test_code)\n",
    "                test_code = re.sub(r'\\n```$', '', test_code)\n",
    "            \n",
    "            tests[requirement['id']] = test_code\n",
    "            logger.info(f\"Successfully generated test for requirement: {requirement['id']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating test for requirement {requirement['id']}: {str(e)}\")\n",
    "    \n",
    "    validated_tests = {}\n",
    "    for req_id, test_code in tests.items():\n",
    "        try:\n",
    "            logger.info(f\"Validating test for requirement: {req_id}\")\n",
    "            validated_code = validate_test_with_llm(\n",
    "                client, \n",
    "                api_type, \n",
    "                test_code, \n",
    "                dsl_guidance, \n",
    "                rate_limit_func\n",
    "            )\n",
    "            validated_tests[req_id] = validated_code\n",
    "            logger.info(f\"Successfully validated test for requirement: {req_id}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error validating test for {req_id}: {str(e)}\")\n",
    "            # Fall back to original if validation fails\n",
    "            validated_tests[req_id] = test_code\n",
    "    \n",
    "    return validated_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_structure(grouped_tests: Dict[str, Dict[str, List[Dict[str, Any]]]], module_name: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Create the file structure for the Inferno tests\n",
    "    \n",
    "    Args:\n",
    "        grouped_tests: Nested dictionary of grouped test specs\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping test specs to file paths\n",
    "    \"\"\"\n",
    "    # Create base directories\n",
    "    module_dir = os.path.join(output_dir, module_name.lower())\n",
    "    os.makedirs(module_dir, exist_ok=True)\n",
    "    \n",
    "    # Create test group directories\n",
    "    test_file_map = {}\n",
    "    \n",
    "    for actor, resource_groups in grouped_tests.items():\n",
    "        actor_dir = os.path.join(module_dir, actor.lower())\n",
    "        os.makedirs(actor_dir, exist_ok=True)\n",
    "        \n",
    "        for resource, test_specs in resource_groups.items():\n",
    "            resource_dir = os.path.join(actor_dir, resource.lower().replace('-', '_').replace(' ', '_'))\n",
    "            os.makedirs(resource_dir, exist_ok=True)\n",
    "            \n",
    "            for spec in test_specs:\n",
    "                # Create a file name based on the requirement ID\n",
    "                req_id = spec['id'].lower().replace('-', '_')\n",
    "                file_name = f\"{req_id}_test.rb\"\n",
    "                file_path = os.path.join(resource_dir, file_name)\n",
    "                \n",
    "                test_file_map[spec['id']] = file_path\n",
    "    \n",
    "    return test_file_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_id_from_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the actual ID defined in a test file using the 'id :xyz' pattern.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Ruby test file\n",
    "        \n",
    "    Returns:\n",
    "        The extracted ID or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Look for 'id :some_id' or 'id:some_id' pattern\n",
    "            id_match = re.search(r'id\\s*:([a-zA-Z0-9_]+)', content)\n",
    "            if id_match:\n",
    "                return id_match.group(1)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting ID from {file_path}: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_test_data(module_dir: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scan all test files in the module directory and extract their actual IDs and metadata.\n",
    "    \n",
    "    Args:\n",
    "        module_dir: Path to the module directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping file paths to metadata about the test files\n",
    "    \"\"\"\n",
    "    test_data = {}\n",
    "    \n",
    "    # Walk through all directories in the module\n",
    "    for root, dirs, files in os.walk(module_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('_test.rb'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, module_dir)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        content = f.read()\n",
    "                        \n",
    "                        # Extract class type (TestGroup or Test)\n",
    "                        class_type_match = re.search(r'class\\s+\\w+\\s+<\\s+Inferno::(\\w+)', content)\n",
    "                        class_type = class_type_match.group(1) if class_type_match else 'Unknown'\n",
    "                        \n",
    "                        # Extract ID\n",
    "                        id_match = re.search(r'id\\s*:([a-zA-Z0-9_]+)', content)\n",
    "                        actual_id = id_match.group(1) if id_match else None\n",
    "                        \n",
    "                        # Extract title\n",
    "                        title_match = re.search(r'title\\s+[\\'\"]([^\\'\"]+)[\\'\"]', content)\n",
    "                        title = title_match.group(1) if title_match else 'Unknown Test'\n",
    "                        \n",
    "                        # Store the data\n",
    "                        test_data[relative_path] = {\n",
    "                            'id': actual_id,\n",
    "                            'type': class_type,\n",
    "                            'title': title,\n",
    "                            'file_path': relative_path\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error processing file {file_path}: {str(e)}\")\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "\n",
    "def create_module_structure(test_data: Dict[str, Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Create a structured representation of the module for the LLM to use.\n",
    "    \n",
    "    Args:\n",
    "        test_data: Dictionary of test file data\n",
    "        \n",
    "    Returns:\n",
    "        String describing the module structure\n",
    "    \"\"\"\n",
    "    structure = \"Module Structure:\\n\"\n",
    "    \n",
    "    # Group by directories\n",
    "    dir_structure = defaultdict(list)\n",
    "    for file_path, data in test_data.items():\n",
    "        dir_name = os.path.dirname(file_path)\n",
    "        dir_structure[dir_name].append((file_path, data))\n",
    "    \n",
    "    # Create a hierarchical description\n",
    "    for dir_name, files in sorted(dir_structure.items()):\n",
    "        structure += f\"- Directory: {dir_name}\\n\"\n",
    "        for file_path, data in sorted(files):\n",
    "            structure += f\"  - File: {os.path.basename(file_path)}\\n\"\n",
    "            structure += f\"    - Type: {data['type']}\\n\"\n",
    "            structure += f\"    - ID: {data['id'] or 'None'}\\n\"\n",
    "            structure += f\"    - Title: {data['title']}\\n\"\n",
    "    \n",
    "    return structure\n",
    "\n",
    "\n",
    "def generate_landing_file_with_llm(\n",
    "    client,\n",
    "    api_type: str,\n",
    "    module_name_camel: str,\n",
    "    module_name_snake: str,\n",
    "    test_data: Dict[str, Dict[str, Any]],\n",
    "    structure: str,\n",
    "    rate_limit_func\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use an LLM to generate the main module file with proper ID references.\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        module_name_camel: CamelCase module name\n",
    "        module_name_snake: snake_case module name\n",
    "        test_data: Dictionary of test file data\n",
    "        structure: String describing the module structure\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Generated module file content\n",
    "    \"\"\"\n",
    "    # Define a system prompt for the LLM\n",
    "    system_prompt = \"\"\"You are an expert Ruby developer specializing in FHIR Inferno test development.\n",
    "Your task is to create a main module file for an Inferno test suite that correctly references all test groups.\"\"\"\n",
    "    \n",
    "    # Create the user prompt\n",
    "    user_prompt = f\"\"\"\n",
    "Generate a main module file for an Inferno test suite called '{module_name_camel}' with a snake_case name of '{module_name_snake}'.\n",
    "\n",
    "The test suite should have:\n",
    "1. A proper module definition with a TestSuite class\n",
    "2. Configuration for a FHIR client with a URL input\n",
    "3. All required imports for the test files\n",
    "4. A properly structured hierarchy of groups that reflects the directory structure\n",
    "5. Correct references to test groups using their actual IDs, not their filenames\n",
    "\n",
    "Here is the structure of the test files:\n",
    "{structure}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Each 'require_relative' statement should reference the actual file path\n",
    "2. When using 'group from:', ALWAYS use the actual ID from the file (what follows 'id :'), NOT the filename\n",
    "3. Organize the groups to match the directory structure\n",
    "4. Configure the FHIR client at the TestSuite level\n",
    "5. Do not require 'inferno/dsl/test_suite' or other non-existent libraries\n",
    "6. The main module file should be named '{module_name_snake}.rb'\n",
    "\n",
    "Return ONLY the Ruby code for the main module file, with no explanations.\n",
    "\"\"\"\n",
    "    \n",
    "    # Call the LLM\n",
    "    landing_file_content = llm_utils.make_llm_request(\n",
    "        client,\n",
    "        api_type,\n",
    "        user_prompt,\n",
    "        system_prompt,\n",
    "        rate_limit_func\n",
    "    )\n",
    "    \n",
    "    # Clean up any markdown formatting\n",
    "    if landing_file_content.startswith('```ruby'):\n",
    "        landing_file_content = re.sub(r'^```ruby\\n', '', landing_file_content)\n",
    "        landing_file_content = re.sub(r'\\n```$', '', landing_file_content)\n",
    "    elif landing_file_content.startswith('```'):\n",
    "        landing_file_content = re.sub(r'^```\\n', '', landing_file_content)\n",
    "        landing_file_content = re.sub(r'\\n```$', '', landing_file_content)\n",
    "    \n",
    "    return landing_file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inferno_test_kit(\n",
    "    api_type: str,\n",
    "    test_plan_file: str,\n",
    "    guidance_file: str = None,\n",
    "    module_name: str = \"PlanNet\",\n",
    "    output_dir: str = OUTPUT_DIR,\n",
    "    expected_actors: List[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a test plan and generate an Inferno test kit\n",
    "    \n",
    "    Args:\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        test_plan_file: Path to test plan markdown file\n",
    "        guidance_file: Path to Inferno guidance file (optional)\n",
    "        module_name: Name of the module for the tests\n",
    "        output_dir: Directory for output files\n",
    "        expected_actors: List of expected actors in the test plan\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing statistics and paths\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting Inferno test generation with {api_type} for {module_name}\")\n",
    "    \n",
    "    # Initialize API clients and rate limiters\n",
    "    clients = llm_utils.setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = llm_utils.API_CONFIGS[api_type]\n",
    "    rate_limiter = llm_utils.create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        llm_utils.check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Standardize module name for Ruby\n",
    "    module_name_snake = module_name.lower().replace('-', '_').replace(' ', '_')\n",
    "    module_name_camel = ''.join(word.capitalize() for word in module_name.split())\n",
    "    \n",
    "    # Create module subdirectory\n",
    "    module_subdir = os.path.join(output_dir, module_name_snake)\n",
    "    os.makedirs(module_subdir, exist_ok=True)\n",
    "    \n",
    "    # Set default actors if none provided\n",
    "    if not expected_actors:\n",
    "        expected_actors = [\"Health Plan API Actor\", \"Application Actor\"]\n",
    "    \n",
    "    try:\n",
    "        # Parse test plan into sections\n",
    "        sections = parse_test_plan(test_plan_file)\n",
    "        logger.info(f\"Parsed test plan into {len(sections)} sections\")\n",
    "        \n",
    "        # Count total requirements\n",
    "        total_requirements = sum(len(section['requirements']) for section in sections.values())\n",
    "        logger.info(f\"Found {total_requirements} total requirements\")\n",
    "        \n",
    "        # Get Inferno guidance\n",
    "        if guidance_file and os.path.exists(guidance_file):\n",
    "            with open(guidance_file, 'r') as f:\n",
    "                dsl_guidance = f.read()\n",
    "            logger.info(f\"Loaded Inferno guidance from {guidance_file}\")\n",
    "        else:\n",
    "            dsl_guidance = get_inferno_guidance()\n",
    "            logger.info(\"Using default Inferno guidance\")\n",
    "            guidance_tokens = llm_utils.count_tokens(dsl_guidance, api_type)\n",
    "            logger.info(f\"Inferno guidance size: {guidance_tokens} tokens\")\n",
    "        \n",
    "        # Generate tests by section\n",
    "        all_tests = {}\n",
    "        for section_name, section in sections.items():\n",
    "            logger.info(f\"Processing section: {section_name} with {len(section['requirements'])} requirements\")\n",
    "            \n",
    "            # Skip empty sections\n",
    "            if not section['requirements']:\n",
    "                continue\n",
    "                \n",
    "            # Generate tests for this section\n",
    "            section_tests = generate_tests_for_section(\n",
    "                client, \n",
    "                api_type, \n",
    "                section, \n",
    "                dsl_guidance, \n",
    "                module_name_camel, \n",
    "                check_limits,\n",
    "                70000\n",
    "            )\n",
    "            \n",
    "            # Add to our collection\n",
    "            all_tests.update(section_tests)\n",
    "            \n",
    "            # Add delay between sections\n",
    "            time.sleep(config[\"delay_between_batches\"])\n",
    "            \n",
    "        logger.info(f\"Generated tests for {len(all_tests)} requirements\")\n",
    "        \n",
    "        # Group requirements by actor and section (original groups from test plan)\n",
    "        grouped_reqs = determine_test_groups(sections, expected_actors)\n",
    "        \n",
    "        # Write test files to the module subdirectory\n",
    "        file_paths = {}\n",
    "        for actor, section_groups in grouped_reqs.items():\n",
    "            # Skip actors with no tests\n",
    "            if not section_groups:\n",
    "                continue\n",
    "                \n",
    "            actor_safe = re.sub(r'[^a-zA-Z0-9_]', '_', actor.lower())\n",
    "            actor_dir = os.path.join(module_subdir, actor_safe)\n",
    "            \n",
    "            # Only create actor directory if it has at least one test\n",
    "            has_tests = False\n",
    "            for section_name, reqs in section_groups.items():\n",
    "                for req in reqs:\n",
    "                    if req['id'] in all_tests:\n",
    "                        has_tests = True\n",
    "                        break\n",
    "                if has_tests:\n",
    "                    break\n",
    "            \n",
    "            if not has_tests:\n",
    "                continue\n",
    "                \n",
    "            os.makedirs(actor_dir, exist_ok=True)\n",
    "            \n",
    "            for section_name, reqs in section_groups.items():\n",
    "                # Skip empty sections or sections with no generated tests\n",
    "                has_section_tests = False\n",
    "                for req in reqs:\n",
    "                    if req['id'] in all_tests:\n",
    "                        has_section_tests = True\n",
    "                        break\n",
    "                \n",
    "                if not has_section_tests:\n",
    "                    continue\n",
    "                    \n",
    "                section_safe = re.sub(r'[^a-zA-Z0-9_]', '_', section_name.lower())\n",
    "                section_dir = os.path.join(actor_dir, section_safe)\n",
    "                os.makedirs(section_dir, exist_ok=True)\n",
    "                \n",
    "                for req in reqs:\n",
    "                    # Skip requirements we couldn't generate tests for\n",
    "                    if req['id'] not in all_tests:\n",
    "                        continue\n",
    "                        \n",
    "                    file_name = f\"{req['id'].lower().replace('-', '_')}_test.rb\"\n",
    "                    file_path = os.path.join(section_dir, file_name)\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(all_tests[req['id']])\n",
    "                    \n",
    "                    file_paths[req['id']] = file_path\n",
    "                    logger.info(f\"Wrote test for {req['id']} to {file_path}\")\n",
    "        \n",
    "        # Extract data from all test files for LLM-based module file generation\n",
    "        logger.info(\"Collecting test data for module file generation\")\n",
    "        test_data = extract_test_data(module_subdir)\n",
    "        \n",
    "        # Create structured representation for the LLM\n",
    "        structure = create_module_structure(test_data)\n",
    "        logger.info(\"Created structure representation of test files\")\n",
    "        \n",
    "        # Generate the module file using an LLM\n",
    "        logger.info(\"Generating module file with LLM\")\n",
    "        module_content = generate_landing_file_with_llm(\n",
    "            client,\n",
    "            api_type,\n",
    "            module_name_camel,\n",
    "            module_name_snake,\n",
    "            test_data,\n",
    "            structure,\n",
    "            check_limits\n",
    "        )\n",
    "        \n",
    "        # Write the module file\n",
    "        module_file_path = os.path.join(output_dir, f\"{module_name_snake}.rb\")\n",
    "        with open(module_file_path, 'w') as f:\n",
    "            f.write(module_content)\n",
    "        logger.info(f\"Wrote module file to {module_file_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"total_sections\": len(sections),\n",
    "            \"total_requirements\": total_requirements,\n",
    "            \"generated_tests\": len(all_tests),\n",
    "            \"module_dir\": module_subdir,\n",
    "            \"module_file\": module_file_path\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Inferno tests: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inferno_test_generator():\n",
    "    \"\"\"\n",
    "    Run the Inferno test generator with user input\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with generation results, or None if an error occurred\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get input from user or set default values\n",
    "    print(\"\\nFHIR Inferno Test Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Let user select the API\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    # Get test plan file path\n",
    "    test_plan_file = input(\"\\nEnter path to test plan markdown file: \")\n",
    "    \n",
    "    # Check if test plan file exists\n",
    "    if not os.path.exists(test_plan_file):\n",
    "        logger.error(f\"Test plan file not found: {test_plan_file}\")\n",
    "        print(f\"Error: Test plan file not found at {test_plan_file}\")\n",
    "        return None\n",
    "    \n",
    "    # the path to the guidance file\n",
    "    guidance_file = os.path.join(CURRENT_DIR, \"dsl-guidance.md\")\n",
    "    \n",
    "    # Check if guidance file exists\n",
    "    if not os.path.exists(guidance_file):\n",
    "        logger.warning(f\"Inferno guidance file not found: {guidance_file}\")\n",
    "        print(f\"Warning: Inferno guidance file not found at {guidance_file}. Using built-in guidance.\")\n",
    "        guidance_file = None\n",
    "    else:\n",
    "        print(f\"Using guidance file: {guidance_file}\")\n",
    "    \n",
    "    # Get module name\n",
    "    module_name = input(\"\\nEnter module name (default 'PlanNet'): \") or \"PlanNet\"\n",
    "    \n",
    "    # Get actor information\n",
    "    print(\"\\nEnter the actors from the test plan (comma-separated, e.g., 'Health Plan API Actor, Application Actor'):\")\n",
    "    actors_input = input(\"Actors: \")\n",
    "    \n",
    "    if actors_input:\n",
    "        expected_actors = [actor.strip() for actor in actors_input.split(',')]\n",
    "    else:\n",
    "        # Default actors if none provided\n",
    "        expected_actors = [\"Health Plan API Actor\", \"Application Actor\"]\n",
    "    \n",
    "    # Get output directory\n",
    "    output_dir = input(f\"\\nEnter output directory (default '{OUTPUT_DIR}'): \") or OUTPUT_DIR\n",
    "    \n",
    "    print(f\"\\nGenerating Inferno tests with {api_type.capitalize()}...\")\n",
    "    print(f\"Using actors: {', '.join(expected_actors)}\")\n",
    "    if guidance_file:\n",
    "        print(f\"Using Inferno guidance from {guidance_file}\")\n",
    "    print(f\"This may take several minutes depending on the number of requirements.\")\n",
    "    \n",
    "    try:\n",
    "        # Process test plan and generate Inferno tests\n",
    "        result = generate_inferno_test_kit(\n",
    "            api_type=api_type,\n",
    "            test_plan_file=test_plan_file,\n",
    "            guidance_file=guidance_file,\n",
    "            module_name=module_name,\n",
    "            output_dir=output_dir,\n",
    "            expected_actors=expected_actors\n",
    "        )\n",
    "        \n",
    "        # Output results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Inferno test generation complete!\")\n",
    "        print(f\"Total sections: {result['total_sections']}\")\n",
    "        print(f\"Total requirements: {result['total_requirements']}\")\n",
    "        print(f\"Successfully generated tests: {result['generated_tests']}\")\n",
    "        print(f\"Module directory: {result['module_dir']}\")\n",
    "        print(f\"Main module file: {result['module_file']}\")\n",
    "        #print(f\"Generation report: {result['report_file']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FHIR Inferno Test Generator\n",
      "==================================================\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n",
      "Using guidance file: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/dsl-guidance.md\n",
      "\n",
      "Enter the actors from the test plan (comma-separated, e.g., 'Health Plan API Actor, Application Actor'):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 09:21:55,925 - __main__ - INFO - Starting Inferno test generation with gemini for test10_gemini\n",
      "2025-06-16 09:21:55,952 - __main__ - INFO - Parsed test plan into 4 sections\n",
      "2025-06-16 09:21:55,953 - __main__ - INFO - Found 5 total requirements\n",
      "2025-06-16 09:21:55,954 - __main__ - INFO - Loaded Inferno guidance from /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/dsl-guidance.md\n",
      "2025-06-16 09:21:55,954 - __main__ - INFO - Processing section: Plan-Net Network with 1 requirements\n",
      "2025-06-16 09:21:55,954 - __main__ - INFO - Generating tests for section: Plan-Net Network\n",
      "2025-06-16 09:21:55,954 - __main__ - INFO - Generating test for requirement: REQ-37\n",
      "2025-06-16 09:21:55,955 - __main__ - INFO - Requirement REQ-37: Sending 6964 tokens to gemini API (limit: 70000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Inferno tests with Gemini...\n",
      "Using actors: Health Plan API, Application Actor\n",
      "Using Inferno guidance from /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/dsl-guidance.md\n",
      "This may take several minutes depending on the number of requirements.\n",
      "Found 5 potential requirements\n",
      "Processing requirement: REQ-37\n",
      "Added requirement REQ-37 to section Plan-Net Network\n",
      "Processing requirement: REQ-34\n",
      "Added requirement REQ-34 to section Plan-Net Organization\n",
      "Processing requirement: REQ-38\n",
      "Added requirement REQ-38 to section Plan-Net Organization\n",
      "Processing requirement: REQ-36\n",
      "Added requirement REQ-36 to section Plan-Net OrganizationAffiliation\n",
      "Processing requirement: REQ-35\n",
      "Added requirement REQ-35 to section Plan-Net PractitionerRole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 09:22:18,267 - __main__ - INFO - Successfully generated test for requirement: REQ-37\n",
      "2025-06-16 09:22:18,269 - __main__ - INFO - Validating test for requirement: REQ-37\n",
      "2025-06-16 09:22:18,270 - __main__ - INFO - Validation for test: Sending 8143 tokens to gemini API\n",
      "2025-06-16 09:22:33,269 - __main__ - INFO - Successfully validated test for requirement: REQ-37\n",
      "2025-06-16 09:22:38,278 - __main__ - INFO - Processing section: Plan-Net Organization with 2 requirements\n",
      "2025-06-16 09:22:38,279 - __main__ - INFO - Generating tests for section: Plan-Net Organization\n",
      "2025-06-16 09:22:38,279 - __main__ - INFO - Sending 6030 tokens to gemini API (limit: 70000)\n",
      "2025-06-16 09:22:38,279 - __main__ - INFO - Attempting to generate tests for entire section: Plan-Net Organization\n",
      "2025-06-16 09:22:54,976 - __main__ - WARNING - Generated tests for only 0 of 2 requirements in section: Plan-Net Organization\n",
      "2025-06-16 09:22:54,978 - __main__ - INFO - Generating test for requirement: REQ-34\n",
      "2025-06-16 09:22:54,980 - __main__ - INFO - Requirement REQ-34: Sending 6906 tokens to gemini API (limit: 70000)\n",
      "2025-06-16 09:23:32,031 - __main__ - INFO - Successfully generated test for requirement: REQ-34\n",
      "2025-06-16 09:23:32,032 - __main__ - INFO - Generating test for requirement: REQ-38\n",
      "2025-06-16 09:23:32,032 - __main__ - INFO - Requirement REQ-38: Sending 6897 tokens to gemini API (limit: 70000)\n",
      "2025-06-16 09:23:50,334 - __main__ - INFO - Successfully generated test for requirement: REQ-38\n",
      "2025-06-16 09:23:50,335 - __main__ - INFO - Validating test for requirement: REQ-34\n",
      "2025-06-16 09:23:50,335 - __main__ - INFO - Validation for test: Sending 8056 tokens to gemini API\n",
      "2025-06-16 09:24:13,293 - __main__ - INFO - Successfully validated test for requirement: REQ-34\n",
      "2025-06-16 09:24:13,294 - __main__ - INFO - Validating test for requirement: REQ-38\n",
      "2025-06-16 09:24:13,295 - __main__ - INFO - Validation for test: Sending 8009 tokens to gemini API\n",
      "2025-06-16 09:24:36,961 - __main__ - INFO - Successfully validated test for requirement: REQ-38\n",
      "2025-06-16 09:24:41,966 - __main__ - INFO - Processing section: Plan-Net OrganizationAffiliation with 1 requirements\n",
      "2025-06-16 09:24:41,967 - __main__ - INFO - Generating tests for section: Plan-Net OrganizationAffiliation\n",
      "2025-06-16 09:24:41,967 - __main__ - INFO - Generating test for requirement: REQ-36\n",
      "2025-06-16 09:24:41,968 - __main__ - INFO - Requirement REQ-36: Sending 6910 tokens to gemini API (limit: 70000)\n",
      "2025-06-16 09:25:21,486 - __main__ - INFO - Successfully generated test for requirement: REQ-36\n",
      "2025-06-16 09:25:21,486 - __main__ - INFO - Validating test for requirement: REQ-36\n",
      "2025-06-16 09:25:21,487 - __main__ - INFO - Validation for test: Sending 8880 tokens to gemini API\n",
      "2025-06-16 09:25:48,136 - __main__ - INFO - Successfully validated test for requirement: REQ-36\n",
      "2025-06-16 09:25:53,139 - __main__ - INFO - Processing section: Plan-Net PractitionerRole with 1 requirements\n",
      "2025-06-16 09:25:53,141 - __main__ - INFO - Generating tests for section: Plan-Net PractitionerRole\n",
      "2025-06-16 09:25:53,141 - __main__ - INFO - Generating test for requirement: REQ-35\n",
      "2025-06-16 09:25:53,144 - __main__ - INFO - Requirement REQ-35: Sending 6904 tokens to gemini API (limit: 70000)\n",
      "2025-06-16 09:26:13,421 - __main__ - INFO - Successfully generated test for requirement: REQ-35\n",
      "2025-06-16 09:26:13,422 - __main__ - INFO - Validating test for requirement: REQ-35\n",
      "2025-06-16 09:26:13,422 - __main__ - INFO - Validation for test: Sending 8669 tokens to gemini API\n",
      "2025-06-16 09:26:35,956 - __main__ - INFO - Successfully validated test for requirement: REQ-35\n",
      "2025-06-16 09:26:40,963 - __main__ - INFO - Generated tests for 5 requirements\n",
      "2025-06-16 09:26:40,968 - __main__ - INFO - Wrote test for REQ-37 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini/health_plan_api/plan_net_network/req_37_test.rb\n",
      "2025-06-16 09:26:40,970 - __main__ - INFO - Wrote test for REQ-34 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini/health_plan_api/plan_net_organization/req_34_test.rb\n",
      "2025-06-16 09:26:40,971 - __main__ - INFO - Wrote test for REQ-38 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini/health_plan_api/plan_net_organization/req_38_test.rb\n",
      "2025-06-16 09:26:40,973 - __main__ - INFO - Wrote test for REQ-36 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini/health_plan_api/plan_net_organizationaffiliation/req_36_test.rb\n",
      "2025-06-16 09:26:40,975 - __main__ - INFO - Wrote test for REQ-35 to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini/health_plan_api/plan_net_practitionerrole/req_35_test.rb\n",
      "2025-06-16 09:26:40,975 - __main__ - INFO - Collecting test data for module file generation\n",
      "2025-06-16 09:26:40,982 - __main__ - INFO - Created structure representation of test files\n",
      "2025-06-16 09:26:40,982 - __main__ - INFO - Generating module file with LLM\n",
      "2025-06-16 09:26:47,901 - __main__ - INFO - Wrote module file to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini.rb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Inferno test generation complete!\n",
      "Total sections: 4\n",
      "Total requirements: 5\n",
      "Successfully generated tests: 5\n",
      "Module directory: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini\n",
      "Main module file: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini.rb\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_sections': 4,\n",
       " 'total_requirements': 5,\n",
       " 'generated_tests': 5,\n",
       " 'module_dir': '/Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini',\n",
       " 'module_file': '/Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_output/test10_gemini.rb'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the test generator\n",
    "run_inferno_test_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
