{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e3a524",
   "metadata": {},
   "source": [
    "## FHIR Implementation Guide Testing Pipeline\n",
    "This notebook provides a comprehensive pipeline for automatically extracting requirements from FHIR Implementation Guides and generating executable test suites. The pipeline transforms Implementation Guide (IG) documentation into structured test code that can validate FHIR server implementations.\n",
    "\n",
    "#### Overview\n",
    "This automated pipeline takes FHIR Implementation Guide documentation and produces comprehensive test suites through several integrated stages:\n",
    "\n",
    "- Implementation Guide Preparation: Convert and clean IG HTML documentation to markdown format\n",
    "- Requirements Extraction: Use AI to identify and extract testable requirements from the IG\n",
    "- Requirements Refinement: Consolidate and refine the extracted requirements\n",
    "- Requirements Downselection: Combine multiple requirement sets and remove duplicates\n",
    "- Test Plan Generation: Convert requirements into detailed test specifications\n",
    "- Test Kit Generation: Generate executable Inferno test code\n",
    "\n",
    "#### Running this Notebook\n",
    "The notebook is structured to run each stage sequentially. You can either:\n",
    "\n",
    "- Run the complete pipeline: Execute all cells to process a complete IG\n",
    "- Run individual stages: Execute specific sections as needed\n",
    "\n",
    "Inputs and output directories can be customized for each step. The pipeline automatically saves intermediate outputs in checkpoint directories for review and iteration.\n",
    "\n",
    "#### Output Structure\n",
    "The pipeline generates organized outputs in checkpoint directories:\n",
    "\n",
    "checkpoints/\n",
    "\n",
    "├── markdown1/          # Converted markdown files\n",
    "\n",
    "├── markdown2/          # Cleaned markdown files  \n",
    "\n",
    "├── requirements_extraction/   # Initial AI-extracted requirements\n",
    "\n",
    "├── revised_reqs_extraction/  # Refined requirements lists\n",
    "\n",
    "├── requirements_downselect/  # Final consolidated requirements\n",
    "\n",
    "├── testplan_generation/     # Detailed test specifications\n",
    "\n",
    "└── testkit_generation/      # Executable Inferno test suites\n",
    "\n",
    "Each stage preserves its outputs, allowing for iteration, review, and alternative processing paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d267b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de19ee6",
   "metadata": {},
   "source": [
    "#### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "import llm_utils2\n",
    "import importlib\n",
    "import tiktoken\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7a6cc",
   "metadata": {},
   "source": [
    "## Initializing LLM Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(llm_utils2)\n",
    "llm_clients = llm_utils2.LLMApiClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39d130b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claude': <anthropic.Anthropic at 0x10e84af60>,\n",
       " 'gemini': genai.GenerativeModel(\n",
       "     model_name='models/gemini-1.5-pro',\n",
       "     generation_config={'max_output_tokens': 8192, 'temperature': 0.3},\n",
       "     safety_settings={<HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>},\n",
       "     tools=None,\n",
       "     system_instruction=None,\n",
       "     cached_content=None\n",
       " ),\n",
       " 'gpt': <openai.OpenAI at 0x10dc23bc0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_clients.clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d80d1",
   "metadata": {},
   "source": [
    "## Implementation Guide Preparation\n",
    "\n",
    "### Stage 1: Text Extraction and Cleaning\n",
    "- Converts HTML IG files to markdown format\n",
    "- Cleans unnecessary content (navigation, headers, formatting artifacts)\n",
    "- Prepares clean, structured text for AI processing\n",
    "\n",
    "Inputs: HTML files from FHIR IG downloads\n",
    "\n",
    "Outputs: Clean markdown files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08717e1c",
   "metadata": {},
   "source": [
    "#### 1a) HTML to Markdown Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61ff8f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 HTML files to process\n",
      "Processed 1/1 files\n",
      "Conversion complete. Successfully processed 1 files. Encountered 0 errors.\n"
     ]
    }
   ],
   "source": [
    "import html_narrative_extractor_01 #import html extractor module\n",
    "\n",
    "# Process directory with default settings\n",
    "result = html_narrative_extractor_01.convert_local_html_to_markdown(\n",
    "    input_dir=\"../us-core/test_set\", #input directory of downloaded IG HTML files\n",
    "    output_dir=\"checkpoints/demo/markdown1/\" #output directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0108ece",
   "metadata": {},
   "source": [
    "#### 1b) Markdown Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aec7806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 markdown files in checkpoints/demo/markdown1\n",
      "Cleaned and saved: checkpoints/demo/markdown2/CapabilityStatement-us-core-server.md\n",
      "\n",
      "Processing complete: 1 files successfully cleaned, 0 failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_files': 1, 'successful': 1, 'failed': 0, 'failed_files': []}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import markdown_cleaner_02 #import markdown cleaner module\n",
    "markdown_cleaner_02.process_directory(\n",
    "    input_dir=\"checkpoints/demo/markdown1\", #input directory of IG markdown files\n",
    "    output_dir=\"checkpoints/demo/markdown2/\") #output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3fa2d1",
   "metadata": {},
   "source": [
    "## Stage 2: Requirements Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0566ab",
   "metadata": {},
   "source": [
    "### 2a) Prompt-based Requirement Extraction\n",
    "LLM Requirements Identification\n",
    "- Processes markdown files using LLM to extract clear, testable requirements\n",
    "- Formats requirements according to set standards, following INCOSE guidance\n",
    "- Generates structured requirements with IDs, descriptions, actors, and conformance levels\n",
    "- Handles large documents through chunking\n",
    "- Provides source tracking\n",
    "\n",
    "Inputs: Cleaned IG markdown files\n",
    "\n",
    "Outputs: Structured requirements list as markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fd5bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'reqs_extraction_03' from '/Users/ceadams/Documents/onclaive/onclaive/pipeline/reqs_extraction_03.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import reqs_extraction_03 #import LLM requirements extraction module\n",
    "importlib.reload(reqs_extraction_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31999d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Implementation Guide with Claude...\n",
      "This may take several minutes depending on the size of the Implementation Guide.\n",
      "\n",
      "[1/1] Processing single file: ConditionProfile-CapabilityStatement-us-core-server.md\n",
      "    Completed 1 chunks                    \n",
      "\n",
      "================================================================================\n",
      "Processing complete!\n",
      "Generated requirements document: checkpoints/demo/requirements_extraction/reqs_list_v1.md\n",
      "Processed 1 files\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "reqs_extraction_03.run_requirements_extractor(\n",
    "    markdown_dir='checkpoints/demo/condition_profile', #input directory of markdown files\n",
    "    output_dir='checkpoints/demo/requirements_extraction/', #output directory\n",
    "    api_type= 'claude', #set API type\n",
    "    client_instance=llm_clients) #initialize llm clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47056998",
   "metadata": {},
   "source": [
    "### 2b) Requirements Refinement\n",
    "LLM-Based Requirements Review & Consolidation\n",
    "- Filters and identifies only testable requirements from raw extractions\n",
    "- Consolidates duplicate requirements and merges related ones\n",
    "- Applies consistent formatting and structure\n",
    "- Removes non-testable assertions and narrative content\n",
    "\n",
    "Inputs: Raw requirements from extraction stage in markdown format\n",
    "\n",
    "Outputs: Refined requirements list in markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547fa2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'reqs_reviewer_04' from '/Users/ceadams/Documents/onclaive/onclaive/pipeline/reqs_reviewer_04.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requirements refinement script as module\n",
    "import reqs_reviewer_04\n",
    "importlib.reload(reqs_reviewer_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53363a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING BATCH PROCESSING\n",
      "==================================================\n",
      "Input: checkpoints/demo/requirements_extraction/reqs_list_v1.md\n",
      "Output: checkpoints/demo/requirements_revision/\n",
      "Batch size: 25 requirements\n",
      "API: claude\n",
      "\n",
      "File size: 12,817 characters\n",
      "Splitting requirements...\n",
      "Found 30 total requirements\n",
      "Will process in 2 batches\n",
      "\n",
      "BATCH 1/2\n",
      "   Requirements: 25 (#1-#25)\n",
      "   Size: 10,648 chars (~2,662 tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in claude API request: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}, 'request_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Completed in 208.8s\n",
      "   Pausing 2s...\n",
      "   Progress: 1/2 (50.0%)\n",
      "   ETA: 3.5 minutes remaining\n",
      "\n",
      "BATCH 2/2\n",
      "   Requirements: 5 (#26-#30)\n",
      "   Size: 2,167 chars (~541 tokens)\n",
      "   Completed in 7.0s\n",
      "   Progress: 2/2 (100.0%)\n",
      "   ETA: 0.0 minutes remaining\n",
      "\n",
      "COMBINING RESULTS\n",
      "--------------------\n",
      "Merging batch results and renumbering...\n",
      "   Processing batch 1 results...\n",
      "   Processing batch 2 results...\n",
      "   Renumbered 30 requirements\n",
      "BATCH PROCESSING COMPLETE!\n",
      "========================================\n",
      "Output saved: checkpoints/demo/requirements_revision/claude_refined_requirements_20250901_212912.md\n",
      "Original requirements: 30\n",
      "Final requirements: 30\n",
      "Successful batches: 2/2\n",
      "Failed batches: 0/2\n",
      "Total time: 3.6 minutes\n",
      "Average per batch: 108.9 seconds\n"
     ]
    }
   ],
   "source": [
    "result = reqs_reviewer_04.run_batch_requirements_refinement(\n",
    "    input_file=\"checkpoints/demo/requirements_extraction/reqs_list_v1.md\", #input requirements list markdown file\n",
    "    output_dir=\"checkpoints/demo/requirements_revision/\", #output directory   \n",
    "    client_instance=llm_clients,  #initialize llm clients\n",
    "    batch_size=25,  #set batch size\n",
    "    api_type=\"claude\"  #set API type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ccc40",
   "metadata": {},
   "source": [
    "### 2c) Requirements Downselection\n",
    "- Combines multiple requirements lists from different extraction runs\n",
    "- Uses semantic similarity analysis to identify and remove duplicates\n",
    "- Creates a deduplicated final requirements set\n",
    "\n",
    "Inputs: Multiple refined requirements files in markdown or JSON format\n",
    "Outputs: Final consolidated requirements in markdown or JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312c3c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'reqs_downselect_05' from '/Users/ceadams/Documents/onclaive/onclaive/pipeline/reqs_downselect_05.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import reqs_downselect_05\n",
    "importlib.reload(reqs_downselect_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a4bb6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files matching '*.md' in checkpoints/demo/requirements_revision/\n",
      "============================================================\n",
      "REQUIREMENT DEDUPLICATION PIPELINE\n",
      "============================================================\n",
      "Markdown files: 4\n",
      "RAG files: 0\n",
      "Similarity threshold: 0.98\n",
      "Output format: markdown\n",
      "\n",
      "Loading requirements from files...\n",
      "  Processing: checkpoints/demo/requirements_revision/10claude_refined_requirements_20250829_102851.md\n",
      "    Loaded 10 requirements\n",
      "  Processing: checkpoints/demo/requirements_revision/claude_refined_requirements_20250829_102328.md\n",
      "    Loaded 35 requirements\n",
      "  Processing: checkpoints/demo/requirements_revision/claude_refined_requirements_20250829_102851.md\n",
      "    Loaded 45 requirements\n",
      "  Processing: checkpoints/demo/requirements_revision/claude_refined_requirements_20250901_212912.md\n",
      "    Loaded 30 requirements\n",
      "\n",
      "Total requirements loaded: 120\n",
      "Loading sentence transformer model...\n",
      "Generating embeddings for 120 requirements...\n",
      "  Completed embeddings for 120 requirements\n",
      "Calculating similarity scores...\n",
      "  Completed 14400 similarity calculations\n",
      "Finding duplicate groups with threshold >= 0.98\n",
      "  Found 158 requirement pairs above threshold\n",
      "  Identified 32 duplicate groups\n",
      "  Will remove 50 duplicate requirements\n",
      "\n",
      "Deduplication Results:\n",
      "  Original requirements: 120\n",
      "  Duplicates removed: 50\n",
      "  Final requirements: 70\n",
      "Writing 70 requirements to checkpoints/demo/requirements_downselect/consolidated_reqs.md\n",
      "\n",
      "Output saved in Markdown format:\n",
      "  checkpoints/demo/requirements_downselect/consolidated_reqs.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_files': {'markdown': ['checkpoints/demo/requirements_revision/10claude_refined_requirements_20250829_102851.md',\n",
       "   'checkpoints/demo/requirements_revision/claude_refined_requirements_20250829_102328.md',\n",
       "   'checkpoints/demo/requirements_revision/claude_refined_requirements_20250829_102851.md',\n",
       "   'checkpoints/demo/requirements_revision/claude_refined_requirements_20250901_212912.md']},\n",
       " 'original_count': 120,\n",
       " 'duplicates_removed': 50,\n",
       " 'final_count': 70,\n",
       " 'threshold': 0.98,\n",
       " 'output_format': 'markdown',\n",
       " 'output_files': ['checkpoints/demo/requirements_downselect/consolidated_reqs.md'],\n",
       " 'output_dir': 'checkpoints/demo/requirements_downselect'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_files_list=reqs_downselect_05.get_md_files_from_directory(\"checkpoints/demo/requirements_revision/\")\n",
    "\n",
    "reqs_downselect_05.full_pass(\n",
    "    md_files=md_files_list,\n",
    "    output_dir=\"checkpoints/demo/requirements_downselect\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a69ed",
   "metadata": {},
   "source": [
    "## Stage 3: Test Plan Generation\n",
    "- Transforms requirements into detailed test specifications\n",
    "- Analyzes IG capability statements for context\n",
    "- Generates implementation strategies with specific FHIR operations\n",
    "- Creates structured test plans with validation criteria\n",
    "\n",
    "Inputs: Refined requirements and IG capability statements in markdown format\n",
    "\n",
    "Outputs: Detailed test plan in markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2179dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "llm_clients.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1f5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing collection found: capability_statements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off send_request(...) for 0.2s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n",
      "INFO:backoff:Backing off send_request(...) for 1.7s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n",
      "INFO:backoff:Backing off send_request(...) for 0.6s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n",
      "ERROR:backoff:Giving up send_request(...) after 4 tries (requests.exceptions.SSLError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Set logging level to reduce noise\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"backoff\").setLevel(logging.ERROR)\n",
    "\n",
    "import test_plan_06 #import test plan generation script as module\n",
    "importlib.reload(test_plan_06)\n",
    "\n",
    "#clearing any existing capability statements from vector database\n",
    "test_plan_06.clear_capability_collection(\"capability_statements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a668d468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FHIR TEST PLAN GENERATION\n",
      "================================================================================\n",
      "Implementation Guide: US Core IG\n",
      "Requirements file: checkpoints/demo/requirements_downselect/consolidated_reqs.md\n",
      "Capability file: checkpoints/markdown2/CapabilityStatement-us-core-server.md\n",
      "API: claude\n",
      "Output directory: checkpoints/demo/testplan_generation\n",
      "\n",
      "Loading requirements...\n",
      "Loaded 11 requirements\n",
      "Setting up capability knowledge base...\n",
      "Capability knowledge base ready\n",
      "\n",
      "Grouping requirements...\n",
      "  Analyzing requirement 1/11: C\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off send_request(...) for 0.1s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n",
      "INFO:backoff:Backing off send_request(...) for 0.6s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n",
      "INFO:backoff:Backing off send_request(...) for 0.1s (requests.exceptions.SSLError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n",
      "ERROR:backoff:Giving up send_request(...) after 4 tries (requests.exceptions.SSLError: HTTPSConnectionPool(host='us.i.posthog.com', port=443): Max retries exceeded with url: /batch/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed grouping 11 requirements                    \n",
      "\n",
      "Requirements organized into 2 groups:\n",
      "  • I cannot analyze this requirement because all the fields (Summary, Text, Context, Verification, Actor, Conformance, Conditional, and Source) are empty. Without any content describing what the requirement actually specifies, it's impossible to determine which resource profile or category it belongs to.: 1 requirements\n",
      "  • Condition: 10 requirements\n",
      "\n",
      "Generating test specifications...\n",
      "\n",
      "[Condition] Processing 10 requirements...\n",
      "  [1/11] REQ-001\n",
      "Processing REQ-001: US Core server should support vread and history-instance interactions for Condition\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-001\n",
      "================================================================================\n",
      "Query: US Core server should support vread and history-instance interactions for Condition \"SHOULD support `vread`, `history-instance`.\" Defining recommended interaction capabilities for Condition resources in US Core Server CapabilityStatement US Core Server FHIR SHOULD\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 0.8615819215774536):\n",
      "  Length: 1452 chars\n",
      "  Preview: FHIR Major Section: ## 14.3 CapabilityStatement: US Core Server CapabilityStatement\n",
      "\n",
      "|  |  |  |  |  ...\n",
      "  ...pi.json) | [Download](us-core-server.openapi.json)\n",
      "\n",
      "  Match 2 (distance: 0.9163459539413452):\n",
      "  Length: 83 chars\n",
      "  Preview: FHIR Document Title: # 14.3 CapabilityStatement: US Core Server CapabilityStatement...\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-001\n",
      "  [2/11] REQ-002\n",
      "Processing REQ-002: US Core server shall be capable of returning Condition resource using GET by ID\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-002\n",
      "================================================================================\n",
      "Query: US Core server shall be capable of returning Condition resource using GET by ID \"A Server SHALL be capable of returning a Condition resource using: `GET [base]/Condition/[id]`\" Defining fetch capability requirements for individual Condition resources US Core Server FHIR SHALL\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 0.8396906852722168):\n",
      "  Length: 1452 chars\n",
      "  Preview: FHIR Major Section: ## 14.3 CapabilityStatement: US Core Server CapabilityStatement\n",
      "\n",
      "|  |  |  |  |  ...\n",
      "  ...pi.json) | [Download](us-core-server.openapi.json)\n",
      "\n",
      "  Match 2 (distance: 0.8701598048210144):\n",
      "  Length: 2229 chars\n",
      "  Preview: FHIR Resource/Component: ### 14.3.2 FHIR RESTful Capabilities\n",
      "\n",
      "The US Core Server **SHALL**:\n",
      "\n",
      "1. Sup...\n",
      "  ... **MAY** support the `history-system` interaction.\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-002\n",
      "  [3/11] REQ-003\n",
      "Processing REQ-003: US Core server shall support Provenance revinclude for Condition searches\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-003\n",
      "================================================================================\n",
      "Query: US Core server shall support Provenance revinclude for Condition searches \"A Server SHALL be capable of supporting the following _revincludes: Provenance:target - `GET [base]/Condition?[parameter=value]&_revinclude=Provenance:target`\" Defining required reverse include capability for retrieving Provenance resources that reference Condition resources US Core Server FHIR SHALL\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 0.8514004945755005):\n",
      "  Length: 90628 chars\n",
      "  Preview: FHIR Resource Detail: #### 14.3.3.2 AllergyIntolerance\n",
      "\n",
      "Conformance Expectation: **SHALL**\n",
      "\n",
      "Supporte...\n",
      "  ...d read/write formats for notes on the server.\n",
      "\n",
      "---\n",
      "\n",
      "  Match 2 (distance: 0.9239374399185181):\n",
      "  Length: 2229 chars\n",
      "  Preview: FHIR Resource/Component: ### 14.3.2 FHIR RESTful Capabilities\n",
      "\n",
      "The US Core Server **SHALL**:\n",
      "\n",
      "1. Sup...\n",
      "  ... **MAY** support the `history-system` interaction.\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-003\n",
      "  [4/11] REQ-004\n",
      "Processing REQ-004: US Core server shall support patient search parameter for Condition\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-004\n",
      "================================================================================\n",
      "Query: US Core server shall support patient search parameter for Condition \"SHALL [patient] reference\" Defining mandatory search parameter support for finding Condition resources by patient reference US Core Server FHIR SHALL\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 0.7029471397399902):\n",
      "  Length: 1452 chars\n",
      "  Preview: FHIR Major Section: ## 14.3 CapabilityStatement: US Core Server CapabilityStatement\n",
      "\n",
      "|  |  |  |  |  ...\n",
      "  ...pi.json) | [Download](us-core-server.openapi.json)\n",
      "\n",
      "  Match 2 (distance: 0.7662544846534729):\n",
      "  Length: 2229 chars\n",
      "  Preview: FHIR Resource/Component: ### 14.3.2 FHIR RESTful Capabilities\n",
      "\n",
      "The US Core Server **SHALL**:\n",
      "\n",
      "1. Sup...\n",
      "  ... **MAY** support the `history-system` interaction.\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-004\n",
      "  [5/11] REQ-005\n",
      "Processing REQ-005: Server support for Encounter Diagnosis referencing encounter\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-005\n",
      "================================================================================\n",
      "Query: Server support for Encounter Diagnosis referencing encounter \"When `Condition.category` is 'encounter-diagnosis' the encounter, **SHOULD** be referenced in `Condition.encounter`.\" When using US Core Condition Encounter Diagnosis Profile for encounter diagnosis conditions, the related encounter should be referenced to maintain clinical context linkage US Core Server FHIR SHOULD\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 0.8486335873603821):\n",
      "  Length: 90628 chars\n",
      "  Preview: FHIR Resource Detail: #### 14.3.3.2 AllergyIntolerance\n",
      "\n",
      "Conformance Expectation: **SHALL**\n",
      "\n",
      "Supporte...\n",
      "  ...d read/write formats for notes on the server.\n",
      "\n",
      "---\n",
      "\n",
      "  Match 2 (distance: 0.884549617767334):\n",
      "  Length: 9735 chars\n",
      "  Preview: FHIR Resource/Component: ### 14.3.3 RESTful Capabilities by Resource/Profile: #### 14.3.3.1 Summary\n",
      "...\n",
      "  ...| [ValueSet](#valueset) | - | - | - | - | expand |\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-005\n",
      "  [6/11] REQ-006\n",
      "Processing REQ-006: Clinical status presence for problems list items\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-006\n",
      "================================================================================\n",
      "Query: Clinical status presence for problems list items \"When `Condition.category` is a 'problems-list-item', the `Condition.clinicalStatus` **SHOULD** be present.\" When using US Core Condition Problems and Health Concerns Profile for problems list items, clinical status should be included to indicate the current state of the condition US Core Server FHIR SHOULD\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 0.8604792356491089):\n",
      "  Length: 1452 chars\n",
      "  Preview: FHIR Major Section: ## 14.3 CapabilityStatement: US Core Server CapabilityStatement\n",
      "\n",
      "|  |  |  |  |  ...\n",
      "  ...pi.json) | [Download](us-core-server.openapi.json)\n",
      "\n",
      "  Match 2 (distance: 0.9102575778961182):\n",
      "  Length: 9735 chars\n",
      "  Preview: FHIR Resource/Component: ### 14.3.3 RESTful Capabilities by Resource/Profile: #### 14.3.3.1 Summary\n",
      "...\n",
      "  ...| [ValueSet](#valueset) | - | - | - | - | expand |\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-006\n",
      "  [7/11] REQ-007\n",
      "Processing REQ-007: Server support for Condition.recordedDate\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-007\n",
      "================================================================================\n",
      "Query: Server support for Condition.recordedDate \"A server **SHALL** support `Condition.recordedDate`.\" For representing dates related to condition diagnosis, servers must support the recorded date element as one of the key temporal indicators US Core Server FHIR SHALL\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 1.0444135665893555):\n",
      "  Length: 1452 chars\n",
      "  Preview: FHIR Major Section: ## 14.3 CapabilityStatement: US Core Server CapabilityStatement\n",
      "\n",
      "|  |  |  |  |  ...\n",
      "  ...pi.json) | [Download](us-core-server.openapi.json)\n",
      "\n",
      "  Match 2 (distance: 1.1116570234298706):\n",
      "  Length: 83 chars\n",
      "  Preview: FHIR Document Title: # 14.3 CapabilityStatement: US Core Server CapabilityStatement...\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-007\n",
      "  [8/11] REQ-008\n",
      "Processing REQ-008: Server support for at least one onset-related date element\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-008\n",
      "================================================================================\n",
      "Query: Server support for at least one onset-related date element \"A server **SHALL** support at least one of the assertedDate Extension and `Condition.onsetDateTime`.\" For representing dates related to condition diagnosis, servers must support at least one onset-related temporal element in addition to recordedDate US Core Server FHIR SHALL\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 1.0340845584869385):\n",
      "  Length: 1452 chars\n",
      "  Preview: FHIR Major Section: ## 14.3 CapabilityStatement: US Core Server CapabilityStatement\n",
      "\n",
      "|  |  |  |  |  ...\n",
      "  ...pi.json) | [Download](us-core-server.openapi.json)\n",
      "\n",
      "  Match 2 (distance: 1.2159615755081177):\n",
      "  Length: 83 chars\n",
      "  Preview: FHIR Document Title: # 14.3 CapabilityStatement: US Core Server CapabilityStatement...\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-008\n",
      "  [9/11] REQ-009\n",
      "Processing REQ-009: Client support for all three date elements\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-009\n",
      "================================================================================\n",
      "Query: Client support for all three date elements \"The client application **SHALL** support all three elements [assertedDate Extension, `Condition.onsetDateTime`, and `Condition.recordedDate`].\" Client applications must be capable of processing all possible date elements that servers might provide for condition temporal information US Core Client FHIR SHALL\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 1.1565313339233398):\n",
      "  Length: 1452 chars\n",
      "  Preview: FHIR Major Section: ## 14.3 CapabilityStatement: US Core Server CapabilityStatement\n",
      "\n",
      "|  |  |  |  |  ...\n",
      "  ...pi.json) | [Download](us-core-server.openapi.json)\n",
      "\n",
      "  Match 2 (distance: 1.2915809154510498):\n",
      "  Length: 83 chars\n",
      "  Preview: FHIR Document Title: # 14.3 CapabilityStatement: US Core Server CapabilityStatement...\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-009\n",
      "  [10/11] REQ-010\n",
      "Processing REQ-010: Updates to lastUpdated for condition changes\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR REQ-010\n",
      "================================================================================\n",
      "Query: Updates to lastUpdated for condition changes \"Updates to `Condition.meta.lastUpdated` **SHOULD** reflect: New problems and health concerns, Changes in the clinical status or verifications status of problems or health concern\" Servers should maintain accurate lastUpdated timestamps to support temporal queries and change tracking for condition resources US Core Server FHIR SHOULD\n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 0.9823336601257324):\n",
      "  Length: 1452 chars\n",
      "  Preview: FHIR Major Section: ## 14.3 CapabilityStatement: US Core Server CapabilityStatement\n",
      "\n",
      "|  |  |  |  |  ...\n",
      "  ...pi.json) | [Download](us-core-server.openapi.json)\n",
      "\n",
      "  Match 2 (distance: 1.088176965713501):\n",
      "  Length: 90628 chars\n",
      "  Preview: FHIR Resource Detail: #### 14.3.3.2 AllergyIntolerance\n",
      "\n",
      "Conformance Expectation: **SHALL**\n",
      "\n",
      "Supporte...\n",
      "  ...d read/write formats for notes on the server.\n",
      "\n",
      "---\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for REQ-010\n",
      "  Completed 10 requirements                    \n",
      "\n",
      "[I cannot analyze this requirement because all the fields (Summary, Text, Context, Verification, Actor, Conformance, Conditional, and Source) are empty. Without any content describing what the requirement actually specifies, it's impossible to determine which resource profile or category it belongs to.] Processing 1 requirements...\n",
      "  [11/11] C\n",
      "Processing C: No summary\n",
      "\n",
      "================================================================================\n",
      "RAG RETRIEVAL FOR C\n",
      "================================================================================\n",
      "Query: FHIR \n",
      "Searching for 2 most relevant capability chunks...\n",
      "\n",
      "Found 2 matching chunks:\n",
      "\n",
      "  Match 1 (distance: 1.1055324077606201):\n",
      "  Length: 2229 chars\n",
      "  Preview: FHIR Resource/Component: ### 14.3.2 FHIR RESTful Capabilities\n",
      "\n",
      "The US Core Server **SHALL**:\n",
      "\n",
      "1. Sup...\n",
      "  ... **MAY** support the `history-system` interaction.\n",
      "\n",
      "  Match 2 (distance: 1.1729710102081299):\n",
      "  Length: 183 chars\n",
      "  Preview: FHIR Resource/Component: ### 14.3.1 SHOULD Support the Following Implementation Guides:\n",
      "\n",
      "* [SMART Ap...\n",
      "  ...(https://hl7.org/fhir/smart-app-launch/index.html)\n",
      "================================================================================\n",
      "\n",
      "Sending request to CLAUDE API...\n",
      "Completed test specification for C\n",
      "  Completed 1 requirements                    \n",
      "\n",
      "================================================================================\n",
      "TEST PLAN GENERATION COMPLETE!\n",
      "Output file: checkpoints/demo/testplan_generation/test_plan.md\n",
      "Total requirements: 11\n",
      "Requirement groups: 2\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'requirements_count': 11,\n",
       " 'group_count': 2,\n",
       " 'test_plan_path': 'checkpoints/demo/testplan_generation/test_plan.md'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_plan_06.generate_consolidated_test_plan(\n",
    "    client_instance=llm_clients, \n",
    "    api_type='claude',\n",
    "    requirements_file=\"checkpoints/demo/requirements_downselect/consolidated_reqs.md\", #input requirements list markdown file\n",
    "    capability_statement_file=\"checkpoints/markdown2/CapabilityStatement-us-core-server.md\", \n",
    "    ig_name=\"US Core IG\", \n",
    "    output_dir='checkpoints/demo/testplan_generation/', \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080ace6",
   "metadata": {},
   "source": [
    "## Stage 4: Test Kit Generation\n",
    "- Converts test specifications into executable Inferno Ruby tests\n",
    "- Generates complete test suites with proper file organization\n",
    "- Creates modular test structures following Inferno framework patterns\n",
    "- Includes validation and alignment checking\n",
    "\n",
    "Inputs: Test plan specification in markdown format\n",
    "\n",
    "Outputs: Complete Inferno test kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ef90585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'test_kit_07' from '/Users/ceadams/Documents/onclaive/onclaive/pipeline/test_kit_07.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import test_kit_07\n",
    "importlib.reload(test_kit_07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771131c2",
   "metadata": {},
   "source": [
    "Without LLM Self Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a684acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERNO TEST KIT GENERATION\n",
      "================================================================================\n",
      "Module: US Core\n",
      "Test plan: checkpoints/demo/testplan_generation/test_plan.md\n",
      "API: claude\n",
      "LLM Validation: Disabled\n",
      "\n",
      "Parsing test plan...\n",
      "Found 10 requirements in test plan\n",
      "Processing requirement: REQ-001\n",
      "Added requirement REQ-001 to section Condition\n",
      "Processing requirement: REQ-002\n",
      "Added requirement REQ-002 to section Condition\n",
      "Processing requirement: REQ-003\n",
      "Added requirement REQ-003 to section Condition\n",
      "Processing requirement: REQ-004\n",
      "Added requirement REQ-004 to section Condition\n",
      "Processing requirement: REQ-005\n",
      "Added requirement REQ-005 to section Condition\n",
      "Processing requirement: REQ-006\n",
      "Added requirement REQ-006 to section Condition\n",
      "Processing requirement: REQ-007\n",
      "Added requirement REQ-007 to section Condition\n",
      "Processing requirement: REQ-008\n",
      "Added requirement REQ-008 to section Condition\n",
      "Processing requirement: REQ-009\n",
      "Added requirement REQ-009 to section Condition\n",
      "Processing requirement: REQ-010\n",
      "Added requirement REQ-010 to section Condition\n",
      "Final sections: ['Condition']\n",
      "  Condition: 10 requirements\n",
      "Found 1 sections with 10 total requirements\n",
      "Loading Inferno DSL guidance...\n",
      "\n",
      "Generating tests...\n",
      "[1/1] Processing: Condition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generated tests for only 0 of 10 requirements in section: Condition\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 10 tests\n",
      "\n",
      "Generated 10 total tests\n",
      "Writing test files...\n",
      "  Created section: condition with 10 tests\n",
      "Analyzing generated test files...\n",
      "Generating main module file...\n",
      "Skipping alignment validation (disabled)\n",
      "\n",
      "================================================================================\n",
      "TEST KIT GENERATION COMPLETE!\n",
      "Output directory: checkpoints/demo/testkit_generation/claude_testkit_20250901_215109\n",
      "Module file: claude_us_core_20250901_215109.rb\n",
      "Total sections: 1\n",
      "Total requirements: 10\n",
      "Generated tests: 10\n",
      "LLM validation: Disabled\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_sections': 1,\n",
       " 'total_requirements': 10,\n",
       " 'generated_tests': 10,\n",
       " 'module_dir': 'checkpoints/demo/testkit_generation/claude_testkit_20250901_215109/us_core',\n",
       " 'module_file': 'checkpoints/demo/testkit_generation/claude_testkit_20250901_215109/claude_us_core_20250901_215109.rb',\n",
       " 'output_dir': 'checkpoints/demo/testkit_generation/claude_testkit_20250901_215109',\n",
       " 'timestamp': '20250901_215109',\n",
       " 'validation_enabled': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faster generation- no LLM self evaluation\n",
    "test_kit_07.generate_inferno_test_kit(\n",
    "    client_instance=llm_clients, #initialize llm clients\n",
    "    api_type='claude',  #set API\n",
    "    test_plan_file='checkpoints/demo/testplan_generation/test_plan.md',  #input test plan file\n",
    "    ig_name='US Core',\n",
    "    output_dir='checkpoints/demo/testkit_generation/',\n",
    "    enable_validation=False  #disable LLM self evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d11cfdf",
   "metadata": {},
   "source": [
    "With LLM Self Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75d5d430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERNO TEST KIT GENERATION\n",
      "================================================================================\n",
      "Module: US Core\n",
      "Test plan: checkpoints/demo/testplan_generation/test_plan.md\n",
      "API: claude\n",
      "LLM Validation: Enabled\n",
      "\n",
      "Parsing test plan...\n",
      "Found 10 requirements in test plan\n",
      "Processing requirement: REQ-001\n",
      "Added requirement REQ-001 to section Condition\n",
      "Processing requirement: REQ-002\n",
      "Added requirement REQ-002 to section Condition\n",
      "Processing requirement: REQ-003\n",
      "Added requirement REQ-003 to section Condition\n",
      "Processing requirement: REQ-004\n",
      "Added requirement REQ-004 to section Condition\n",
      "Processing requirement: REQ-005\n",
      "Added requirement REQ-005 to section Condition\n",
      "Processing requirement: REQ-006\n",
      "Added requirement REQ-006 to section Condition\n",
      "Processing requirement: REQ-007\n",
      "Added requirement REQ-007 to section Condition\n",
      "Processing requirement: REQ-008\n",
      "Added requirement REQ-008 to section Condition\n",
      "Processing requirement: REQ-009\n",
      "Added requirement REQ-009 to section Condition\n",
      "Processing requirement: REQ-010\n",
      "Added requirement REQ-010 to section Condition\n",
      "Final sections: ['Condition']\n",
      "  Condition: 10 requirements\n",
      "Found 1 sections with 10 total requirements\n",
      "Loading Inferno DSL guidance...\n",
      "\n",
      "Generating tests...\n",
      "[1/1] Processing: Condition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generated tests for only 0 of 10 requirements in section: Condition\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 10 tests\n",
      "\n",
      "Generated 10 total tests\n",
      "Writing test files...\n",
      "  Created section: condition with 10 tests\n",
      "Analyzing generated test files...\n",
      "Generating main module file...\n",
      "Performing alignment validation...\n",
      "  Alignment validation completed successfully\n",
      "  Applied fixes to module file\n",
      "\n",
      "================================================================================\n",
      "TEST KIT GENERATION COMPLETE!\n",
      "Output directory: checkpoints/demo/testkit_generation/claude_testkit_20250901_215618\n",
      "Module file: claude_us_core_20250901_215618.rb\n",
      "Total sections: 1\n",
      "Total requirements: 10\n",
      "Generated tests: 10\n",
      "LLM validation: Enabled\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_sections': 1,\n",
       " 'total_requirements': 10,\n",
       " 'generated_tests': 10,\n",
       " 'module_dir': 'checkpoints/demo/testkit_generation/claude_testkit_20250901_215618/us_core',\n",
       " 'module_file': 'checkpoints/demo/testkit_generation/claude_testkit_20250901_215618/claude_us_core_20250901_215618.rb',\n",
       " 'output_dir': 'checkpoints/demo/testkit_generation/claude_testkit_20250901_215618',\n",
       " 'timestamp': '20250901_215618',\n",
       " 'validation_enabled': True}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thorough generation- with LLM self evaluation\n",
    "test_kit_07.generate_inferno_test_kit(\n",
    "    client_instance=llm_clients, #initialize llm clients\n",
    "    api_type='claude',  #set API\n",
    "    test_plan_file='checkpoints/demo/testplan_generation/test_plan.md',  #input test plan file\n",
    "    ig_name='US Core',\n",
    "    output_dir='checkpoints/demo/testkit_generation/',\n",
    "    enable_validation=True  #enable LLM self evaluation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
