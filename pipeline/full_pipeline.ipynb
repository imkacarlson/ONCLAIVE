{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e3a524",
   "metadata": {},
   "source": [
    "# IG to Test Kit FULL pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d267b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de19ee6",
   "metadata": {},
   "source": [
    "### Importing Notebooks as Modules (from the [Jupyter Notebook Documentation](https://jupyter-notebook.readthedocs.io/en/4.x/examples/Notebook/rstversions/Importing%20Notebooks.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9fc5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ac24b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/5w3wfdld3gv5brcy7cz9m7f80000gn/T/ipykernel_2497/1429934450.py:3: DeprecationWarning: nbformat.current is deprecated since before nbformat 3.0\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  from nbformat import current\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        \n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "          for cell in nb.worksheets[0].cells:\n",
    "            if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "    \n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "    \n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d80d1",
   "metadata": {},
   "source": [
    "## Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08717e1c",
   "metadata": {},
   "source": [
    "### HTML to Markdown Conversion Using Markdownify (Langchain Tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9d960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from HTML_extractor_pipeline.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import HTML_extractor_pipeline as HTMLextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d026195",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = json.loads(inspect.getsource(HTMLextract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93d6f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['# Extracting items from HTML']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': [\"This notebook explores extracting narrative elements from HTML files using multiple methods incuding LangChain's `AsyncHtmlLoader` to convert URLs to to Markdown, LangChain's `UnstructuredHTMLLoader` and `BSHTMLLoader` methods to convert HTML from local folders, as well as a custom content extractor method `ContextExtractor` using BeautifulSoup to parse HTML content and extract specific elements.\\n\",\n",
       "    '\\n',\n",
       "    'This notebook currently extracts Markdown from the following 6 links. There may be more links to add in the future.\\n',\n",
       "    '\\n',\n",
       "    '- https://hl7.org/fhir/us/davinci-pdex-plan-net/index.html\\n',\n",
       "    '- https://hl7.org/fhir/us/davinci-pdex-plan-net/ChangeHistory.html\\n',\n",
       "    '- https://hl7.org/fhir/us/davinci-pdex-plan-net/examples.html\\n',\n",
       "    '- https://hl7.org/fhir/us/davinci-pdex-plan-net/implementation.html\\n',\n",
       "    '- https://hl7.org/fhir/us/davinci-pdex-plan-net/profiles.html\\n',\n",
       "    '- https://hl7.org/fhir/us/davinci-pdex-plan-net/artifacts.html\\n',\n",
       "    '- https://hl7.org/fhir/us/davinci-pdex-plan-net/CapabilityStatement-plan-net.html']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stderr',\n",
       "     'output_type': 'stream',\n",
       "     'text': [\"/Users/amathur/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\\n\",\n",
       "      '  warnings.warn(\\n',\n",
       "      'USER_AGENT environment variable not set, consider setting it to identify your requests.\\n']}],\n",
       "   'source': ['from bs4 import BeautifulSoup\\n',\n",
       "    'import os\\n',\n",
       "    'from pathlib import Path\\n',\n",
       "    'from IPython.display import display, HTML\\n',\n",
       "    'from bs4.element import Tag\\n',\n",
       "    'from langchain_community.document_loaders import AsyncHtmlLoader\\n',\n",
       "    'from langchain_community.document_loaders import BSHTMLLoader\\n',\n",
       "    'from langchain_community.document_loaders import UnstructuredHTMLLoader\\n',\n",
       "    'from langchain_community.document_transformers import MarkdownifyTransformer\\n',\n",
       "    'from urllib.parse import urlparse\\n',\n",
       "    '\\n',\n",
       "    \"CERT_PATH = '/Users/jrockhill/miniconda3/envs/ONCL310/lib/python3.10/site-packages/pip/_vendor/certifi/cacert.pem'\"]},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Langchain tool (Markdownify) to convert HTML to Markdown']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['Create a directory to store Markdown files.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# # Directory where you want to save the markdown files\\n',\n",
       "    \"# output_dir = 'PlanNet/site/markdown'\\n\",\n",
       "    \"# # Create output directory if it doesn't exist\\n\",\n",
       "    '# Path(output_dir).mkdir(parents=True, exist_ok=True)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['Install necessary libraries if not already installed']},\n",
       "  {'cell_type': 'markdown', 'metadata': {}, 'source': []},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 3,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# %pip install --upgrade --quiet  markdownify\\n',\n",
       "    '# %pip install -U lxml\\n',\n",
       "    '# %pip install unstructured']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 4,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def get_filename_from_url(url):\\n',\n",
       "    '    \"\"\"Extract filename from URL and convert to markdown filename.\"\"\"\\n',\n",
       "    '    # Parse the URL and get the path\\n',\n",
       "    '    path = urlparse(url).path\\n',\n",
       "    '    \\n',\n",
       "    '    # Get the last part of the path (filename)\\n',\n",
       "    \"    filename = path.split('/')[-1]\\n\",\n",
       "    '    \\n',\n",
       "    '    # Remove .html extension if present\\n',\n",
       "    \"    filename = filename.replace('.html', '')\\n\",\n",
       "    '    \\n',\n",
       "    '    # Convert to title case and replace special characters\\n',\n",
       "    \"    filename = filename.replace('-', '_')\\n\",\n",
       "    '    \\n',\n",
       "    '    # Add .md extension\\n',\n",
       "    '    return f\"{filename}.md\"']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def convert_urls_to_markdown(urls, output_dir=\"PlanNet/site/markdown\"):\\n',\n",
       "    '    \"\"\"Convert multiple URLs to markdown files.\"\"\"\\n',\n",
       "    '    # Initialize loaders and transformers\\n',\n",
       "    '    loader = AsyncHtmlLoader(urls, verify_ssl=False)\\n',\n",
       "    '    md_transformer = MarkdownifyTransformer()\\n',\n",
       "    '    \\n',\n",
       "    '    # Load all documents\\n',\n",
       "    '    docs = loader.load()\\n',\n",
       "    '    converted_docs = md_transformer.transform_documents(docs)\\n',\n",
       "    '    \\n',\n",
       "    \"    # Create output directory if it doesn't exist\\n\",\n",
       "    '    os.makedirs(output_dir, exist_ok=True)\\n',\n",
       "    '    \\n',\n",
       "    '    # Process each document\\n',\n",
       "    '    for url, doc in zip(urls, converted_docs):\\n',\n",
       "    '        # Generate filename from URL\\n',\n",
       "    '        filename = get_filename_from_url(url)\\n',\n",
       "    '        \\n',\n",
       "    '        # Create full file path\\n',\n",
       "    '        file_path = os.path.join(output_dir, filename)\\n',\n",
       "    '        \\n',\n",
       "    '        # Write content to file\\n',\n",
       "    \"        with open(file_path, 'w', encoding='utf-8') as f:\\n\",\n",
       "    '            f.write(doc.page_content)\\n',\n",
       "    '        \\n',\n",
       "    '        print(f\"Created: {filename}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# urls = [\\n',\n",
       "    '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/index.html\",\\n',\n",
       "    '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/ChangeHistory.html\",\\n',\n",
       "    '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/examples.html\",\\n',\n",
       "    '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/implementation.html\",\\n',\n",
       "    '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/profiles.html\",\\n',\n",
       "    '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/artifacts.html\",\\n',\n",
       "    '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/CapabilityStatement-plan-net.html\"\\n',\n",
       "    '# ]\\n']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stderr',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['Fetching pages: 100%|##########| 7/7 [00:00<00:00,  8.63it/s]\\n']},\n",
       "    {'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['Created: index.md\\n',\n",
       "      'Created: ChangeHistory.md\\n',\n",
       "      'Created: examples.md\\n',\n",
       "      'Created: implementation.md\\n',\n",
       "      'Created: profiles.md\\n',\n",
       "      'Created: artifacts.md\\n',\n",
       "      'Created: CapabilityStatement_plan_net.md\\n']}],\n",
       "   'source': ['# convert_urls_to_markdown(urls)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['Files should be stored in the `PlanNet/site/markdown` folder.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Experimental coverters']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### Converting from local HTML files to Markdown\\n',\n",
       "    '\\n',\n",
       "    \"WIP: While this function works, it's not perfect. It's not preserving the structure of the HTML well. Would reccomend using the langchain tool above instead. I've tried using two document loader functions `BSHTMLLoader` as well as `UnstructuredHTMLLoader` but neither seem to work well at this point.\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 11,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def get_filename_from_html(html_file):\\n',\n",
       "    '    \"\"\"Convert HTML filename to markdown filename.\"\"\"\\n',\n",
       "    '    # Get the base filename without extension\\n',\n",
       "    '    base_name = os.path.splitext(os.path.basename(html_file))[0]\\n',\n",
       "    '    \\n',\n",
       "    '    # Convert to title case and replace special characters\\n',\n",
       "    \"    filename = base_name.replace('-', '_')\\n\",\n",
       "    '    \\n',\n",
       "    '    # Add .md extension\\n',\n",
       "    '    return f\"{filename}.md\"']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 15,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def convert_html_to_markdown(html_files):\\n',\n",
       "    '    \"\"\"Convert multiple local HTML files to markdown files.\"\"\"\\n',\n",
       "    \"    # Create output directory if it doesn't exist\\n\",\n",
       "    '    output_dir = \"PlanNet/site/markdown\" #NOTE: This is the same directory as the url to markdown converter above. Change to keep the results seprate. \\n',\n",
       "    '    os.makedirs(output_dir, exist_ok=True)\\n',\n",
       "    '    \\n',\n",
       "    '    # Process each HTML file\\n',\n",
       "    '    for html_file in html_files:\\n',\n",
       "    '        try:\\n',\n",
       "    '            # Load and convert the HTML file\\n',\n",
       "    '            loader = UnstructuredHTMLLoader(html_file)\\n',\n",
       "    '            doc = loader.load()[0]  # BSHTMLLoader returns a list\\n',\n",
       "    '            \\n',\n",
       "    '            # Transform to markdown\\n',\n",
       "    '            md_transformer = MarkdownifyTransformer()\\n',\n",
       "    '            converted_doc = md_transformer.transform_documents([doc])[0]\\n',\n",
       "    '            \\n',\n",
       "    '            # Generate output filename\\n',\n",
       "    '            output_filename = get_filename_from_html(html_file)\\n',\n",
       "    '            output_path = os.path.join(output_dir, output_filename)\\n',\n",
       "    '            \\n',\n",
       "    '            # Write content to file\\n',\n",
       "    \"            with open(output_path, 'w', encoding='utf-8') as f:\\n\",\n",
       "    '                f.write(converted_doc.page_content)\\n',\n",
       "    '            \\n',\n",
       "    '            print(f\"Successfully converted {html_file} to {output_filename}\")\\n',\n",
       "    '            \\n',\n",
       "    '        except Exception as e:\\n',\n",
       "    '            print(f\"Error processing {html_file}: {str(e)}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# html_files = [\\n',\n",
       "    '#     \"PlanNet/site/CapabilityStatement-plan-net.html\",\\n',\n",
       "    '#     \"PlanNet/site/ChangeHistory.html\",\\n',\n",
       "    '#     \"PlanNet/site/index.html\",\\n',\n",
       "    '#     \"PlanNet/site/examples.html\",\\n',\n",
       "    '#     \"PlanNet/site/implementation.html\",\\n',\n",
       "    '#     \"PlanNet/site/profiles.html\",\\n',\n",
       "    '#     \"PlanNet/site/artifacts.html\"\\n',\n",
       "    '# ]']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# convert_html_to_markdown(html_files)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### Custom Content Extractor']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['We define a class `ContentExtractor` to to extract content from HTML files. This class has methods to extract text, tables, and list elements from the HTML. The extracted content is then formatted as markdown and written to a file. The class has methods that also check to see if elements have been processed to avoid duplicates. From images, there is a method (`_extract_images`) to pull the src and alt text and format as markdown image with additional source info.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 2,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['class ContextExtractor:\\n',\n",
       "    '    def __init__(self):\\n',\n",
       "    '        \"\"\"Initialize the context extractor\"\"\"\\n',\n",
       "    '        self.processed_elements = set()\\n',\n",
       "    '        \\n',\n",
       "    '    def _has_been_processed(self, element):\\n',\n",
       "    '        \"\"\"Check if an element has already been processed\"\"\"\\n',\n",
       "    '        if not isinstance(element, Tag):\\n',\n",
       "    '            return False\\n',\n",
       "    \"        return element.get('data-processed') == 'true'\\n\",\n",
       "    '    \\n',\n",
       "    '    def _mark_processed(self, element):\\n',\n",
       "    '        \"\"\"Mark an element as processed\"\"\"\\n',\n",
       "    '        if isinstance(element, Tag):\\n',\n",
       "    \"            element['data-processed'] = 'true'\\n\",\n",
       "    '            self.processed_elements.add(element)\\n',\n",
       "    '\\n',\n",
       "    '    def _extract_images(self, element):\\n',\n",
       "    '        \"\"\"\\n',\n",
       "    '        Extract image information including src and alt text\\n',\n",
       "    '        \\n',\n",
       "    '        Args:\\n',\n",
       "    '            element: BeautifulSoup element containing images\\n',\n",
       "    '        Returns:\\n',\n",
       "    '            list: Formatted image information in Markdown\\n',\n",
       "    '        \"\"\"\\n',\n",
       "    '        if self._has_been_processed(element):\\n',\n",
       "    '            return []\\n',\n",
       "    '            \\n',\n",
       "    '        images = []\\n',\n",
       "    \"        for img in element.find_all('img', recursive=False):\\n\",\n",
       "    \"            src = img.get('src', '')\\n\",\n",
       "    \"            alt = img.get('alt', '')\\n\",\n",
       "    '            if src:\\n',\n",
       "    '                # Format as Markdown image with additional source info\\n',\n",
       "    '                images.append(f\"![{alt}]({src})\")\\n',\n",
       "    '                images.append(f\"*Image source: {src}*\")\\n',\n",
       "    '                images.append(f\"*Image description: {alt}*\")\\n',\n",
       "    '                images.append(\"\")  # Add blank line after each image\\n',\n",
       "    '                \\n',\n",
       "    '        self._mark_processed(element)\\n',\n",
       "    '        return images\\n',\n",
       "    '\\n',\n",
       "    '    def _extract_list_items(self, list_element, level=0, parent_type=None):\\n',\n",
       "    '        \"\"\"Extract list items with improved nested list handling\"\"\"\\n',\n",
       "    '        if self._has_been_processed(list_element):\\n',\n",
       "    '            return []\\n',\n",
       "    '            \\n',\n",
       "    '        items = []\\n',\n",
       "    \"        for item in list_element.find_all('li', recursive=False):\\n\",\n",
       "    '            if not self._has_been_processed(item):\\n',\n",
       "    '                # Get direct text content of the li element (excluding nested list text)\\n',\n",
       "    \"                item_text = ''\\n\",\n",
       "    '                for content in item.children:\\n',\n",
       "    '                    if isinstance(content, Tag):\\n',\n",
       "    \"                        if content.name not in ['ul', 'ol']:\\n\",\n",
       "    \"                            if content.name == 'img':\\n\",\n",
       "    '                                # Handle images within list items\\n',\n",
       "    '                                image_info = self._extract_images(content.parent)\\n',\n",
       "    '                                items.extend([f\"{\\'    \\' * level}{line}\" for line in image_info])\\n',\n",
       "    '                            else:\\n',\n",
       "    \"                                item_text += content.get_text(strip=True) + ' '\\n\",\n",
       "    '                    else:\\n',\n",
       "    \"                        item_text += content.strip() + ' '\\n\",\n",
       "    '                item_text = item_text.strip()\\n',\n",
       "    '                \\n',\n",
       "    '                # Format the list item\\n',\n",
       "    \"                prefix = '    ' * level\\n\",\n",
       "    \"                if list_element.name == 'ol':\\n\",\n",
       "    '                    items.append(f\"{prefix}1. {item_text}\")\\n',\n",
       "    '                else:\\n',\n",
       "    '                    items.append(f\"{prefix}- {item_text}\")\\n',\n",
       "    '                \\n',\n",
       "    '                # Handle nested lists\\n',\n",
       "    \"                nested_lists = item.find_all(['ul', 'ol'], recursive=False)\\n\",\n",
       "    '                for nested_list in nested_lists:\\n',\n",
       "    '                    nested_items = []\\n',\n",
       "    \"                    for nested_item in nested_list.find_all('li', recursive=False):\\n\",\n",
       "    '                        nested_text = nested_item.get_text(strip=True)\\n',\n",
       "    \"                        if nested_list.name == 'ol':\\n\",\n",
       "    '                            nested_items.append(f\"{prefix}    * {nested_text}\")\\n',\n",
       "    '                        else:\\n',\n",
       "    '                            nested_items.append(f\"{prefix}    * {nested_text}\")\\n',\n",
       "    '                    items.extend(nested_items)\\n',\n",
       "    '                \\n',\n",
       "    '                self._mark_processed(item)\\n',\n",
       "    '        \\n',\n",
       "    '        self._mark_processed(list_element)\\n',\n",
       "    '        return items\\n',\n",
       "    '\\n',\n",
       "    '    def _extract_table(self, table):\\n',\n",
       "    '        \"\"\"Extract table content in Markdown format\"\"\"\\n',\n",
       "    '        if self._has_been_processed(table):\\n',\n",
       "    '            return \"\"\\n',\n",
       "    '            \\n',\n",
       "    '        rows = []\\n',\n",
       "    '        headers = []\\n',\n",
       "    '        \\n',\n",
       "    \"        header_row = table.find('thead') or table.find('tr')\\n\",\n",
       "    '        if header_row:\\n',\n",
       "    \"            headers = [cell.get_text(strip=True) for cell in header_row.find_all(['th', 'td'])]\\n\",\n",
       "    '        \\n',\n",
       "    \"        for row in table.find_all('tr')[1:] if headers else table.find_all('tr'):\\n\",\n",
       "    \"            row_data = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\\n\",\n",
       "    '            if any(row_data):\\n',\n",
       "    '                rows.append(row_data)\\n',\n",
       "    '        \\n',\n",
       "    '        table_str = []\\n',\n",
       "    '        if headers:\\n',\n",
       "    '            table_str.append(\"| \" + \" | \".join(headers) + \" |\")\\n',\n",
       "    '            table_str.append(\"|\" + \"|\".join([\" --- \" for _ in headers]) + \"|\")\\n',\n",
       "    '        \\n',\n",
       "    '        for row in rows:\\n',\n",
       "    '            if headers:\\n',\n",
       "    \"                row.extend([''] * (len(headers) - len(row)))\\n\",\n",
       "    '            table_str.append(\"| \" + \" | \".join(row) + \" |\")\\n',\n",
       "    '        \\n',\n",
       "    '        self._mark_processed(table)\\n',\n",
       "    '        return \"\\\\n\".join(table_str)\\n',\n",
       "    '\\n',\n",
       "    '    def extract_context(self, html_content):\\n',\n",
       "    '        \"\"\"Extract content with improved list and image handling\"\"\"\\n',\n",
       "    \"        soup = BeautifulSoup(html_content, 'html.parser')\\n\",\n",
       "    '        self.processed_elements.clear()\\n',\n",
       "    '        context_elements = []\\n',\n",
       "    '        \\n',\n",
       "    '        # Remove script and style elements\\n',\n",
       "    \"        for script in soup(['script', 'style', 'nav', 'footer']):\\n\",\n",
       "    '            script.decompose()\\n',\n",
       "    '        \\n',\n",
       "    '        # Process headers and their content\\n',\n",
       "    \"        for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\\n\",\n",
       "    '            if not self._has_been_processed(header):\\n',\n",
       "    '                level = int(header.name[1])\\n',\n",
       "    '                header_text = header.get_text().strip()\\n',\n",
       "    '                \\n',\n",
       "    '                if header_text:\\n',\n",
       "    '                    context_elements.append(f\"\\\\n{\\'#\\' * level} {header_text}\\\\n\")\\n',\n",
       "    '                    self._mark_processed(header)\\n',\n",
       "    '                    \\n',\n",
       "    '                    # Process content until next header\\n',\n",
       "    '                    next_element = header.find_next()\\n',\n",
       "    \"                    while next_element and not next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\\n\",\n",
       "    '                        if not self._has_been_processed(next_element):\\n',\n",
       "    \"                            if next_element.name == 'p':\\n\",\n",
       "    '                                # Handle images within paragraphs\\n',\n",
       "    \"                                if next_element.find('img'):\\n\",\n",
       "    '                                    image_info = self._extract_images(next_element)\\n',\n",
       "    '                                    context_elements.extend(image_info)\\n',\n",
       "    '                                else:\\n',\n",
       "    '                                    text = next_element.get_text().strip()\\n',\n",
       "    '                                    if text:\\n',\n",
       "    '                                        context_elements.append(text)\\n',\n",
       "    '                                        context_elements.append(\"\")\\n',\n",
       "    \"                            elif next_element.name in ['ul', 'ol']:\\n\",\n",
       "    '                                list_items = self._extract_list_items(next_element)\\n',\n",
       "    '                                if list_items:\\n',\n",
       "    '                                    context_elements.extend(list_items)\\n',\n",
       "    '                                    context_elements.append(\"\")\\n',\n",
       "    \"                            elif next_element.name == 'table':\\n\",\n",
       "    '                                table_content = self._extract_table(next_element)\\n',\n",
       "    '                                if table_content:\\n',\n",
       "    '                                    context_elements.append(table_content)\\n',\n",
       "    '                                    context_elements.append(\"\")\\n',\n",
       "    '                        \\n',\n",
       "    '                        next_element = next_element.find_next()\\n',\n",
       "    '\\n',\n",
       "    '        # Process any remaining top-level images\\n',\n",
       "    \"        for img_container in soup.find_all('p'):\\n\",\n",
       "    \"            if img_container.find('img') and not self._has_been_processed(img_container):\\n\",\n",
       "    '                image_info = self._extract_images(img_container)\\n',\n",
       "    '                if image_info:\\n',\n",
       "    '                    context_elements.extend(image_info)\\n',\n",
       "    '        \\n',\n",
       "    '        # Clean up repeated empty lines\\n',\n",
       "    '        cleaned_elements = []\\n',\n",
       "    '        prev_empty = False\\n',\n",
       "    '        for element in context_elements:\\n',\n",
       "    '            if element.strip() == \"\":\\n',\n",
       "    '                if not prev_empty:\\n',\n",
       "    '                    cleaned_elements.append(element)\\n',\n",
       "    '                    prev_empty = True\\n',\n",
       "    '            else:\\n',\n",
       "    '                cleaned_elements.append(element)\\n',\n",
       "    '                prev_empty = False\\n',\n",
       "    '        \\n',\n",
       "    '        return cleaned_elements\\n',\n",
       "    '\\n',\n",
       "    '    def save_context(self, context_elements, output_file):\\n',\n",
       "    '        \"\"\"Save context to Markdown file\"\"\"\\n',\n",
       "    \"        with open(output_file, 'w', encoding='utf-8') as f:\\n\",\n",
       "    '            for element in context_elements:\\n',\n",
       "    '                f.write(f\"{element}\\\\n\")\\n',\n",
       "    '\\n',\n",
       "    '    def process_html_file(self, input_file, output_file):\\n',\n",
       "    '        \"\"\"Process HTML file to Markdown\"\"\"\\n',\n",
       "    '        try:\\n',\n",
       "    \"            output_file = os.path.splitext(output_file)[0] + '.md'\\n\",\n",
       "    '            \\n',\n",
       "    \"            with open(input_file, 'r', encoding='utf-8') as f:\\n\",\n",
       "    '                html_content = f.read()\\n',\n",
       "    '            \\n',\n",
       "    '            context_elements = self.extract_context(html_content)\\n',\n",
       "    '            self.save_context(context_elements, output_file)\\n',\n",
       "    '            self._display_summary(input_file, output_file, len(context_elements))\\n',\n",
       "    '            \\n',\n",
       "    '            return context_elements\\n',\n",
       "    '            \\n',\n",
       "    '        except Exception as e:\\n',\n",
       "    '            display(HTML(f\\'<div style=\"color: red;\">Error processing file: {str(e)}</div>\\'))\\n',\n",
       "    '            return []\\n',\n",
       "    '\\n',\n",
       "    '    def process_directory(self, input_dir, output_dir):\\n',\n",
       "    '        \"\"\"Process directory of HTML files\"\"\"\\n',\n",
       "    '        Path(output_dir).mkdir(parents=True, exist_ok=True)\\n',\n",
       "    '        \\n',\n",
       "    \"        for file in Path(input_dir).glob('*.html'):\\n\",\n",
       "    '            output_file = Path(output_dir) / f\"{file.stem}.md\"\\n',\n",
       "    '            self.process_html_file(str(file), str(output_file))\\n',\n",
       "    '\\n',\n",
       "    '    def _display_summary(self, input_file, output_file, num_elements):\\n',\n",
       "    '        \"\"\"Display processing summary\"\"\"\\n',\n",
       "    '        summary_html = f\"\"\"\\n',\n",
       "    '        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px; margin: 10px 0;\">\\n',\n",
       "    '            <p><strong>Processed file:</strong> {os.path.basename(input_file)}</p>\\n',\n",
       "    '            <p><strong>Output saved to:</strong> {os.path.basename(output_file)}</p>\\n',\n",
       "    '            <p><strong>Extracted elements:</strong> {num_elements}</p>\\n',\n",
       "    '        </div>\\n',\n",
       "    '        \"\"\"\\n',\n",
       "    '        display(HTML(summary_html))']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### Example Use of Custom Content Extractor']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['`ContentExtractor` is called by creating an instance of the class and then calling the `process_html_file` or `process_directory` method. The `process_html_file` method takes two arguments: the path to the input HTML file and the desired name of the output Markdown file. The `process_directory` method takes two arguments: the path to the input directory containing HTML files and the path to the output directory where the Markdown files will be saved.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# extractor = ContextExtractor()\\n',\n",
       "    '# context = extractor.process_html_file(\\n',\n",
       "    \"#     input_file='PlanNet/site/index.html',\\n\",\n",
       "    \"#     output_file='context'\\n\",\n",
       "    '# )']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# for i, element in enumerate(context[:5]):  # Show first 5 elements\\n',\n",
       "    '#     print(f\"\\\\nElement {i+1}:\")\\n',\n",
       "    '#     print(element[:200] + \"...\" if len(element) > 200 else element)']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# # Directory containing your HTML files\\n',\n",
       "    \"# input_dir = 'PlanNet/site'\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# # Process all HTML files in the input directory\\n',\n",
       "    \"# for html_file in Path(input_dir).glob('**/*.html'):\\n\",\n",
       "    '#     # Create corresponding output path while preserving directory structure\\n',\n",
       "    '#     relative_path = html_file.relative_to(input_dir)\\n',\n",
       "    \"#     output_path = Path(output_dir) / relative_path.with_suffix('.md')\\n\",\n",
       "    '    \\n',\n",
       "    '#     # Create necessary subdirectories\\n',\n",
       "    '#     output_path.parent.mkdir(parents=True, exist_ok=True)\\n',\n",
       "    '    \\n',\n",
       "    '#     # Process the file\\n',\n",
       "    '#     context = extractor.process_html_file(\\n',\n",
       "    '#         input_file=str(html_file),\\n',\n",
       "    '#         output_file=str(output_path)\\n',\n",
       "    '#     )']}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'ONCL310',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'file_extension': '.py',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'name': 'python',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'version': '3.10.16'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e143378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stderr',\n",
       "    'output_type': 'stream',\n",
       "    'text': [\"/Users/amathur/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\\n\",\n",
       "     '  warnings.warn(\\n',\n",
       "     'USER_AGENT environment variable not set, consider setting it to identify your requests.\\n']}],\n",
       "  'source': ['from bs4 import BeautifulSoup\\n',\n",
       "   'import os\\n',\n",
       "   'from pathlib import Path\\n',\n",
       "   'from IPython.display import display, HTML\\n',\n",
       "   'from bs4.element import Tag\\n',\n",
       "   'from langchain_community.document_loaders import AsyncHtmlLoader\\n',\n",
       "   'from langchain_community.document_loaders import BSHTMLLoader\\n',\n",
       "   'from langchain_community.document_loaders import UnstructuredHTMLLoader\\n',\n",
       "   'from langchain_community.document_transformers import MarkdownifyTransformer\\n',\n",
       "   'from urllib.parse import urlparse\\n',\n",
       "   '\\n',\n",
       "   \"CERT_PATH = '/Users/jrockhill/miniconda3/envs/ONCL310/lib/python3.10/site-packages/pip/_vendor/certifi/cacert.pem'\"]},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# # Directory where you want to save the markdown files\\n',\n",
       "   \"# output_dir = 'PlanNet/site/markdown'\\n\",\n",
       "   \"# # Create output directory if it doesn't exist\\n\",\n",
       "   '# Path(output_dir).mkdir(parents=True, exist_ok=True)']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 3,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# %pip install --upgrade --quiet  markdownify\\n',\n",
       "   '# %pip install -U lxml\\n',\n",
       "   '# %pip install unstructured']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 4,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['def get_filename_from_url(url):\\n',\n",
       "   '    \"\"\"Extract filename from URL and convert to markdown filename.\"\"\"\\n',\n",
       "   '    # Parse the URL and get the path\\n',\n",
       "   '    path = urlparse(url).path\\n',\n",
       "   '    \\n',\n",
       "   '    # Get the last part of the path (filename)\\n',\n",
       "   \"    filename = path.split('/')[-1]\\n\",\n",
       "   '    \\n',\n",
       "   '    # Remove .html extension if present\\n',\n",
       "   \"    filename = filename.replace('.html', '')\\n\",\n",
       "   '    \\n',\n",
       "   '    # Convert to title case and replace special characters\\n',\n",
       "   \"    filename = filename.replace('-', '_')\\n\",\n",
       "   '    \\n',\n",
       "   '    # Add .md extension\\n',\n",
       "   '    return f\"{filename}.md\"']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['def convert_urls_to_markdown(urls, output_dir=\"PlanNet/site/markdown\"):\\n',\n",
       "   '    \"\"\"Convert multiple URLs to markdown files.\"\"\"\\n',\n",
       "   '    # Initialize loaders and transformers\\n',\n",
       "   '    loader = AsyncHtmlLoader(urls, verify_ssl=False)\\n',\n",
       "   '    md_transformer = MarkdownifyTransformer()\\n',\n",
       "   '    \\n',\n",
       "   '    # Load all documents\\n',\n",
       "   '    docs = loader.load()\\n',\n",
       "   '    converted_docs = md_transformer.transform_documents(docs)\\n',\n",
       "   '    \\n',\n",
       "   \"    # Create output directory if it doesn't exist\\n\",\n",
       "   '    os.makedirs(output_dir, exist_ok=True)\\n',\n",
       "   '    \\n',\n",
       "   '    # Process each document\\n',\n",
       "   '    for url, doc in zip(urls, converted_docs):\\n',\n",
       "   '        # Generate filename from URL\\n',\n",
       "   '        filename = get_filename_from_url(url)\\n',\n",
       "   '        \\n',\n",
       "   '        # Create full file path\\n',\n",
       "   '        file_path = os.path.join(output_dir, filename)\\n',\n",
       "   '        \\n',\n",
       "   '        # Write content to file\\n',\n",
       "   \"        with open(file_path, 'w', encoding='utf-8') as f:\\n\",\n",
       "   '            f.write(doc.page_content)\\n',\n",
       "   '        \\n',\n",
       "   '        print(f\"Created: {filename}\")']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# urls = [\\n',\n",
       "   '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/index.html\",\\n',\n",
       "   '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/ChangeHistory.html\",\\n',\n",
       "   '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/examples.html\",\\n',\n",
       "   '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/implementation.html\",\\n',\n",
       "   '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/profiles.html\",\\n',\n",
       "   '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/artifacts.html\",\\n',\n",
       "   '#     \"https://hl7.org/fhir/us/davinci-pdex-plan-net/CapabilityStatement-plan-net.html\"\\n',\n",
       "   '# ]\\n']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stderr',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['Fetching pages: 100%|##########| 7/7 [00:00<00:00,  8.63it/s]\\n']},\n",
       "   {'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['Created: index.md\\n',\n",
       "     'Created: ChangeHistory.md\\n',\n",
       "     'Created: examples.md\\n',\n",
       "     'Created: implementation.md\\n',\n",
       "     'Created: profiles.md\\n',\n",
       "     'Created: artifacts.md\\n',\n",
       "     'Created: CapabilityStatement_plan_net.md\\n']}],\n",
       "  'source': ['# convert_urls_to_markdown(urls)']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 11,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['def get_filename_from_html(html_file):\\n',\n",
       "   '    \"\"\"Convert HTML filename to markdown filename.\"\"\"\\n',\n",
       "   '    # Get the base filename without extension\\n',\n",
       "   '    base_name = os.path.splitext(os.path.basename(html_file))[0]\\n',\n",
       "   '    \\n',\n",
       "   '    # Convert to title case and replace special characters\\n',\n",
       "   \"    filename = base_name.replace('-', '_')\\n\",\n",
       "   '    \\n',\n",
       "   '    # Add .md extension\\n',\n",
       "   '    return f\"{filename}.md\"']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 15,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['def convert_html_to_markdown(html_files):\\n',\n",
       "   '    \"\"\"Convert multiple local HTML files to markdown files.\"\"\"\\n',\n",
       "   \"    # Create output directory if it doesn't exist\\n\",\n",
       "   '    output_dir = \"PlanNet/site/markdown\" #NOTE: This is the same directory as the url to markdown converter above. Change to keep the results seprate. \\n',\n",
       "   '    os.makedirs(output_dir, exist_ok=True)\\n',\n",
       "   '    \\n',\n",
       "   '    # Process each HTML file\\n',\n",
       "   '    for html_file in html_files:\\n',\n",
       "   '        try:\\n',\n",
       "   '            # Load and convert the HTML file\\n',\n",
       "   '            loader = UnstructuredHTMLLoader(html_file)\\n',\n",
       "   '            doc = loader.load()[0]  # BSHTMLLoader returns a list\\n',\n",
       "   '            \\n',\n",
       "   '            # Transform to markdown\\n',\n",
       "   '            md_transformer = MarkdownifyTransformer()\\n',\n",
       "   '            converted_doc = md_transformer.transform_documents([doc])[0]\\n',\n",
       "   '            \\n',\n",
       "   '            # Generate output filename\\n',\n",
       "   '            output_filename = get_filename_from_html(html_file)\\n',\n",
       "   '            output_path = os.path.join(output_dir, output_filename)\\n',\n",
       "   '            \\n',\n",
       "   '            # Write content to file\\n',\n",
       "   \"            with open(output_path, 'w', encoding='utf-8') as f:\\n\",\n",
       "   '                f.write(converted_doc.page_content)\\n',\n",
       "   '            \\n',\n",
       "   '            print(f\"Successfully converted {html_file} to {output_filename}\")\\n',\n",
       "   '            \\n',\n",
       "   '        except Exception as e:\\n',\n",
       "   '            print(f\"Error processing {html_file}: {str(e)}\")']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# html_files = [\\n',\n",
       "   '#     \"PlanNet/site/CapabilityStatement-plan-net.html\",\\n',\n",
       "   '#     \"PlanNet/site/ChangeHistory.html\",\\n',\n",
       "   '#     \"PlanNet/site/index.html\",\\n',\n",
       "   '#     \"PlanNet/site/examples.html\",\\n',\n",
       "   '#     \"PlanNet/site/implementation.html\",\\n',\n",
       "   '#     \"PlanNet/site/profiles.html\",\\n',\n",
       "   '#     \"PlanNet/site/artifacts.html\"\\n',\n",
       "   '# ]']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# convert_html_to_markdown(html_files)']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 2,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['class ContextExtractor:\\n',\n",
       "   '    def __init__(self):\\n',\n",
       "   '        \"\"\"Initialize the context extractor\"\"\"\\n',\n",
       "   '        self.processed_elements = set()\\n',\n",
       "   '        \\n',\n",
       "   '    def _has_been_processed(self, element):\\n',\n",
       "   '        \"\"\"Check if an element has already been processed\"\"\"\\n',\n",
       "   '        if not isinstance(element, Tag):\\n',\n",
       "   '            return False\\n',\n",
       "   \"        return element.get('data-processed') == 'true'\\n\",\n",
       "   '    \\n',\n",
       "   '    def _mark_processed(self, element):\\n',\n",
       "   '        \"\"\"Mark an element as processed\"\"\"\\n',\n",
       "   '        if isinstance(element, Tag):\\n',\n",
       "   \"            element['data-processed'] = 'true'\\n\",\n",
       "   '            self.processed_elements.add(element)\\n',\n",
       "   '\\n',\n",
       "   '    def _extract_images(self, element):\\n',\n",
       "   '        \"\"\"\\n',\n",
       "   '        Extract image information including src and alt text\\n',\n",
       "   '        \\n',\n",
       "   '        Args:\\n',\n",
       "   '            element: BeautifulSoup element containing images\\n',\n",
       "   '        Returns:\\n',\n",
       "   '            list: Formatted image information in Markdown\\n',\n",
       "   '        \"\"\"\\n',\n",
       "   '        if self._has_been_processed(element):\\n',\n",
       "   '            return []\\n',\n",
       "   '            \\n',\n",
       "   '        images = []\\n',\n",
       "   \"        for img in element.find_all('img', recursive=False):\\n\",\n",
       "   \"            src = img.get('src', '')\\n\",\n",
       "   \"            alt = img.get('alt', '')\\n\",\n",
       "   '            if src:\\n',\n",
       "   '                # Format as Markdown image with additional source info\\n',\n",
       "   '                images.append(f\"![{alt}]({src})\")\\n',\n",
       "   '                images.append(f\"*Image source: {src}*\")\\n',\n",
       "   '                images.append(f\"*Image description: {alt}*\")\\n',\n",
       "   '                images.append(\"\")  # Add blank line after each image\\n',\n",
       "   '                \\n',\n",
       "   '        self._mark_processed(element)\\n',\n",
       "   '        return images\\n',\n",
       "   '\\n',\n",
       "   '    def _extract_list_items(self, list_element, level=0, parent_type=None):\\n',\n",
       "   '        \"\"\"Extract list items with improved nested list handling\"\"\"\\n',\n",
       "   '        if self._has_been_processed(list_element):\\n',\n",
       "   '            return []\\n',\n",
       "   '            \\n',\n",
       "   '        items = []\\n',\n",
       "   \"        for item in list_element.find_all('li', recursive=False):\\n\",\n",
       "   '            if not self._has_been_processed(item):\\n',\n",
       "   '                # Get direct text content of the li element (excluding nested list text)\\n',\n",
       "   \"                item_text = ''\\n\",\n",
       "   '                for content in item.children:\\n',\n",
       "   '                    if isinstance(content, Tag):\\n',\n",
       "   \"                        if content.name not in ['ul', 'ol']:\\n\",\n",
       "   \"                            if content.name == 'img':\\n\",\n",
       "   '                                # Handle images within list items\\n',\n",
       "   '                                image_info = self._extract_images(content.parent)\\n',\n",
       "   '                                items.extend([f\"{\\'    \\' * level}{line}\" for line in image_info])\\n',\n",
       "   '                            else:\\n',\n",
       "   \"                                item_text += content.get_text(strip=True) + ' '\\n\",\n",
       "   '                    else:\\n',\n",
       "   \"                        item_text += content.strip() + ' '\\n\",\n",
       "   '                item_text = item_text.strip()\\n',\n",
       "   '                \\n',\n",
       "   '                # Format the list item\\n',\n",
       "   \"                prefix = '    ' * level\\n\",\n",
       "   \"                if list_element.name == 'ol':\\n\",\n",
       "   '                    items.append(f\"{prefix}1. {item_text}\")\\n',\n",
       "   '                else:\\n',\n",
       "   '                    items.append(f\"{prefix}- {item_text}\")\\n',\n",
       "   '                \\n',\n",
       "   '                # Handle nested lists\\n',\n",
       "   \"                nested_lists = item.find_all(['ul', 'ol'], recursive=False)\\n\",\n",
       "   '                for nested_list in nested_lists:\\n',\n",
       "   '                    nested_items = []\\n',\n",
       "   \"                    for nested_item in nested_list.find_all('li', recursive=False):\\n\",\n",
       "   '                        nested_text = nested_item.get_text(strip=True)\\n',\n",
       "   \"                        if nested_list.name == 'ol':\\n\",\n",
       "   '                            nested_items.append(f\"{prefix}    * {nested_text}\")\\n',\n",
       "   '                        else:\\n',\n",
       "   '                            nested_items.append(f\"{prefix}    * {nested_text}\")\\n',\n",
       "   '                    items.extend(nested_items)\\n',\n",
       "   '                \\n',\n",
       "   '                self._mark_processed(item)\\n',\n",
       "   '        \\n',\n",
       "   '        self._mark_processed(list_element)\\n',\n",
       "   '        return items\\n',\n",
       "   '\\n',\n",
       "   '    def _extract_table(self, table):\\n',\n",
       "   '        \"\"\"Extract table content in Markdown format\"\"\"\\n',\n",
       "   '        if self._has_been_processed(table):\\n',\n",
       "   '            return \"\"\\n',\n",
       "   '            \\n',\n",
       "   '        rows = []\\n',\n",
       "   '        headers = []\\n',\n",
       "   '        \\n',\n",
       "   \"        header_row = table.find('thead') or table.find('tr')\\n\",\n",
       "   '        if header_row:\\n',\n",
       "   \"            headers = [cell.get_text(strip=True) for cell in header_row.find_all(['th', 'td'])]\\n\",\n",
       "   '        \\n',\n",
       "   \"        for row in table.find_all('tr')[1:] if headers else table.find_all('tr'):\\n\",\n",
       "   \"            row_data = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\\n\",\n",
       "   '            if any(row_data):\\n',\n",
       "   '                rows.append(row_data)\\n',\n",
       "   '        \\n',\n",
       "   '        table_str = []\\n',\n",
       "   '        if headers:\\n',\n",
       "   '            table_str.append(\"| \" + \" | \".join(headers) + \" |\")\\n',\n",
       "   '            table_str.append(\"|\" + \"|\".join([\" --- \" for _ in headers]) + \"|\")\\n',\n",
       "   '        \\n',\n",
       "   '        for row in rows:\\n',\n",
       "   '            if headers:\\n',\n",
       "   \"                row.extend([''] * (len(headers) - len(row)))\\n\",\n",
       "   '            table_str.append(\"| \" + \" | \".join(row) + \" |\")\\n',\n",
       "   '        \\n',\n",
       "   '        self._mark_processed(table)\\n',\n",
       "   '        return \"\\\\n\".join(table_str)\\n',\n",
       "   '\\n',\n",
       "   '    def extract_context(self, html_content):\\n',\n",
       "   '        \"\"\"Extract content with improved list and image handling\"\"\"\\n',\n",
       "   \"        soup = BeautifulSoup(html_content, 'html.parser')\\n\",\n",
       "   '        self.processed_elements.clear()\\n',\n",
       "   '        context_elements = []\\n',\n",
       "   '        \\n',\n",
       "   '        # Remove script and style elements\\n',\n",
       "   \"        for script in soup(['script', 'style', 'nav', 'footer']):\\n\",\n",
       "   '            script.decompose()\\n',\n",
       "   '        \\n',\n",
       "   '        # Process headers and their content\\n',\n",
       "   \"        for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\\n\",\n",
       "   '            if not self._has_been_processed(header):\\n',\n",
       "   '                level = int(header.name[1])\\n',\n",
       "   '                header_text = header.get_text().strip()\\n',\n",
       "   '                \\n',\n",
       "   '                if header_text:\\n',\n",
       "   '                    context_elements.append(f\"\\\\n{\\'#\\' * level} {header_text}\\\\n\")\\n',\n",
       "   '                    self._mark_processed(header)\\n',\n",
       "   '                    \\n',\n",
       "   '                    # Process content until next header\\n',\n",
       "   '                    next_element = header.find_next()\\n',\n",
       "   \"                    while next_element and not next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\\n\",\n",
       "   '                        if not self._has_been_processed(next_element):\\n',\n",
       "   \"                            if next_element.name == 'p':\\n\",\n",
       "   '                                # Handle images within paragraphs\\n',\n",
       "   \"                                if next_element.find('img'):\\n\",\n",
       "   '                                    image_info = self._extract_images(next_element)\\n',\n",
       "   '                                    context_elements.extend(image_info)\\n',\n",
       "   '                                else:\\n',\n",
       "   '                                    text = next_element.get_text().strip()\\n',\n",
       "   '                                    if text:\\n',\n",
       "   '                                        context_elements.append(text)\\n',\n",
       "   '                                        context_elements.append(\"\")\\n',\n",
       "   \"                            elif next_element.name in ['ul', 'ol']:\\n\",\n",
       "   '                                list_items = self._extract_list_items(next_element)\\n',\n",
       "   '                                if list_items:\\n',\n",
       "   '                                    context_elements.extend(list_items)\\n',\n",
       "   '                                    context_elements.append(\"\")\\n',\n",
       "   \"                            elif next_element.name == 'table':\\n\",\n",
       "   '                                table_content = self._extract_table(next_element)\\n',\n",
       "   '                                if table_content:\\n',\n",
       "   '                                    context_elements.append(table_content)\\n',\n",
       "   '                                    context_elements.append(\"\")\\n',\n",
       "   '                        \\n',\n",
       "   '                        next_element = next_element.find_next()\\n',\n",
       "   '\\n',\n",
       "   '        # Process any remaining top-level images\\n',\n",
       "   \"        for img_container in soup.find_all('p'):\\n\",\n",
       "   \"            if img_container.find('img') and not self._has_been_processed(img_container):\\n\",\n",
       "   '                image_info = self._extract_images(img_container)\\n',\n",
       "   '                if image_info:\\n',\n",
       "   '                    context_elements.extend(image_info)\\n',\n",
       "   '        \\n',\n",
       "   '        # Clean up repeated empty lines\\n',\n",
       "   '        cleaned_elements = []\\n',\n",
       "   '        prev_empty = False\\n',\n",
       "   '        for element in context_elements:\\n',\n",
       "   '            if element.strip() == \"\":\\n',\n",
       "   '                if not prev_empty:\\n',\n",
       "   '                    cleaned_elements.append(element)\\n',\n",
       "   '                    prev_empty = True\\n',\n",
       "   '            else:\\n',\n",
       "   '                cleaned_elements.append(element)\\n',\n",
       "   '                prev_empty = False\\n',\n",
       "   '        \\n',\n",
       "   '        return cleaned_elements\\n',\n",
       "   '\\n',\n",
       "   '    def save_context(self, context_elements, output_file):\\n',\n",
       "   '        \"\"\"Save context to Markdown file\"\"\"\\n',\n",
       "   \"        with open(output_file, 'w', encoding='utf-8') as f:\\n\",\n",
       "   '            for element in context_elements:\\n',\n",
       "   '                f.write(f\"{element}\\\\n\")\\n',\n",
       "   '\\n',\n",
       "   '    def process_html_file(self, input_file, output_file):\\n',\n",
       "   '        \"\"\"Process HTML file to Markdown\"\"\"\\n',\n",
       "   '        try:\\n',\n",
       "   \"            output_file = os.path.splitext(output_file)[0] + '.md'\\n\",\n",
       "   '            \\n',\n",
       "   \"            with open(input_file, 'r', encoding='utf-8') as f:\\n\",\n",
       "   '                html_content = f.read()\\n',\n",
       "   '            \\n',\n",
       "   '            context_elements = self.extract_context(html_content)\\n',\n",
       "   '            self.save_context(context_elements, output_file)\\n',\n",
       "   '            self._display_summary(input_file, output_file, len(context_elements))\\n',\n",
       "   '            \\n',\n",
       "   '            return context_elements\\n',\n",
       "   '            \\n',\n",
       "   '        except Exception as e:\\n',\n",
       "   '            display(HTML(f\\'<div style=\"color: red;\">Error processing file: {str(e)}</div>\\'))\\n',\n",
       "   '            return []\\n',\n",
       "   '\\n',\n",
       "   '    def process_directory(self, input_dir, output_dir):\\n',\n",
       "   '        \"\"\"Process directory of HTML files\"\"\"\\n',\n",
       "   '        Path(output_dir).mkdir(parents=True, exist_ok=True)\\n',\n",
       "   '        \\n',\n",
       "   \"        for file in Path(input_dir).glob('*.html'):\\n\",\n",
       "   '            output_file = Path(output_dir) / f\"{file.stem}.md\"\\n',\n",
       "   '            self.process_html_file(str(file), str(output_file))\\n',\n",
       "   '\\n',\n",
       "   '    def _display_summary(self, input_file, output_file, num_elements):\\n',\n",
       "   '        \"\"\"Display processing summary\"\"\"\\n',\n",
       "   '        summary_html = f\"\"\"\\n',\n",
       "   '        <div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px; margin: 10px 0;\">\\n',\n",
       "   '            <p><strong>Processed file:</strong> {os.path.basename(input_file)}</p>\\n',\n",
       "   '            <p><strong>Output saved to:</strong> {os.path.basename(output_file)}</p>\\n',\n",
       "   '            <p><strong>Extracted elements:</strong> {num_elements}</p>\\n',\n",
       "   '        </div>\\n',\n",
       "   '        \"\"\"\\n',\n",
       "   '        display(HTML(summary_html))']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# extractor = ContextExtractor()\\n',\n",
       "   '# context = extractor.process_html_file(\\n',\n",
       "   \"#     input_file='PlanNet/site/index.html',\\n\",\n",
       "   \"#     output_file='context'\\n\",\n",
       "   '# )']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# for i, element in enumerate(context[:5]):  # Show first 5 elements\\n',\n",
       "   '#     print(f\"\\\\nElement {i+1}:\")\\n',\n",
       "   '#     print(element[:200] + \"...\" if len(element) > 200 else element)']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# # Directory containing your HTML files\\n',\n",
       "   \"# input_dir = 'PlanNet/site'\"]},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# # Process all HTML files in the input directory\\n',\n",
       "   \"# for html_file in Path(input_dir).glob('**/*.html'):\\n\",\n",
       "   '#     # Create corresponding output path while preserving directory structure\\n',\n",
       "   '#     relative_path = html_file.relative_to(input_dir)\\n',\n",
       "   \"#     output_path = Path(output_dir) / relative_path.with_suffix('.md')\\n\",\n",
       "   '    \\n',\n",
       "   '#     # Create necessary subdirectories\\n',\n",
       "   '#     output_path.parent.mkdir(parents=True, exist_ok=True)\\n',\n",
       "   '    \\n',\n",
       "   '#     # Process the file\\n',\n",
       "   '#     context = extractor.process_html_file(\\n',\n",
       "   '#         input_file=str(html_file),\\n',\n",
       "   '#         output_file=str(output_path)\\n',\n",
       "   '#     )']}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x['cell_type'] == 'code', src['cells']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e951fd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 6/6 [00:00<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: components.md\n",
      "Created: conformance.md\n",
      "Created: OperationDefinition_backport_subscription_get_ws_binding_token.md\n",
      "Created: OperationDefinition_backport_subscription_events.md\n",
      "Created: Bundle_r4_notification_empty.md\n",
      "Created: CapabilityStatement_backport_subscription_server_r4.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://hl7.org/fhir/uv/subscriptions-backport/STU1.1/components.html\",\n",
    "    \"https://hl7.org/fhir/uv/subscriptions-backport/STU1.1/conformance.html\",\n",
    "    \"https://hl7.org/fhir/uv/subscriptions-backport/STU1.1/OperationDefinition-backport-subscription-get-ws-binding-token.html\", # negative\n",
    "    \"https://hl7.org/fhir/uv/subscriptions-backport/STU1.1/OperationDefinition-backport-subscription-events.html\",\n",
    "    \"https://hl7.org/fhir/uv/subscriptions-backport/STU1.1/Bundle-r4-notification-empty.html\", # negative\n",
    "    \"https://hl7.org/fhir/uv/subscriptions-backport/STU1.1/CapabilityStatement-backport-subscription-server-r4.html\"\n",
    "]\n",
    "\n",
    "HTMLextract.convert_urls_to_markdown(urls, output_dir=\"text_extraction/uv_subscriptions_backport/markdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0340e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 7/7 [00:00<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: index.md\n",
      "Created: ChangeHistory.md\n",
      "Created: examples.md\n",
      "Created: implementation.md\n",
      "Created: profiles.md\n",
      "Created: artifacts.md\n",
      "Created: CapabilityStatement_plan_net.md\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/index.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/ChangeHistory.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/examples.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/implementation.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/profiles.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/artifacts.html\",\n",
    "    \"https://hl7.org/fhir/us/davinci-pdex-plan-net/CapabilityStatement-plan-net.html\"\n",
    "]\n",
    "\n",
    "HTMLextract.convert_urls_to_markdown(urls, output_dir=\"text_extraction/PlanNet/site/markdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0108ece",
   "metadata": {},
   "source": [
    "### Markdown Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec7806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from markdown_cleaner_pipeline.ipynb\n",
      "Found 6 markdown files in text_extraction/uv_subscriptions_backport/markdown/\n",
      "Cleaned and saved: checkpoints/post_processing/uv_subscriptions_backport/conformance.md\n",
      "Cleaned and saved: checkpoints/post_processing/uv_subscriptions_backport/OperationDefinition_backport_subscription_events.md\n",
      "Cleaned and saved: checkpoints/post_processing/uv_subscriptions_backport/OperationDefinition_backport_subscription_get_ws_binding_token.md\n",
      "Cleaned and saved: checkpoints/post_processing/uv_subscriptions_backport/components.md\n",
      "Cleaned and saved: checkpoints/post_processing/uv_subscriptions_backport/Bundle_r4_notification_empty.md\n",
      "Cleaned and saved: checkpoints/post_processing/uv_subscriptions_backport/CapabilityStatement_backport_subscription_server_r4.md\n",
      "\n",
      "Processing complete: 6 files successfully cleaned, 0 failed\n"
     ]
    }
   ],
   "source": [
    "import markdown_cleaner_pipeline\n",
    "\n",
    "markdown_cleaner_pipeline.process_directory(\"text_extraction/uv_subscriptions_backport/markdown/\", \"checkpoints/post_processing/uv_subscriptions_backport/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3fa2d1",
   "metadata": {},
   "source": [
    "## Requirements Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0566ab",
   "metadata": {},
   "source": [
    "### Prompt-based Requirement Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fd5bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from reqs_extraction_pipeline.ipynb\n"
     ]
    }
   ],
   "source": [
    "import reqs_extraction_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31999d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting processing with claude on directory: checkpoints/post_processing\n",
      "INFO:root:Found 7 markdown files:\n",
      "INFO:root:  - implementation.md\n",
      "INFO:root:  - examples.md\n",
      "INFO:root:  - profiles.md\n",
      "INFO:root:  - ChangeHistory.md\n",
      "INFO:root:  - artifacts.md\n",
      "INFO:root:  - index.md\n",
      "INFO:root:  - CapabilityStatement_plan_net.md\n",
      "INFO:root:Organized 7 files into 6 processing groups\n",
      "INFO:root:Processing combined group of 2 files\n",
      "INFO:root:Split combined content into 2 chunks\n",
      "INFO:root:Processing chunk 1/2 of combined files\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 2/2 of combined files\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing single file: examples.md\n",
      "INFO:root:Split examples.md into 3 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/3 of examples.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.398195 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 2/3 of examples.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 3/3 of examples.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing single file: implementation.md\n",
      "INFO:root:Split implementation.md into 2 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/2 of implementation.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 2/2 of implementation.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing single file: CapabilityStatement_plan_net.md\n",
      "INFO:root:Split CapabilityStatement_plan_net.md into 4 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/4 of CapabilityStatement_plan_net.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 2/4 of CapabilityStatement_plan_net.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 3/4 of CapabilityStatement_plan_net.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 4/4 of CapabilityStatement_plan_net.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing single file: index.md\n",
      "INFO:root:Split index.md into 9 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 2/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 3/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 4/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 5/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 6/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 7/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 8/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 9/9 of index.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing single file: artifacts.md\n",
      "INFO:root:Split artifacts.md into 7 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/7 of artifacts.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.423501 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.948690 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 2/7 of artifacts.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 3/7 of artifacts.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 4/7 of artifacts.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.464207 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 5/7 of artifacts.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 6/7 of artifacts.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processing chunk 7/7 of artifacts.md\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Completed processing 7 files\n",
      "INFO:root:Generated requirements document saved to checkpoints/requirements_extraction/claude_reqs_list_v1_20250429_081756.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'processed_files': ['profiles.md',\n",
       "  'ChangeHistory.md',\n",
       "  'examples.md',\n",
       "  'implementation.md',\n",
       "  'CapabilityStatement_plan_net.md',\n",
       "  'index.md',\n",
       "  'artifacts.md'],\n",
       " 'srs_document': 'This content does not contain any explicit conformance language (SHALL, SHOULD, MAY, MUST, etc.) or testable requirements. The text describes profiles and change history but does not specify conformance requirements. Therefore, no INCOSE-style requirements can be extracted from this content.This content appears to be a change log and version history document that does not contain any explicit, testable requirements with conformance language (SHALL, SHOULD, MAY, MUST, etc.). While it references various changes and updates that were made across different versions, it does not specify new requirements - rather it documents the changes that were implemented.\\n\\nTherefore, I do not have any INCOSE-style requirements to extract from this particular content section.This content contains no explicit conformance language (SHALL, SHOULD, MAY, MUST, etc.) or testable requirements. The provided text is descriptive/informative content about examples and versioning rather than normative requirements.This content appears to be a reference table of examples mapping resource instances to their types and profiles. While it provides important implementation context, it does not contain any explicit conformance requirements (SHALL, SHOULD, MAY, etc.) that can be extracted into testable INCOSE-style requirements.\\n\\nAs there are no explicit requirements to extract from this examples table, I will not generate any requirements for this chunk. The table serves as reference material rather than normative requirements content.This appears to be an examples/artifacts listing page from a FHIR Implementation Guide that only contains links to example resources and profiles. It does not contain any normative requirements with conformance language (SHALL, SHOULD, MAY, etc.) that could be extracted into INCOSE-style requirements.\\n\\nThis content consists solely of:\\n- Links to example Location resources\\n- Links to example Organization resources  \\n- Links to example OrganizationAffiliation resources\\n- Links to example Practitioner resources\\n- Links to example PractitionerRole resources\\n- Links to their base FHIR resource types\\n- Links to their profiles\\n\\nSince there are no testable requirements to extract, I am not generating any INCOSE-formatted requirements from this content.## REQ-01\\n\\n**Summary**: No authentication for directory access\\n**Description**: \"Access to the Plan-Net service should not require authentication, and the server should not maintain any records that could associate the consumer with the entities that were queried.\"\\n**Verification**: Inspection\\n**Notes**: Actor: Server, Conformance: SHOULD, Conditional: False\\n**Source**: Privacy Considerations Section\\n\\n## REQ-02\\n\\n**Summary**: No consumer identification required\\n**Description**: \"A conformant Plan-Net service SHALL NOT require a directory mobile application to send consumer identifying information in order to query content.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL NOT, Conditional: False\\n**Source**: Privacy Considerations Section\\n\\n## REQ-03\\n\\n**Summary**: No consumer information sent in queries\\n**Description**: \"A directory mobile application SHALL NOT send consumer identifiable information when querying a Plan-Net service.\"\\n**Verification**: Test\\n**Notes**: Actor: Client Application, Conformance: SHALL NOT, Conditional: False\\n**Source**: Privacy Considerations Section\\n\\n## REQ-04\\n\\n**Summary**: LastUpdate timestamp inclusion\\n**Description**: \"Each profile in this guide requires that the lastUpdate timestamp be provided as part of the profile\\'s data content.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by \"requires\"), Conditional: False\\n**Source**: Client Detection of Updates Directory Content Section## REQ-01\\n\\n**Summary**: Optional Bulk Data Support\\n**Description**: \"A server MAY support [Bulk Data IG] for the retrieval of directory data.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: MAY, Conditional: False\\n**Source**: Bulk Data Section## REQ-01\\n\\n**Summary**: Server Mandatory Profile Support\\n**Description**: \"The Plan-Net Server SHALL Support all profiles defined in this Implementation Guide.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: FHIR RESTful Capabilities Section\\n\\n## REQ-02\\n\\n**Summary**: RESTful Behavior Implementation\\n**Description**: \"The Plan-Net Server SHALL Implement the RESTful behavior according to the FHIR specification.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: FHIR RESTful Capabilities Section\\n\\n## REQ-03\\n\\n**Summary**: Response Class Support\\n**Description**: \"The Plan-Net Server SHALL Return the following response classes: (Status 400): invalid parameter, (Status 401/4xx): unauthorized request, (Status 403): insufficient scope, (Status 404): unknown resource, (Status 410): deleted resource.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: FHIR RESTful Capabilities Section\\n\\n## REQ-04\\n\\n**Summary**: JSON Format Support\\n**Description**: \"The Plan-Net Server SHALL Support json source formats for all Plan-Net interactions.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: FHIR RESTful Capabilities Section\\n\\n## REQ-05\\n\\n**Summary**: Search Parameter Support\\n**Description**: \"The Plan-Net Server SHALL Support the searchParameters on each profile individually and in combination.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: FHIR RESTful Capabilities Section\\n\\n## REQ-06\\n\\n**Summary**: XML Format Support\\n**Description**: \"The Plan-Net Server SHOULD Support xml source formats for all Plan-Net interactions.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHOULD, Conditional: False\\n**Source**: FHIR RESTful Capabilities Section\\n\\n## REQ-07\\n\\n**Summary**: Unauthorized Request Handling\\n**Description**: \"A server SHALL reject any unauthorized requests by returning an HTTP 401 unauthorized response code.\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Security Section\\n\\n## REQ-08\\n\\n**Summary**: Endpoint Resource Support\\n**Description**: \"A Server SHALL be capable of returning a Endpoint resource using: GET [base]/Endpoint/[id]\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Endpoint Section\\n\\n## REQ-09\\n\\n**Summary**: Endpoint Search Support\\n**Description**: \"A Server SHALL be capable of supporting the following _includes: Endpoint:organization - GET [base]/Endpoint?[parameter=value]&_include=Endpoint:organization\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Endpoint Section## REQ-1\\n**Summary**: Support HealthcareService basic interactions\\n**Description**: \"Server SHALL support search-type, read\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: HealthcareService Profile Interaction Summary\\n\\n## REQ-2\\n**Summary**: Support HealthcareService fetch by ID\\n**Description**: \"A Server SHALL be capable of returning a HealthcareService resource using: GET [base]/HealthcareService/[id]\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: HealthcareService Fetch and Search Criteria\\n\\n## REQ-3\\n**Summary**: Support HealthcareService includes\\n**Description**: \"A Server SHALL be capable of supporting the following _includes: HealthcareService:location, HealthcareService:coverage-area, HealthcareService:organization, HealthcareService:endpoint\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: HealthcareService Fetch and Search Criteria\\n\\n## REQ-4\\n**Summary**: Support HealthcareService reverse includes\\n**Description**: \"A Server SHALL be capable of supporting the following _revincludes: PractitionerRole:service, OrganizationAffiliation:service\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: HealthcareService Fetch and Search Criteria\\n\\n## REQ-5\\n**Summary**: Support InsurancePlan basic interactions\\n**Description**: \"Server SHALL support search-type, read\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: InsurancePlan Profile Interaction Summary\\n\\n## REQ-6\\n**Summary**: Support InsurancePlan fetch by ID\\n**Description**: \"A Server SHALL be capable of returning a InsurancePlan resource using: GET [base]/InsurancePlan/[id]\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: InsurancePlan Fetch and Search Criteria\\n\\n## REQ-7\\n**Summary**: Support InsurancePlan includes\\n**Description**: \"A Server SHALL be capable of supporting the following _includes: InsurancePlan:administered-by, InsurancePlan:owned-by, InsurancePlan:coverage-area\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: InsurancePlan Fetch and Search Criteria\\n\\n## REQ-8\\n**Summary**: Support Location basic interactions\\n**Description**: \"Server SHALL support search-type, read\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: Location Profile Interaction Summary\\n\\n## REQ-9\\n**Summary**: Support Location fetch by ID\\n**Description**: \"A Server SHALL be capable of returning a Location resource using: GET [base]/Location/[id]\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: Location Fetch and Search Criteria\\n\\n## REQ-10\\n**Summary**: Support Location includes\\n**Description**: \"A Server SHALL be capable of supporting the following _includes: Location:endpoint, Location:organization, Location:partof\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: Location Fetch and Search Criteria\\n\\n## REQ-11\\n**Summary**: Support Location reverse includes\\n**Description**: \"A Server SHALL be capable of supporting the following _revincludes: HealthcareService:location, InsurancePlan:coverage-area, OrganizationAffiliation:location, PractitionerRole:location\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: Location Fetch and Search Criteria## REQ-1\\n\\n**Summary**: Support Organization Resource Read Operation\\n**Description**: \"A Server SHALL be capable of returning a Organization resource using: GET [base]/Organization/[id]\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Organization Section, Fetch and Search Criteria\\n\\n## REQ-2\\n\\n**Summary**: Support Organization Search Operation\\n**Description**: \"Server SHALL support search-type, read\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Organization Section, Profile Interaction Summary\\n\\n## REQ-3\\n\\n**Summary**: Support Organization Search Parameters\\n**Description**: \"Server SHALL support the following search parameters: partof, endpoint, address, name, _id, _lastUpdated, type, coverage-area\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Organization Section, Search Parameter Summary\\n\\n## REQ-4\\n\\n**Summary**: Support Organization Resource Includes\\n**Description**: \"A Server SHALL be capable of supporting the following _includes: Organization:partof, Organization:endpoint, Organization:coverage-area\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Organization Section, Fetch and Search Criteria\\n\\n## REQ-5\\n\\n**Summary**: Support Organization Resource Reverse Includes\\n**Description**: \"A Server SHALL be capable of supporting the following _revincludes: Endpoint:organization, HealthcareService:organization, InsurancePlan:administered-by, InsurancePlan:owned-by, OrganizationAffiliation:primary-organization, PractitionerRole:organization, PractitionerRole:network, OrganizationAffiliation:participating-organization\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Organization Section, Fetch and Search Criteria\\n\\n## REQ-6\\n\\n**Summary**: Support OrganizationAffiliation Resource Operations\\n**Description**: \"Server SHALL support search-type, read\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: OrganizationAffiliation Section, Profile Interaction Summary\\n\\n## REQ-7\\n\\n**Summary**: Support OrganizationAffiliation Resource Read\\n**Description**: \"A Server SHALL be capable of returning a OrganizationAffiliation resource using: GET [base]/OrganizationAffiliation/[id]\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: OrganizationAffiliation Section, Fetch and Search Criteria\\n\\n## REQ-8\\n\\n**Summary**: Support OrganizationAffiliation Search Parameters\\n**Description**: \"Server SHALL support the following search parameters: primary-organization, participating-organization, location, service, network, endpoint, role, specialty, _id, _lastUpdated\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: OrganizationAffiliation Section, Search Parameter Summary\\n\\n## REQ-9\\n\\n**Summary**: Support OrganizationAffiliation Resource Includes\\n**Description**: \"A Server SHALL be capable of supporting the following _includes: OrganizationAffiliation:primary-organization, OrganizationAffiliation:participating-organization, OrganizationAffiliation:location, OrganizationAffiliation:service, OrganizationAffiliation:endpoint, OrganizationAffiliation:network\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: OrganizationAffiliation Section, Fetch and Search Criteria\\n\\n## REQ-10\\n\\n**Summary**: Support Practitioner Resource Operations\\n**Description**: \"Server SHALL support search-type, read\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Practitioner Section, Profile Interaction Summary\\n\\n## REQ-11\\n\\n**Summary**: Support Practitioner Resource Read\\n**Description**: \"A Server SHALL be capable of returning a Practitioner resource using: GET [base]/Practitioner/[id]\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Practitioner Section, Fetch and Search Criteria\\n\\n## REQ-12\\n\\n**Summary**: Support Practitioner Search Parameters\\n**Description**: \"Server SHALL support the following search parameters: name, _id, _lastUpdated, family, given\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Practitioner Section, Search Parameter Summary\\n\\n## REQ-13\\n\\n**Summary**: Support Practitioner Resource Reverse Includes\\n**Description**: \"A Server SHALL be capable of supporting the following _revincludes: PractitionerRole:practitioner\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\\n**Source**: Practitioner Section, Fetch and Search Criteria## REQ-1\\n\\n**Summary**: Support basic PractitionerRole interactions\\n**Description**: \"A Server SHALL support search-type, read\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: PractitionerRole Profile Interaction Summary\\n\\n## REQ-2\\n\\n**Summary**: Return PractitionerRole by ID\\n**Description**: \"A Server SHALL be capable of returning a PractitionerRole resource using: GET [base]/PractitionerRole/[id]\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: PractitionerRole Fetch and Search Criteria\\n\\n## REQ-3\\n\\n**Summary**: Support PractitionerRole includes\\n**Description**: \"A Server SHALL be capable of supporting the following _includes:\\nPractitionerRole:practitioner\\nPractitionerRole:organization\\nPractitionerRole:location\\nPractitionerRole:service\\nPractitionerRole:network\\nPractitionerRole:endpoint\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: PractitionerRole Fetch and Search Criteria\\n\\n## REQ-4\\n\\n**Summary**: Support mandatory search parameters\\n**Description**: \"Server SHALL support the following search parameters: practitioner, organization, location, service, network, endpoint, role, specialty, _id, _lastUpdated\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL\\n**Source**: PractitionerRole Search Parameter Summary\\n\\n## REQ-5\\n\\n**Summary**: Support version read\\n**Description**: \"A Server SHOULD be capable of returning a PractitionerRole resource using: GET [base]/PractitionerRole/[id]/_history/vid\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHOULD\\n**Source**: PractitionerRole Fetch and Search Criteria\\n\\n## REQ-6\\n\\n**Summary**: Support vread interaction\\n**Description**: \"Server SHOULD support vread\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHOULD\\n**Source**: PractitionerRole Profile Interaction Summary## REQ-01\\n\\n**Summary**: Base resource profiles on US Core where available\\n**Description**: \"This implementation guide... profiles resources from US Core STU 4, where available (Practitioner, Organization and Location), and otherwise from R4 (OrganizationAffiliation, PractitionerRole, HealthCareService, Endpoint)\"\\n**Verification**: Inspection\\n**Notes**: Actor: Implementation Guide/Profile Authors, Conformance: SHALL, Conditional: False  \\n**Source**: Relation to US Core and other IGs Section\\n\\n## REQ-02\\n\\n**Summary**: Query-only API capability\\n**Description**: \"This is a query only API (GET) and does not support PUT or POST\"\\n**Verification**: Test\\n**Notes**: Actor: Health Plan API Actor, Conformance: SHALL, Conditional: False\\n**Source**: Introduction Section\\n\\n## REQ-03\\n\\n**Summary**: Endpoint discovery out of scope\\n**Description**: \"This implementation guide assumes that the directory endpoint is known to the client. There is an overarching system architecture issue that is critical to resolve -- how does the client discover the FHIR endpoint of interest. For the purposes of this IG, we consider that problem out of scope.\"\\n**Verification**: Inspection\\n**Notes**: Actor: Application Actor, Conformance: SHALL, Conditional: False\\n**Source**: Disclaimers and Assumptions Section\\n```This chunk of the Implementation Guide contains no testable requirements - it is acknowledgement and credits text only. Moving to next chunk.This content chunk contains only dependency declarations and package version information without any explicit conformance statements or testable requirements. There are no INCOSE-style requirements to extract from this section.This content appears to be a dependency list of FHIR packages and implementation guides. While it provides important contextual information about required dependencies, it does not contain any explicit, testable requirements with conformance language (SHALL, SHOULD, MAY, etc.). No INCOSE-style requirements can be extracted from this particular content section.This content does not contain any explicit testable requirements - it appears to be a list of package dependencies and their descriptions without any SHALL, SHOULD, MAY or other conformance language. No INCOSE-style requirements can be extracted from this text.This content chunk contains only informational text about licensing and intellectual property rights. It does not contain any explicit conformance language or testable requirements that could be formatted according to INCOSE standards. There are no requirements to extract from this section.From the provided content chunk, I do not find any specific testable requirements containing explicit conformance language (SHALL, SHOULD, MAY, MUST, etc.). The content appears to be a list of links and references without stating any explicit requirements.This content appears to be a metadata listing of code systems, value sets, and extensions without any actual requirements text containing conformance language (SHALL, SHOULD, MAY, etc.). There are no explicit requirements to extract and format according to the INCOSE style specified.This section appears to be a terminology reference list showing code systems, value sets and structure definitions used in the implementation guide. It does not contain any explicit conformance requirements (SHALL, SHOULD, MAY, etc.) that could be formatted into testable requirements. There are no specific behavioral or functional requirements to extract in INCOSE format from this content.## REQ-1\\n\\n**Summary**: Declare conformance to capability statements\\n**Description**: \"Systems conforming to this implementation guide are expected to declare conformance to one or more of the following capability statements.\"\\n**Verification**: Test\\n**Notes**: Actor: All Systems, Conformance: SHALL (implied by \"expected\"), Conditional: False \\n**Source**: Behavior: Capability Statements section\\n\\n## REQ-2\\n\\n**Summary**: Implement Plan-Net Server capabilities\\n**Description**: \"The Plan-Net Server actor [...] is responsible for providing responses to the queries submitted by the Plan-Net Requestors. Systems implementing this capability statement should meet the CMS Final Rule requirement for provider directory access.\"\\n**Verification**: Test\\n**Notes**: Actor: Plan-Net Server, Conformance: SHALL (implied by \"responsible\"), Conditional: False\\n**Source**: Behavior: Capability Statements section## REQ-1\\n\\n**Summary**: Support Endpoint Organization Search Parameter\\n**Description**: \"Select Endpoints managed by the specified organization\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/endpoint-organization\\n\\n## REQ-2\\n\\n**Summary**: Support HealthcareService Category Search Parameter\\n**Description**: \"Select HealthcareServices providing the specified category of services\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-service-category\\n\\n## REQ-3\\n\\n**Summary**: Support HealthcareService Coverage Area Search Parameter\\n**Description**: \"Select services available in a region described by the specified location\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-coverage-area\\n\\n## REQ-4\\n\\n**Summary**: Support HealthcareService Delivery Method Search Parameter\\n**Description**: \"Select HealthcareServices based on the delivery method type\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-delivery-method\\n\\n## REQ-5\\n\\n**Summary**: Support HealthcareService Endpoint Search Parameter\\n**Description**: \"Select HealthcareServices with the specified endpoint\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-endpoint\\n\\n## REQ-6\\n\\n**Summary**: Support HealthcareService Location Search Parameter\\n**Description**: \"Select HealthcareServices available at the specified location\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-location\\n\\n## REQ-7\\n\\n**Summary**: Support HealthcareService Name Search Parameter\\n**Description**: \"Select HealthcareServices with the specified name\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-name\\n\\n## REQ-8\\n\\n**Summary**: Support HealthcareService Organization Search Parameter\\n**Description**: \"Select HealthcareServices provided by the specified organization\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-organization\\n\\n## REQ-9\\n\\n**Summary**: Support HealthcareService Specialty Search Parameter\\n**Description**: \"Select services associated with the specified specialty\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-specialty\\n\\n## REQ-10\\n\\n**Summary**: Support HealthcareService Type Search Parameter\\n**Description**: \"Select HealthcareServices of the specified type\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/healthcareservice-service-type\\n\\n## REQ-11\\n\\n**Summary**: Support InsurancePlan Administered-by Search Parameter\\n**Description**: \"Select products that are administered by the specified organization\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/insuranceplan-administered-by\\n\\n## REQ-12\\n\\n**Summary**: Support InsurancePlan Coverage Area Search Parameter\\n**Description**: \"Select products that are offered in the specified location\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/insuranceplan-coverage-area\\n\\n## REQ-13\\n\\n**Summary**: Support InsurancePlan Identifier Search Parameter\\n**Description**: \"Select products with the specified identifier\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/insuranceplan-identifier\\n\\n## REQ-14\\n\\n**Summary**: Support InsurancePlan Name Search Parameter\\n**Description**: \"Select products with the specified name\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/insuranceplan-name\\n\\n## REQ-15\\n\\n**Summary**: Support InsurancePlan Network Search Parameter\\n**Description**: \"Select Organization entries in a given InsurancePlan\\'s network\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/insuranceplan-network\\n\\n## REQ-16\\n\\n**Summary**: Support InsurancePlan Owned-by Search Parameter\\n**Description**: \"Select products that are owned by the specified organization\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/insuranceplan-owned-by\\n\\n## REQ-17\\n\\n**Summary**: Support InsurancePlan Plan-type Search Parameter\\n**Description**: \"Select plans of the specified type\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/insuranceplan-plan-type\\n\\n## REQ-18\\n\\n**Summary**: Support InsurancePlan Type Search Parameter\\n**Description**: \"Select insurance plans of the specified type\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/insuranceplan-type\\n\\n## REQ-19\\n\\n**Summary**: Support Location Address Search Parameter\\n**Description**: \"Select Locations with the specified address\"\\n**Verification**: Test\\n**Notes**: Actor: Server, Conformance: SHALL (implied by SearchParameter definition), Conditional: False\\n**Source**: SearchParameter/location-addressBased on the provided content, this appears to be a list of search parameter definitions without explicit conformance statements (SHALL, SHOULD, MAY, etc.) or testable requirements. The content defines capabilities for searching different resource types but does not specify requirements about their implementation or usage.\\n\\nSince there are no explicit, testable requirements that meet the INCOSE format criteria in this chunk, I am not generating any formatted requirements. The content appears to be descriptive rather than normative.\\n\\nNo INCOSE-style requirements can be extracted from this particular chunk of the Implementation Guide.## REQ-1\\n\\n**Summary**: Department Contact Information Format\\n**Description**: \"When the contact is a department name, rather than a human (e.g., patient help line), [the system] SHALL include a blank family and given name, and provide the department name in contact.name.text\"\\n**Verification**: Test\\n**Notes**: Actor: Implementing System, Conformance: SHALL, Conditional: False (applies whenever contact is a department)\\n**Source**: Plan-Net Organization Profile Description\\n\\n## REQ-2\\n\\n**Summary**: PractitionerRole Optional Practitioner Association\\n**Description**: \"The absence of a Practitioner resource does not imply that the Organization itself is playing the role of a Practitioner, instead it implies that that role has been established by the Organization and MAY apply that to a specific Practitioner.\"\\n**Verification**: Test\\n**Notes**: Actor: Implementing System, Conformance: MAY, Conditional: True (when Practitioner resource is absent)\\n**Source**: Plan-Net PractitionerRole Profile Description## REQ-1\\n\\n**Summary**: Specify delivery modalities for virtual service\\n**Description**: \"If service delivery is virtual, one or more delivery modalities should be specified.\"\\n**Verification**: Test\\n**Notes**: Actor: System, Conformance: SHOULD, Conditional: True (only if service delivery is virtual)\\n**Source**: Delivery Method Extension Definition\\n\\n## REQ-2\\n\\n**Summary**: Accept networks for practitioner roles\\n**Description**: \"While the role is constrained to participation in a single network where it describes the single role this practitioner is playing (e.g., internal medicine physician) this does not in any way prohibit the practitioner from seeing patients from any other network\"\\n**Verification**: Test\\n**Notes**: Actor: System, Conformance: SHALL NOT (prohibit), Conditional: False\\n**Source**: New Patients Extension Definition\\n\\n## REQ-3\\n\\n**Summary**: Endpoint payload type constraint\\n**Description**: \"Endpoint Payload Types are constrained to NA (Not Applicable) as part of this IG\"\\n**Verification**: Test\\n**Notes**: Actor: System, Conformance: SHALL, Conditional: False\\n**Source**: Endpoint Payload Types VS## REQ-1\\n\\n**Summary**: Code system support for accepting patients status\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Accepting Patients Codes code system to identify if the practice is accepting new patients\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Accepting Patients Codes\\n\\n## REQ-2 \\n\\n**Summary**: Code system support for accessibility categories\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Accessibility CS code system for general categories of accommodations available\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Accessibility CS\\n\\n## REQ-3\\n\\n**Summary**: Code system support for delivery methods\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Delivery Methods code system for categories of healthcare service delivery methods\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False \\n**Source**: Terminology: Code Systems - Delivery Methods\\n\\n## REQ-4\\n\\n**Summary**: Code system support for endpoint payload types\\n**Description**: \"Systems conforming to this implementation guide SHALL constrain Endpoint Payload Types to NA (Not Applicable) as part of this IG\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Endpoint Payload Types\\n\\n## REQ-5\\n\\n**Summary**: Code system support for healthcare service categories \\n**Description**: \"Systems conforming to this implementation guide SHALL use the Healthcare Service Category code system for broad categories of healthcare services being performed or delivered\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Healthcare Service Category\\n\\n## REQ-6\\n\\n**Summary**: Code system support for insurance plan types\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Insurance Plan Type code system for categories of cost-sharing used by plans\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Insurance Plan Type\\n\\n## REQ-7\\n\\n**Summary**: Code system support for insurance product types\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Insurance Product Type code system for distinct packages of health insurance coverage benefits offered using a particular product network type\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Insurance Product Type\\n\\n## REQ-8\\n\\n**Summary**: Code system support for language proficiency\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Language Proficiency CS code system for documenting spoken language proficiency based on the Interagency Language Roundtable scale\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Language Proficiency CS\\n\\n## REQ-9\\n\\n**Summary**: Code system support for organization affiliation roles\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Organization Affiliation Role code system for codes for organization affiliation roles\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Organization Affiliation Role\\n\\n## REQ-10\\n\\n**Summary**: Code system support for organization types\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Organization Type code system for categories of organizations based on criteria in provider directories\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Organization Type\\n\\n## REQ-11\\n\\n**Summary**: Code system support for provider roles\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Provider Role Codes code system for capabilities that individuals, groups, or organizations are acknowledged to have in a payer network\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Provider Role Codes\\n\\n## REQ-12\\n\\n**Summary**: Code system support for qualification status\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Qualification Status code system for states indicating if a qualification is currently valid\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Qualification Status\\n\\n## REQ-13\\n\\n**Summary**: Code system support for virtual modalities\\n**Description**: \"Systems conforming to this implementation guide SHALL use the Virtual Modalities code system for categories of virtual service delivery modalities\"\\n**Verification**: Test\\n**Notes**: Actor: Conforming Systems, Conformance: SHALL, Conditional: False\\n**Source**: Terminology: Code Systems - Virtual Modalities\\n```This section of the FHIR Implementation Guide appears to be a listing of example instances and does not contain any specific, testable requirements with conformance language (SHALL, SHOULD, MAY, etc.). Therefore, there are no INCOSE-style requirements to extract from this content.',\n",
       " 'output_file': 'checkpoints/requirements_extraction/claude_reqs_list_v1_20250429_081756.md'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqs_extraction_pipeline.process_markdown_content_for_incose_srs(\n",
    "    'claude', \n",
    "    'checkpoints/post_processing',\n",
    "    'checkpoints/requirements_extraction'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402ce7",
   "metadata": {},
   "source": [
    "### RAG-based Requirement Extraction\n",
    "\n",
    "This extraction requirement extraction method differs from the first in that, as a part of the creation of its prompt, it performs a semantic search on example sections of FHIR IG text and the human-generated requirements that were produced in reference to those sections of text to find the most similar section(s) of FHIR IG text in the database and their associated requirement(s). Those sets of IG text and requirement(s) are then supplied to the LLM as few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c6bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "669a69ed",
   "metadata": {},
   "source": [
    "## Test Plan Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73ebb1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ONCL310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
