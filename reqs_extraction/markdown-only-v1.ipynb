{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM requirements extraction from IG with markdown content only\n",
    "37 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Union, Optional, Any\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Current working directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction\n",
      "INFO:root:Project root: /Users/ceadams/Documents/onclaive/onclaive\n",
      "INFO:root:Markdown directory: /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown\n",
      "INFO:root:Found markdown directory at /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown\n",
      "INFO:root:Found 7 markdown files\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory and set up paths\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level from reqs_extraction to onclaive root\n",
    "MARKDOWN_DIR = os.path.join(PROJECT_ROOT, 'full-ig', 'markdown')\n",
    "\n",
    "# Add debug logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.info(f\"Current working directory: {Path.cwd()}\")\n",
    "logging.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "logging.info(f\"Markdown directory: {MARKDOWN_DIR}\")\n",
    "\n",
    "# Verify the markdown directory exists\n",
    "if os.path.exists(MARKDOWN_DIR):\n",
    "    logging.info(f\"Found markdown directory at {MARKDOWN_DIR}\")\n",
    "    markdown_files = [f for f in os.listdir(MARKDOWN_DIR) if f.endswith('.md')]\n",
    "    logging.info(f\"Found {len(markdown_files)} markdown files\")\n",
    "else:\n",
    "    logging.error(f\"Markdown directory not found at {MARKDOWN_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "load_dotenv()\n",
    "\n",
    "# API Configuration\n",
    "API_CONFIGS = {\n",
    "    \"claude\": {\n",
    "        \"model_name\": \"claude-3-5-sonnet-20240620\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"batch_size\": 3,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 10,\n",
    "        \"requests_per_minute\": 25,\n",
    "        \"max_requests_per_day\": 5000,\n",
    "        \"delay_between_requests\": 2\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-1.5-pro-001\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"batch_size\": 1,  # More conservative batch size\n",
    "        \"delay_between_chunks\": 5,\n",
    "        \"delay_between_batches\": 30,\n",
    "        \"requests_per_minute\": 30,\n",
    "        \"max_requests_per_day\": 60000,\n",
    "        \"delay_between_requests\": 3,\n",
    "        \"timeout\": 120  # Longer timeout for larger content\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.7,\n",
    "        \"batch_size\": 3,\n",
    "        \"delay_between_chunks\": 3,\n",
    "        \"delay_between_batches\": 15,\n",
    "        \"requests_per_minute\": 200,\n",
    "        \"max_requests_per_day\": 10000,\n",
    "        \"delay_between_requests\": 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"claude\": \"\"\"You are a seasoned Healthcare Integration Test Engineer \n",
    "                analyzing a FHIR Implementation Guide to extract precise testable requirements.\"\"\",\n",
    "    \"gemini\": \"\"\"Analyze FHIR Implementation Guide content to identify \n",
    "                 testable requirements as a Healthcare Integration Test Engineer.\"\"\",\n",
    "    \"gpt\": \"\"\"As a Healthcare Integration Test Engineer, analyze this FHIR \n",
    "              Implementation Guide content to extract specific testable requirements.\"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_markdown_files(markdown_dir):\n",
    "    \"\"\"Debug function to list all markdown files\"\"\"\n",
    "    if not os.path.exists(markdown_dir):\n",
    "        logging.error(f\"Directory does not exist: {markdown_dir}\")\n",
    "        return\n",
    "    \n",
    "    files = [f for f in os.listdir(markdown_dir) if f.endswith('.md')]\n",
    "    logging.info(f\"Found {len(files)} markdown files:\")\n",
    "    for file in files:\n",
    "        logging.info(f\"  - {file}\")\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Markdown Processing Functions\n",
    "def clean_markdown(text: str) -> str:\n",
    "    \"\"\"Clean markdown content\"\"\"\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\\\(.)', r'\\1', text)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'[-\\s]*\\n[-\\s]*', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_markdown(content: str, max_size: int = 2000) -> List[str]:\n",
    "    \"\"\"Split markdown into manageable chunks\"\"\"\n",
    "    chunks = []\n",
    "    lines = content.split('\\n')\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line_size = len(line)\n",
    "        if current_size + line_size > max_size:\n",
    "            if current_chunk:\n",
    "                chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [line]\n",
    "            current_size = line_size\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_size += line_size\n",
    "            \n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rate Limiting\n",
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in API_CONFIGS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    config = API_CONFIGS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= config['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= config['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:]\n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < config['delay_between_requests']:\n",
    "        time.sleep(config['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompting Functions\n",
    "def create_requirements_extraction_prompt(content: str) -> str:\n",
    "    \"\"\"Create a prompt that aligns with Inferno's requirements extraction process\"\"\"\n",
    "    return f\"\"\"Analyze this FHIR Implementation Guide content to extract precise requirements following these guidelines:\n",
    "\n",
    "For each requirement you identify, provide:\n",
    "\n",
    "1. REQUIREMENT TEXT\n",
    "- Extract direct quotes from the source\n",
    "- For compound requirements, split into atomic requirements\n",
    "- Maintain context when splitting\n",
    "- Use [...] for added clarifications\n",
    "- Use ... for removed text\n",
    "- Format using markdown syntax for code blocks, italics, etc.\n",
    "\n",
    "2. REQUIREMENT METADATA\n",
    "- Conformance Level (SHALL, SHOULD, MAY, SHOULD NOT, SHALL NOT)\n",
    "- Actor(s) the requirement applies to\n",
    "- Whether the requirement is conditional (True/False)\n",
    "- Any sub-requirements or referenced requirements\n",
    "\n",
    "3. SOURCE TRACEABILITY\n",
    "- Note the specific section or location this requirement comes from\n",
    "\n",
    "When analyzing content, focus on:\n",
    "\n",
    "a) Making requirements atomic and testable\n",
    "b) Maintaining the original text while adding necessary context\n",
    "c) Identifying implicit requirements for each actor\n",
    "d) Distinguishing between conjunctive (\"and\") and disjunctive (\"or\") requirements\n",
    "e) Capturing terminology bindings and must-support elements\n",
    "f) Noting RESTful API conformance requirements\n",
    "g) Identifying conditional requirements\n",
    "\n",
    "Content to analyze:\n",
    "{content}\n",
    "\n",
    "Format each requirement as:\n",
    "```\n",
    "Requirement Text: <quoted text with [...] for clarifications and ... for elisions>\n",
    "Conformance: <conformance level>\n",
    "Actor: <actor name(s)>\n",
    "Conditional: <True/False>\n",
    "Sub-Requirements: <list of referenced requirements if any>\n",
    "Source: <specific location in documentation>\n",
    "```\"\"\"\n",
    "\n",
    "def format_content_for_api(content: Union[str, dict, list], api_type: str) -> Union[str, List[dict], dict]:\n",
    "    \"\"\"Format content appropriately for each API\"\"\"\n",
    "    base_prompt = create_requirements_extraction_prompt(content)\n",
    "    \n",
    "    if api_type == \"claude\":\n",
    "        return [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": base_prompt\n",
    "        }]\n",
    "    elif api_type == \"gemini\":\n",
    "        return [{  # Changed from dict to list with single dict\n",
    "            \"parts\": [{\n",
    "                \"text\": base_prompt\n",
    "            }]\n",
    "        }]\n",
    "    return base_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(5),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def make_api_request(client, api_type: str, content: str, rate_limit_func) -> str:\n",
    "    \"\"\"Make rate-limited API request with retries\"\"\"\n",
    "    rate_limit_func()\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    formatted_content = format_content_for_api(content, api_type)\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": formatted_content\n",
    "                }],\n",
    "                system=SYSTEM_PROMPTS[api_type]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            # Extract the text content for Gemini\n",
    "            prompt_text = formatted_content[0][\"parts\"][0][\"text\"]\n",
    "            response = client.generate_content(\n",
    "                prompt_text,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            elif response.candidates:\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPTS[api_type]},\n",
    "                    {\"role\": \"user\", \"content\": formatted_content}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_content_batch(api_type: str, contents: List[str], \n",
    "                       config: dict, client, rate_limit_func) -> List[str]:\n",
    "    \"\"\"Process a batch of content with rate limiting\"\"\"\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        result = make_api_request(client, api_type, content, rate_limit_func)\n",
    "        results.append(result)\n",
    "        time.sleep(config[\"delay_between_chunks\"])\n",
    "    return results\n",
    "\n",
    "# Results Processing\n",
    "def process_llm_requirements_output(output: str) -> List[Dict]:\n",
    "    \"\"\"Process LLM output into standardized requirements format\"\"\"\n",
    "    requirements = []\n",
    "    current_req = {}\n",
    "    \n",
    "    # Split output into individual requirements\n",
    "    req_blocks = output.split('\\n\\n')\n",
    "    \n",
    "    for block in req_blocks:\n",
    "        if block.strip().startswith('Requirement Text:'):\n",
    "            # Save previous requirement if it exists\n",
    "            if current_req:\n",
    "                requirements.append(current_req)\n",
    "                current_req = {}\n",
    "            \n",
    "            # Parse new requirement\n",
    "            lines = block.strip().split('\\n')\n",
    "            for line in lines:\n",
    "                if ': ' in line:\n",
    "                    key, value = line.split(': ', 1)\n",
    "                    key = key.lower().replace(' ', '_')\n",
    "                    current_req[key] = value.strip()\n",
    "    \n",
    "    # Add final requirement\n",
    "    if current_req:\n",
    "        requirements.append(current_req)\n",
    "        \n",
    "    return requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_requirements_to_csv(requirements: List[Dict], output_file: str):\n",
    "    \"\"\"Save extracted requirements to CSV format\"\"\"\n",
    "    df = pd.DataFrame(requirements)\n",
    "    \n",
    "    # Rename columns to match Inferno's format\n",
    "    column_mapping = {\n",
    "        'requirement_text': 'Requirement',\n",
    "        'conformance': 'Conformance',\n",
    "        'actor': 'Actor',\n",
    "        'conditional': 'Conditionality',\n",
    "        'source': 'URL',\n",
    "        'sub_requirements': 'Sub-Requirement(s)'\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Add required columns if missing\n",
    "    required_columns = ['Req Set', 'Id'] + list(column_mapping.values())\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "            \n",
    "    # Generate sequential IDs if not present\n",
    "    if 'Id' in df.columns and df['Id'].isna().all():\n",
    "        df['Id'] = range(1, len(df) + 1)\n",
    "        \n",
    "    df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main Processing\n",
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        verify_path = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "        http_client = httpx.Client(\n",
    "            verify=verify_path if os.path.exists(verify_path) else True,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        gemini.configure(api_key=gemini_api_key)\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "        openai_client = OpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gpt\": openai_client,\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_markdown_content(api_type: str, markdown_dir: str = MARKDOWN_DIR) -> Dict[str, Any]:\n",
    "    \"\"\"Process all markdown content and generate requirements\"\"\"\n",
    "    logging.info(f\"Starting processing with {api_type} on directory: {markdown_dir}\")\n",
    "    \n",
    "    # List files before processing\n",
    "    markdown_files = list_markdown_files(markdown_dir)\n",
    "    if not markdown_files:\n",
    "        logging.error(\"No markdown files found to process\")\n",
    "        return {\"requirements\": [], \"processed_files\": [], \"output_file\": None}\n",
    "        \n",
    "    clients = setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = API_CONFIGS[api_type]\n",
    "    rate_limiter = create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    try:\n",
    "        all_requirements = []\n",
    "        processed_files = []\n",
    "        \n",
    "        for md_file in markdown_files:\n",
    "            file_path = os.path.join(markdown_dir, md_file)\n",
    "            logging.info(f\"Processing {md_file}\")\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                content = clean_markdown(f.read())\n",
    "                logging.debug(f\"Content length for {md_file}: {len(content)} characters\")\n",
    "                \n",
    "            chunks = split_markdown(content)\n",
    "            logging.info(f\"Split {md_file} into {len(chunks)} chunks\")\n",
    "            \n",
    "            for chunk_idx, chunk in enumerate(chunks, 1):\n",
    "                logging.info(f\"Processing chunk {chunk_idx}/{len(chunks)} of {md_file}\")\n",
    "                response = make_api_request(client, api_type, chunk, check_limits)\n",
    "                chunk_requirements = process_llm_requirements_output(response)\n",
    "                logging.info(f\"Extracted {len(chunk_requirements)} requirements from chunk\")\n",
    "                all_requirements.extend(chunk_requirements)\n",
    "                time.sleep(config[\"delay_between_chunks\"])\n",
    "            \n",
    "            processed_files.append(md_file)\n",
    "            time.sleep(config[\"delay_between_batches\"])\n",
    "        \n",
    "        # Save requirements to CSV\n",
    "        output_directory = os.path.join(PROJECT_ROOT, 'reqs_extraction', 'processed_output')\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        output_file = os.path.join(output_directory, f\"test_requirements_{api_type}_markdown1.csv\")\n",
    "        save_requirements_to_csv(all_requirements, output_file)\n",
    "        \n",
    "        logging.info(f\"Completed processing {len(processed_files)} files, extracted {len(all_requirements)} requirements\")\n",
    "        \n",
    "        return {\n",
    "            \"requirements\": all_requirements,\n",
    "            \"processed_files\": processed_files,\n",
    "            \"output_file\": output_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing content: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing with gemini...\n",
      "INFO:root:Starting processing with gemini on directory: /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown\n",
      "INFO:root:Found 7 markdown files:\n",
      "INFO:root:  - implementation.md\n",
      "INFO:root:  - examples.md\n",
      "INFO:root:  - profiles.md\n",
      "INFO:root:  - ChangeHistory.md\n",
      "INFO:root:  - artifacts.md\n",
      "INFO:root:  - index.md\n",
      "INFO:root:  - CapabilityStatement_plan_net.md\n",
      "INFO:root:Processing implementation.md\n",
      "INFO:root:Split implementation.md into 8 chunks\n",
      "INFO:root:Processing chunk 1/8 of implementation.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 2/8 of implementation.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 3/8 of implementation.md\n",
      "INFO:root:Extracted 4 requirements from chunk\n",
      "INFO:root:Processing chunk 4/8 of implementation.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 5/8 of implementation.md\n",
      "INFO:root:Extracted 3 requirements from chunk\n",
      "INFO:root:Processing chunk 6/8 of implementation.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 7/8 of implementation.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 8/8 of implementation.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing examples.md\n",
      "INFO:root:Split examples.md into 6 chunks\n",
      "INFO:root:Processing chunk 1/6 of examples.md\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing chunk 2/6 of examples.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 3/6 of examples.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 4/6 of examples.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 5/6 of examples.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 6/6 of examples.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing profiles.md\n",
      "INFO:root:Split profiles.md into 4 chunks\n",
      "INFO:root:Processing chunk 1/4 of profiles.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 2/4 of profiles.md\n",
      "INFO:root:Extracted 8 requirements from chunk\n",
      "INFO:root:Processing chunk 3/4 of profiles.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 4/4 of profiles.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing ChangeHistory.md\n",
      "INFO:root:Split ChangeHistory.md into 2 chunks\n",
      "INFO:root:Processing chunk 1/2 of ChangeHistory.md\n",
      "INFO:root:Extracted 1 requirements from chunk\n",
      "INFO:root:Processing chunk 2/2 of ChangeHistory.md\n",
      "INFO:root:Extracted 7 requirements from chunk\n",
      "INFO:root:Processing artifacts.md\n",
      "INFO:root:Split artifacts.md into 19 chunks\n",
      "INFO:root:Processing chunk 1/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 2/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 3/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 4/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 5/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 6/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 7/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 8/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 9/19 of artifacts.md\n",
      "INFO:root:Extracted 8 requirements from chunk\n",
      "INFO:root:Processing chunk 10/19 of artifacts.md\n",
      "INFO:root:Extracted 10 requirements from chunk\n",
      "INFO:root:Processing chunk 11/19 of artifacts.md\n",
      "INFO:root:Extracted 9 requirements from chunk\n",
      "INFO:root:Processing chunk 12/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 13/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 14/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 15/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 16/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 17/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 18/19 of artifacts.md\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing chunk 19/19 of artifacts.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing index.md\n",
      "INFO:root:Split index.md into 5 chunks\n",
      "INFO:root:Processing chunk 1/5 of index.md\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing chunk 2/5 of index.md\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing chunk 3/5 of index.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 4/5 of index.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 5/5 of index.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing CapabilityStatement_plan_net.md\n",
      "INFO:root:Split CapabilityStatement_plan_net.md into 14 chunks\n",
      "INFO:root:Processing chunk 1/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing chunk 2/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing chunk 3/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 4/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 3 requirements from chunk\n",
      "INFO:root:Processing chunk 5/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 6/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 7/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 14 requirements from chunk\n",
      "INFO:root:Processing chunk 8/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 9/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 10/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 11/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 12/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 15 requirements from chunk\n",
      "INFO:root:Processing chunk 13/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 14/14 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Completed processing 7 files, extracted 102 requirements\n",
      "INFO:root:Saved gemini results to /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/processed_output/test_requirements_gemini_markdown1.json\n"
     ]
    }
   ],
   "source": [
    "# Define input and output directories using absolute paths\n",
    "markdown_dir = MARKDOWN_DIR\n",
    "output_directory = os.path.join(PROJECT_ROOT, 'reqs_extraction', 'processed_output')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process with each API\n",
    "#apis = [\"claude\", \"gemini\", \"gpt\"]\n",
    "apis=['gemini']\n",
    "results = {}\n",
    "\n",
    "for api_type in apis:\n",
    "    try:\n",
    "        logging.info(f\"Processing with {api_type}...\")\n",
    "        api_results = process_markdown_content(api_type, markdown_dir)\n",
    "        results[api_type] = api_results\n",
    "        \n",
    "        # Save JSON results\n",
    "        json_output = os.path.join(output_directory, f\"test_requirements_{api_type}_markdown1.json\")\n",
    "        with open(json_output, 'w') as f:\n",
    "            json.dump(api_results, f, indent=2)\n",
    "        logging.info(f\"Saved {api_type} results to {json_output}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {api_type}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
