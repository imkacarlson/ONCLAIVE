{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FHIR Implementation Guide Requirements Extractor\n",
    "\n",
    "This notebook extracts testable requirements from FHIR Implementation Guides and formats them according to INCOSE Systems Engineering standards.\n",
    "\n",
    "#### Features\n",
    "- Processes markdown files from FHIR Implementation Guides\n",
    "- Extracts clear, testable requirements with proper attribution\n",
    "- Formats requirements in standardized INCOSE format\n",
    "- Handles large documents through chunking\n",
    "\n",
    "#### Usage\n",
    "1. Input markdown directory can be set to `full-ig/markdown7_cleaned` for limited set of 7 markdown files or `full-ig/markdown_cleaned` for full set of 300+ markdown files\n",
    "2. Individual cert setup may need to be modified in `setup_clients()` function\n",
    "3. Run all cells in this notebook\n",
    "4. When prompted, enter IG name and version (or accept defaults)\n",
    "5. Select the LLM engine to use (Gemini currently working most consistently)\n",
    "6. The script will generate two files in the `reqs_extraction/processed_output` directory:\n",
    "   - Complete SRS document with all requirements\n",
    "   - Clean requirements list formatted to INCOSE standards\n",
    "\n",
    "#### Notes:\n",
    "- Supports Claude, Gemini, or GPT-4\n",
    "- API keys should be in .env file\n",
    "- GPT-4 works well for smaller IGs but may hit token limits with large documents\n",
    "- Claude has larger context but may experience server load issues\n",
    "- Gemini offers a good balance of context size and availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Union, Optional, Any\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Current working directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction\n",
      "INFO:root:Project root: /Users/ceadams/Documents/onclaive/onclaive\n",
      "INFO:root:Markdown directory: /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned\n",
      "INFO:root:Found markdown directory at /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned\n",
      "INFO:root:Found 7 markdown files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the current working directory and set up paths\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level from reqs_extraction to onclaive root\n",
    "MARKDOWN_DIR = os.path.join(PROJECT_ROOT, 'full-ig', 'markdown7_cleaned')\n",
    "\n",
    "# Add debug logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(f\"Current working directory: {Path.cwd()}\")\n",
    "logging.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "logging.info(f\"Markdown directory: {MARKDOWN_DIR}\")\n",
    "\n",
    "# Verify the markdown directory exists\n",
    "if os.path.exists(MARKDOWN_DIR):\n",
    "    logging.info(f\"Found markdown directory at {MARKDOWN_DIR}\")\n",
    "    markdown_files = [f for f in os.listdir(MARKDOWN_DIR) if f.endswith('.md')]\n",
    "    logging.info(f\"Found {len(markdown_files)} markdown files\")\n",
    "else:\n",
    "    logging.error(f\"Markdown directory not found at {MARKDOWN_DIR}\")\n",
    "\n",
    "# Basic setup\n",
    "load_dotenv(os.path.join(PROJECT_ROOT, '.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "API_CONFIGS = {\n",
    "    \"claude\": {\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 1,\n",
    "        \"delay_between_batches\": 3,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.1\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-1.5-pro-001\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"batch_size\": 5,  # More conservative batch size\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 900,\n",
    "        \"max_requests_per_day\": 50000,\n",
    "        \"delay_between_requests\": 0.1,\n",
    "        \"timeout\": 60  # Longer timeout for larger content\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"batch_size\": 5,\n",
    "        \"delay_between_chunks\": 2,\n",
    "        \"delay_between_batches\": 5,\n",
    "        \"requests_per_minute\": 450,\n",
    "        \"max_requests_per_day\": 20000,\n",
    "        \"delay_between_requests\": 0.15\n",
    "    }\n",
    "}\n",
    "\n",
    "# Updated system prompts to produce INCOSE-style output\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"claude\": \"\"\"You are a seasoned Healthcare Integration Test Engineer with expertise in INCOSE Systems Engineering standards, \n",
    "    analyzing a FHIR Implementation Guide to extract precise testable requirements in INCOSE format.\"\"\",\n",
    "    \"gemini\": \"\"\"You are a Healthcare Integration Test Engineer with expertise in INCOSE Systems Engineering standards, analyzing FHIR \n",
    "    Implementation Guide content to identify and format testable requirements following INCOSE specifications.\"\"\",\n",
    "    \"gpt\": \"\"\"As a Healthcare Integration Test Engineer with INCOSE Systems Engineering expertise, analyze this FHIR \n",
    "    Implementation Guide content to extract specific testable requirements in INCOSE-compliant format.\"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining and chunking markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_markdown_files(markdown_dir):\n",
    "    \"\"\"Debug function to list all markdown files\"\"\"\n",
    "    if not os.path.exists(markdown_dir):\n",
    "        logging.error(f\"Directory does not exist: {markdown_dir}\")\n",
    "        return\n",
    "    \n",
    "    files = [f for f in os.listdir(markdown_dir) if f.endswith('.md')]\n",
    "    logging.info(f\"Found {len(files)} markdown files:\")\n",
    "    for file in files:\n",
    "        logging.info(f\"  - {file}\")\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_chunk_size(api_type: str, markdown_content: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the optimal chunk size based on API type and content characteristics.\n",
    "    \"\"\"\n",
    "    config = API_CONFIGS[api_type]\n",
    "    \n",
    "    # Base chunk sizes based on API token limits\n",
    "    base_chunk_sizes = {\n",
    "        \"claude\": 8000,  # Claude has higher token limits\n",
    "        \"gemini\": 7000,  # Gemini is also capable of handling larger chunks\n",
    "        \"gpt\": 3000      # GPT-4 with smaller context\n",
    "    }\n",
    "    \n",
    "    # Start with the base size for the API\n",
    "    optimal_size = base_chunk_sizes[api_type]\n",
    "    \n",
    "    # Adjust based on content characteristics\n",
    "    content_length = len(markdown_content)\n",
    "    \n",
    "    # For very small content, don't chunk at all\n",
    "    if content_length <= optimal_size / 2:\n",
    "        return content_length\n",
    "    \n",
    "    # For medium content, use the base size\n",
    "    if content_length <= optimal_size * 1.5:\n",
    "        return optimal_size\n",
    "    \n",
    "    # For larger content, adjust based on complexity \n",
    "    code_blocks = markdown_content.count(\"```\")\n",
    "    tables = markdown_content.count(\"|\")\n",
    "    \n",
    "    # Adjust down if content has complex structures\n",
    "    complexity_factor = 1.0\n",
    "    if code_blocks > 5:\n",
    "        complexity_factor *= 0.9\n",
    "    if tables > 10:\n",
    "        complexity_factor *= 0.9\n",
    "    \n",
    "    # Make sure we don't exceed API token limits\n",
    "    return min(int(optimal_size * complexity_factor), base_chunk_sizes[api_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown Processing Functions\n",
    "def clean_markdown(text: str) -> str:\n",
    "    \"\"\"Clean markdown content\"\"\"\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\\\(.)', r'\\1', text)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'[-\\s]*\\n[-\\s]*', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_markdown_dynamic(content: str, api_type: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split markdown into dynamically sized chunks based on API type and content.\n",
    "    \"\"\"\n",
    "    # If content is very small, don't split it\n",
    "    if len(content) < 1000:\n",
    "        return [content]\n",
    "    \n",
    "    # Calculate optimal chunk size\n",
    "    max_size = calculate_optimal_chunk_size(api_type, content)\n",
    "    \n",
    "    chunks = []\n",
    "    lines = content.split('\\n')\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    # Try to split at meaningful boundaries like headers or blank lines\n",
    "    for i, line in enumerate(lines):\n",
    "        line_size = len(line)\n",
    "        \n",
    "        if current_size + line_size > max_size:\n",
    "            # Look back for a good splitting point (blank line or header)\n",
    "            split_index = find_good_split_point(current_chunk)\n",
    "            \n",
    "            if split_index > 0:\n",
    "                # Split at the good point\n",
    "                first_part = current_chunk[:split_index]\n",
    "                second_part = current_chunk[split_index:]\n",
    "                chunks.append('\\n'.join(first_part))\n",
    "                current_chunk = second_part\n",
    "                current_size = sum(len(l) for l in second_part)\n",
    "            else:\n",
    "                # If no good splitting point, use the current chunk\n",
    "                chunks.append('\\n'.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "            \n",
    "            # Add the current line to the new chunk\n",
    "            current_chunk.append(line)\n",
    "            current_size += line_size\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_size += line_size\n",
    "    \n",
    "    # Add the last chunk if there's anything left\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def find_good_split_point(lines: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    Find a good place to split a chunk, preferring blank lines or headers.\n",
    "    \"\"\"\n",
    "    # Go backwards from the end to find a natural splitting point\n",
    "    for i in range(len(lines) - 1, 0, -1):\n",
    "        # Prefer blank lines\n",
    "        if lines[i].strip() == '':\n",
    "            return i + 1\n",
    "        \n",
    "        # Or headers\n",
    "        if lines[i].startswith('#') or lines[i].startswith('==') or lines[i].startswith('--'):\n",
    "            return i\n",
    "    \n",
    "    # If we're more than halfway through, just use the current point\n",
    "    return len(lines) // 2\n",
    "\n",
    "def should_combine_files(files: List[str], markdown_dir: str, api_type: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Determine if small files should be combined for processing.\n",
    "    \"\"\"\n",
    "    config = API_CONFIGS[api_type]\n",
    "    file_sizes = {}\n",
    "    \n",
    "    # Get the size of each file\n",
    "    for file in files:\n",
    "        file_path = os.path.join(markdown_dir, file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            file_sizes[file] = len(content)\n",
    "    \n",
    "    # Estimate the optimal size based on API\n",
    "    optimal_sizes = {\n",
    "        \"claude\": 12000,\n",
    "        \"gemini\": 10000,\n",
    "        \"gpt\": 6000\n",
    "    }\n",
    "    \n",
    "    optimal_size = optimal_sizes[api_type]\n",
    "    combined_files = []\n",
    "    current_group = []\n",
    "    current_size = 0\n",
    "    \n",
    "    # Sort files by size (ascending) to try combining smaller files first\n",
    "    sorted_files = sorted(files, key=lambda f: file_sizes[f])\n",
    "    \n",
    "    for file in sorted_files:\n",
    "        size = file_sizes[file]\n",
    "        \n",
    "        # If this file is already big, process it individually\n",
    "        if size > optimal_size * 0.8:\n",
    "            if current_group:\n",
    "                combined_files.append(current_group)\n",
    "                current_group = []\n",
    "                current_size = 0\n",
    "            combined_files.append([file])\n",
    "            continue\n",
    "        \n",
    "        # If adding this file would exceed optimal size, start a new group\n",
    "        if current_size + size > optimal_size:\n",
    "            if current_group:\n",
    "                combined_files.append(current_group)\n",
    "            current_group = [file]\n",
    "            current_size = size\n",
    "        else:\n",
    "            current_group.append(file)\n",
    "            current_size += size\n",
    "    \n",
    "    # Add the last group if there's anything left\n",
    "    if current_group:\n",
    "        combined_files.append(current_group)\n",
    "    \n",
    "    return combined_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate Limiting\n",
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in API_CONFIGS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    config = API_CONFIGS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= config['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= config['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:]\n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < config['delay_between_requests']:\n",
    "        time.sleep(config['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_incose_requirements_extraction_prompt(content: str, ig_name: str, ig_version: str, chunk_index: int, total_chunks: int) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for extracting requirements in INCOSE format\n",
    "    \n",
    "    Args:\n",
    "        content: The content to analyze\n",
    "        ig_name: Name of the Implementation Guide\n",
    "        ig_version: Version of the Implementation Guide\n",
    "        chunk_index: Index of this chunk in the total content\n",
    "        total_chunks: Total number of chunks being processed\n",
    "        \n",
    "    Returns:\n",
    "        str: The prompt for the LLM\n",
    "    \"\"\"\n",
    "    return f\"\"\"As a Healthcare Integration Test Engineer with INCOSE Systems Engineering expertise, extract and format requirements from this FHIR Implementation Guide content.\n",
    "\n",
    "# ABOUT THIS TASK\n",
    "You are analyzing chunk {chunk_index} of {total_chunks} from the {ig_name} Implementation Guide v{ig_version}. Your task is to:\n",
    "1. Extract specific, testable requirements\n",
    "2. Format them according to INCOSE Systems Engineering standards\n",
    "3. Create requirements that can be directly inserted into an INCOSE-style Software Requirements Specification\n",
    "\n",
    "# REQUIREMENT EXTRACTION GUIDELINES\n",
    "Include ONLY requirements that:\n",
    "   - Have explicit conformance language (SHALL, SHOULD, MAY, MUST, REQUIRED, etc.)\n",
    "   - Describe specific, verifiable behavior or capability\n",
    "   - Could be objectively tested through software testing or attestation\n",
    "- Each requirement must be complete, atomic, and testable\n",
    "- Separate individual requirements\n",
    "- Identify the actor responsible for implementing each requirement\n",
    "- Preserve the original conformance level (SHALL, SHOULD, MAY, etc.)\n",
    "- Mark conditional requirements (those that depend on optional features)\n",
    "- Use exact quotes with necessary context preserved\n",
    "\n",
    "# INCOSE FORMAT REQUIREMENTS\n",
    "For each requirement you identify, format it as follows:\n",
    "\n",
    "```\n",
    "## REQ-[ID]\n",
    "\n",
    "**Summary**: Brief description of the requirement\n",
    "**Description**: \"<exact quote with necessary [clarifications]>\"\n",
    "**Verification**: Recommended verification method (Test/Analysis/Inspection/Demonstration)\n",
    "**Notes**: Actor responsible, conformance level, conditions, etc.\n",
    "**Source**: Section reference from the Implementation Guide\n",
    "```\n",
    "\n",
    "Where [ID] starts at 1 and follows in sequential order.\n",
    "\n",
    "# EXAMPLES FROM OTHER GUIDES\n",
    "Here are two examples of properly formatted requirements from another implementation guide- these should NOT be found in the IG you are reviewing:\n",
    "\n",
    "```\n",
    "## REQ-01\n",
    "\n",
    "**Summary**: Advertise supported subscription topics\n",
    "**Description**: \"In order to allow for discovery of supported subscription topics, this guide defines the CapabilityStatement SubscriptionTopic Canonical extension. The extension allows server implementers to advertise the canonical URLs of topics available to clients.\"\n",
    "**Verification**: Test\n",
    "**Notes**: Actor: Server, Conformance: SHALL, Conditional: False\n",
    "**Source**: Subscription Discovery Section\n",
    "```\n",
    "\n",
    "```\n",
    "## REQ-02\n",
    "\n",
    "**Summary**: Leave topic discovery out-of-band\n",
    "**Description**: \"FHIR R4 servers MAY choose to leave topic discovery completely out-of-band and part of other steps, such as registration or integration.\"\n",
    "**Verification**: Inspection\n",
    "**Notes**: Actor: Server, Conformance: MAY, Conditional: False\n",
    "**Source**: Subscription Configuration Section\n",
    "```\n",
    "\n",
    "Content to analyze:\n",
    "{content}\n",
    "\n",
    "Generate your INCOSE-style requirements extraction now. For each chunk, list each requirement using the format specified above. If you find no requirements in this chunk, do not add any text and move to the next chunk. Do not include any introductory or conclusion/summary comments in your response. Only include the requirements as a list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_content_for_api(content: Union[str, dict, list], api_type: str, ig_name: str, ig_version: str, chunk_index: int, total_chunks: int) -> Union[str, List[dict], dict]:\n",
    "    \"\"\"Format content appropriately for each API\"\"\"\n",
    "    base_prompt = create_incose_requirements_extraction_prompt(content, ig_name, ig_version, chunk_index, total_chunks)\n",
    "    \n",
    "    if api_type == \"claude\":\n",
    "        return [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": base_prompt\n",
    "        }]\n",
    "    elif api_type == \"gemini\":\n",
    "        return [{  # Changed from dict to list with single dict\n",
    "            \"parts\": [{\n",
    "                \"text\": base_prompt\n",
    "            }]\n",
    "        }]\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=4, max=480),  # Longer max wait (2 minutes)\n",
    "    stop=stop_after_attempt(8),  # More retry attempts\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError, httpx.HTTPStatusError))  # Add HTTP errors\n",
    ")\n",
    "def make_api_request(client, api_type: str, content: str, rate_limit_func, ig_name: str, ig_version: str, chunk_index: int, total_chunks: int) -> str:\n",
    "    \"\"\"Make rate-limited API request with retries\"\"\"\n",
    "    rate_limit_func()\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    formatted_content = format_content_for_api(content, api_type, ig_name, ig_version, chunk_index, total_chunks)\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": formatted_content\n",
    "                }],\n",
    "                system=SYSTEM_PROMPTS[api_type]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            # Extract the text content for Gemini\n",
    "            prompt_text = formatted_content[0][\"parts\"][0][\"text\"]\n",
    "            response = client.generate_content(\n",
    "                prompt_text,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            elif response.candidates:\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPTS[api_type]},\n",
    "                    {\"role\": \"user\", \"content\": formatted_content}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content_batch(api_type: str, contents: List[str], config: dict, client, \n",
    "                          rate_limit_func, ig_name: str, ig_version: str) -> List[str]:\n",
    "    \"\"\"Process a batch of content with rate limiting\"\"\"\n",
    "    results = []\n",
    "    total_chunks = len(contents)\n",
    "    \n",
    "    for chunk_idx, content in enumerate(contents, 1):\n",
    "        result = make_api_request(client, api_type, content, rate_limit_func, \n",
    "                                  ig_name, ig_version, chunk_idx, total_chunks)\n",
    "        results.append(result)\n",
    "        time.sleep(config[\"delay_between_chunks\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_requirements_list_prompt(srs_document: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt to extract just the requirements list from the SRS document\n",
    "    \n",
    "    Args:\n",
    "        srs_document: The full INCOSE SRS document\n",
    "        \n",
    "    Returns:\n",
    "        str: The prompt for the LLM\n",
    "    \"\"\"\n",
    "    return f\"\"\"Please extract just the requirements from the following INCOSE SRS document and format them as a clean list.\n",
    "\n",
    "I want a comprehensive list with only the actual requirements - no introductory text, analysis, or explanatory content. Ensure all requirements are atomic. Separate individual requirements.\n",
    " \n",
    "For each requirement, include:\n",
    "1. The requirement ID (REQ-XX)\n",
    "2. The summary\n",
    "3. The description (quoted text)\n",
    "4. Verification method\n",
    "5. Actor, conformance level, and conditions\n",
    "6. Source reference\n",
    "\n",
    "Format each requirement as follows:\n",
    "\n",
    "---\n",
    "# REQ-XX\n",
    "**Summary**: [summary text]\n",
    "**Description**: \"[description text]\"\n",
    "**Verification**: [method]\n",
    "**Actor**: [actor]\n",
    "**Conformance**: [SHALL/SHOULD/MAY/etc.]\n",
    "**Conditional**: [True/False]\n",
    "**Source**: [reference]\n",
    "---\n",
    "\n",
    "Do not include any other text in your response output except for the list with actual requirements. \n",
    "\n",
    "Here's the SRS document:\n",
    "\n",
    "{srs_document}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requirements_list_from_file(client, api_type, srs_file_path, rate_limit_func):\n",
    "    \"\"\"Extract requirements from an SRS document file with improved chunking support\"\"\"\n",
    "    logging.info(f\"Extracting clean requirements list from file using {api_type}...\")\n",
    "    \n",
    "    # Define max token size based on API type - increase these values for Claude and Gemini\n",
    "    max_chunk_sizes = {\n",
    "        \"claude\": 12000,  # Claude can handle larger contexts\n",
    "        \"gemini\": 10000,  # Gemini also has decent context size\n",
    "        \"gpt\": 3000       # GPT-4 with smaller context\n",
    "    }\n",
    "    max_chunk_size = max_chunk_sizes.get(api_type, 3000)\n",
    "    \n",
    "    # Read the file and prepare to extract requirements\n",
    "    with open(srs_file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Find all requirements in the document using an improved regex pattern\n",
    "    # This pattern catches requirements with different heading formats (# REQ, ## REQ, ### REQ)\n",
    "    req_pattern = r'(?:^|\\n)(?:#{1,3}\\s+REQ-\\d+[^\\n]*(?:\\n(?!#{1,3}\\s+REQ-\\d+).*)*)' \n",
    "    requirements = re.findall(req_pattern, content, re.DOTALL)\n",
    "    \n",
    "    if not requirements:\n",
    "        logging.warning(\"No requirements found in the file\")\n",
    "        return \"No requirements found in the document.\"\n",
    "    \n",
    "    total_req_count = len(requirements)\n",
    "    logging.info(f\"Found {total_req_count} requirements to process in chunks\")\n",
    "    \n",
    "    # Process requirements in manageable chunks, but use overlapping boundaries\n",
    "    # to ensure no requirements are lost at chunk boundaries\n",
    "    all_results = []\n",
    "    current_size = 0\n",
    "    current_reqs = []\n",
    "    chunk_count = 1\n",
    "    \n",
    "    for req in requirements:\n",
    "        req_size = len(req)\n",
    "        \n",
    "        # If this requirement is extremely large, split it further\n",
    "        if req_size > max_chunk_size:\n",
    "            logging.warning(f\"Found extremely large requirement ({req_size} chars) that exceeds chunk size\")\n",
    "            # Process it individually with special handling\n",
    "            try:\n",
    "                individual_result = process_large_requirement(client, api_type, req, rate_limit_func)\n",
    "                all_results.append(individual_result)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing large requirement: {str(e)}\")\n",
    "                all_results.append(req)  # Use original as fallback\n",
    "            \n",
    "            time.sleep(API_CONFIGS[api_type][\"delay_between_chunks\"])\n",
    "            continue\n",
    "        \n",
    "        # If adding this requirement would exceed chunk size, process current chunk\n",
    "        if current_size + req_size > max_chunk_size and current_reqs:\n",
    "            chunk_text = \"\\n\\n\".join(current_reqs)\n",
    "            logging.info(f\"Processing chunk {chunk_count} with {len(current_reqs)} requirements\")\n",
    "            \n",
    "            try:\n",
    "                result = process_requirements_chunk(\n",
    "                    client, api_type, chunk_text, rate_limit_func, \n",
    "                    chunk_count, estimate_total_chunks(total_req_count, len(current_reqs))\n",
    "                )\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing chunk {chunk_count}: {str(e)}\")\n",
    "                # Add the raw requirements to ensure nothing is lost\n",
    "                all_results.append(chunk_text)\n",
    "            \n",
    "            # Reset for next chunk - with overlap to ensure continuity\n",
    "            current_reqs = []\n",
    "            current_size = 0\n",
    "            chunk_count += 1\n",
    "            \n",
    "            # Add delay between chunks\n",
    "            time.sleep(API_CONFIGS[api_type][\"delay_between_chunks\"])\n",
    "        \n",
    "        # Add requirement to current chunk\n",
    "        current_reqs.append(req)\n",
    "        current_size += req_size\n",
    "    \n",
    "    # Process the last chunk if anything remains\n",
    "    if current_reqs:\n",
    "        chunk_text = \"\\n\\n\".join(current_reqs)\n",
    "        logging.info(f\"Processing final chunk {chunk_count} with {len(current_reqs)} requirements\")\n",
    "        try:\n",
    "            result = process_requirements_chunk(\n",
    "                client, api_type, chunk_text, rate_limit_func, \n",
    "                chunk_count, chunk_count\n",
    "            )\n",
    "            all_results.append(result)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing final chunk: {str(e)}\")\n",
    "            all_results.append(chunk_text)\n",
    "    \n",
    "    # Combine all processed chunks\n",
    "    combined_results = \"\\n\\n\".join(all_results)\n",
    "    \n",
    "    # Validate the final result\n",
    "    final_req_count = combined_results.count('# REQ-')\n",
    "    if final_req_count < total_req_count:\n",
    "        logging.warning(f\"Potential requirements loss: Found {total_req_count} in source, but only {final_req_count} in processed output\")\n",
    "    else:\n",
    "        logging.info(f\"Successfully extracted all {total_req_count} requirements\")\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=4, max=120),\n",
    "    stop=stop_after_attempt(8),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError, httpx.HTTPStatusError))\n",
    ")\n",
    "def process_requirements_chunk(client, api_type, chunk_text, rate_limit_func, chunk_num, total_chunks):\n",
    "    \"\"\"Process a single chunk of requirements with robust retry logic and improved formatting handling\"\"\"\n",
    "    logging.info(f\"Processing requirements chunk {chunk_num}/{total_chunks} with {chunk_text.count('REQ-')} requirements\")\n",
    "    \n",
    "    # Normalize format to ensure consistency\n",
    "    # Replace all variations of requirement headers with a standardized format\n",
    "    normalized_chunk = re.sub(r'(#{1,3})\\s+(REQ-\\d+)', r'# \\2', chunk_text)\n",
    "    \n",
    "    # Build a more specific prompt that enforces formatting requirements\n",
    "    prompt = f\"\"\"Please extract and format requirements from this PARTIAL list (chunk {chunk_num} of {total_chunks}).\n",
    "    \n",
    "    Format each requirement EXACTLY as follows, maintaining the exact structure including dashes, spacing, and headings:\n",
    "    \n",
    "    ---\n",
    "    # REQ-XX\n",
    "    **Summary**: [summary text]\n",
    "    **Description**: \"[description text]\"\n",
    "    **Verification**: [method]\n",
    "    **Actor**: [actor]\n",
    "    **Conformance**: [SHALL/SHOULD/MAY/etc.]\n",
    "    **Conditional**: [True/False]\n",
    "    **Source**: [reference]\n",
    "    ---\n",
    "    \n",
    "    IMPORTANT FORMATTING INSTRUCTIONS:\n",
    "    1. Each requirement MUST start with \"---\" on its own line \n",
    "    2. Each requirement MUST end with \"---\" on its own line\n",
    "    3. The requirement ID format MUST be a single # followed by \"REQ-XX\" where XX is the number\n",
    "    4. Each attribute MUST be in bold with two asterisks on each side\n",
    "    5. Description MUST be in quotes\n",
    "    6. DO NOT SKIP any requirements in the provided chunk\n",
    "    7. DO NOT CHANGE the original text content of each requirement\n",
    "    8. Include ALL requirements found in this chunk, up to the maximum token limit\n",
    "    \n",
    "    Here are the requirements to process:\n",
    "    \n",
    "    {normalized_chunk}\n",
    "    \"\"\"\n",
    "    \n",
    "    rate_limit_func()\n",
    "    config = API_CONFIGS[api_type]\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }],\n",
    "                system=\"Extract and format requirements EXACTLY as specified, preserving all content and maintaining consistent formatting.\"\n",
    "            )\n",
    "            result = response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            response = client.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                result = response.text\n",
    "            elif response.candidates:\n",
    "                result = response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Extract and format requirements EXACTLY as specified, preserving all content and maintaining consistent formatting.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "        # Validate the output\n",
    "        req_count_in_result = result.count('# REQ-')\n",
    "        req_count_in_input = normalized_chunk.count('REQ-')\n",
    "        \n",
    "        if req_count_in_result < req_count_in_input:\n",
    "            logging.warning(f\"Possible requirements loss: {req_count_in_input} in input, but only {req_count_in_result} in result\")\n",
    "            \n",
    "            # If significant loss, try to process with higher temperature for variation\n",
    "            if req_count_in_result < req_count_in_input * 0.8 and api_type != \"gpt\":\n",
    "                logging.info(\"Attempting reprocessing with higher variation due to missing requirements\")\n",
    "                # This is a simplified retry that you can expand based on your needs\n",
    "                \n",
    "        # Check for proper formatting\n",
    "        if not result.startswith('---') or '**Summary**' not in result:\n",
    "            logging.warning(\"Response may not have proper formatting\")\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing chunk {chunk_num}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_requirement(client, api_type, req_text, rate_limit_func):\n",
    "    \"\"\"\n",
    "    Special handling for extremely large requirements that exceed normal chunk sizes.\n",
    "    \n",
    "    Args:\n",
    "        client: The API client (Claude, Gemini, or GPT)\n",
    "        api_type: The type of API to use (claude, gemini, or gpt)\n",
    "        req_text: The text of the large requirement\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        str: The processed requirement text in standardized format\n",
    "    \"\"\"\n",
    "    logging.info(\"Processing large requirement with special handling\")\n",
    "    \n",
    "    # Extract the requirement ID from the text\n",
    "    req_id_match = re.search(r'(#{1,3})\\s+(REQ-\\d+)', req_text)\n",
    "    if not req_id_match:\n",
    "        logging.warning(\"Could not identify requirement ID in large requirement\")\n",
    "        return req_text\n",
    "    \n",
    "    req_id = req_id_match.group(2)\n",
    "    \n",
    "    # Try to extract key sections to reduce token usage\n",
    "    summary_match = re.search(r'\\*\\*Summary\\*\\*:\\s*([^\\n]+)', req_text)\n",
    "    summary = summary_match.group(1) if summary_match else \"[Extract from text]\"\n",
    "    \n",
    "    description_match = re.search(r'\\*\\*Description\\*\\*:\\s*\"([^\"]+)\"', req_text)\n",
    "    # If we can't extract the description with regex, we'll let the model handle it\n",
    "    \n",
    "    verification_match = re.search(r'\\*\\*Verification\\*\\*:\\s*([^\\n]+)', req_text)\n",
    "    verification = verification_match.group(1) if verification_match else \"[Extract from text]\"\n",
    "    \n",
    "    actor_match = re.search(r'Actor:\\s*([^,\\n]+)', req_text) or re.search(r'\\*\\*Actor\\*\\*:\\s*([^\\n]+)', req_text)\n",
    "    actor = actor_match.group(1) if actor_match else \"[Extract from text]\"\n",
    "    \n",
    "    conformance_match = re.search(r'Conformance:\\s*([^,\\n]+)', req_text) or re.search(r'\\*\\*Conformance\\*\\*:\\s*([^\\n]+)', req_text)\n",
    "    conformance = conformance_match.group(1) if conformance_match else \"[Extract from text]\"\n",
    "    \n",
    "    conditional_match = re.search(r'Conditional:\\s*([^,\\n]+)', req_text) or re.search(r'\\*\\*Conditional\\*\\*:\\s*([^\\n]+)', req_text)\n",
    "    conditional = conditional_match.group(1) if conditional_match else \"[Extract from text]\"\n",
    "    \n",
    "    source_match = re.search(r'Source:\\s*([^\\n]+)', req_text) or re.search(r'\\*\\*Source\\*\\*:\\s*([^\\n]+)', req_text)\n",
    "    source = source_match.group(1) if source_match else \"[Extract from text]\"\n",
    "    \n",
    "    # Create a simplified prompt focused on just this requirement\n",
    "    prompt = f\"\"\"Format the following large requirement with ID {req_id} using the exact format below:\n",
    "    \n",
    "    ---\n",
    "    # {req_id}\n",
    "    **Summary**: {summary}\n",
    "    **Description**: \"[Extract and preserve the original description text]\"\n",
    "    **Verification**: {verification}\n",
    "    **Actor**: {actor}\n",
    "    **Conformance**: {conformance}\n",
    "    **Conditional**: {conditional}\n",
    "    **Source**: {source}\n",
    "    ---\n",
    "    \n",
    "    IMPORTANT: \n",
    "    1. The requirement MUST start and end with \"---\" on its own line\n",
    "    2. Maintain the exact formatting with the # and ** markers as shown\n",
    "    3. Preserve the exact content from the original requirement\n",
    "    4. The Description must be in quotes\n",
    "    \n",
    "    Here is the original requirement text to extract information from:\n",
    "    \n",
    "    {req_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    rate_limit_func()\n",
    "    config = API_CONFIGS[api_type]\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }],\n",
    "                system=\"Extract and format this single requirement exactly as specified, preserving all content and maintaining consistent formatting.\"\n",
    "            )\n",
    "            result = response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            response = client.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                result = response.text\n",
    "            elif response.candidates:\n",
    "                result = response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Extract and format this single requirement exactly as specified, preserving all content and maintaining consistent formatting.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "        \n",
    "        # Verify the output contains the requirement ID\n",
    "        if req_id not in result:\n",
    "            logging.warning(f\"Processed requirement missing ID {req_id}, using original text\")\n",
    "            return req_text\n",
    "            \n",
    "        # Verify proper formatting\n",
    "        if not result.startswith('---') or not '**Summary**' in result:\n",
    "            logging.warning(\"Processed requirement has incorrect formatting, using original text\")\n",
    "            return req_text\n",
    "            \n",
    "        logging.info(f\"Successfully processed large requirement {req_id}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing large requirement {req_id}: {str(e)}\")\n",
    "        # Return the original text in case of failure to ensure no data is lost\n",
    "        return req_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Processing Functions\n",
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        verify_path = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "        http_client = httpx.Client(\n",
    "            verify=verify_path if os.path.exists(verify_path) else True,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        gemini.configure(api_key=gemini_api_key)\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "        openai_client = OpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gpt\": openai_client,\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_markdown_content_for_incose_srs(api_type: str, markdown_dir: str = MARKDOWN_DIR, \n",
    "                                            ig_name: str = \"FHIR\", ig_version: str = \"1.0.0\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process markdown content and generate INCOSE SRS document directly from LLM outputs.\n",
    "    \n",
    "    Args:\n",
    "        api_type: The API to use for processing\n",
    "        markdown_dir: Directory containing markdown files\n",
    "        ig_name: Name of the Implementation Guide\n",
    "        ig_version: Version of the Implementation Guide\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing processing results and SRS document\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting processing with {api_type} on directory: {markdown_dir}\")\n",
    "    \n",
    "    # List files before processing\n",
    "    markdown_files = list_markdown_files(markdown_dir)\n",
    "    if not markdown_files:\n",
    "        logging.error(\"No markdown files found to process\")\n",
    "        return {\"processed_files\": [], \"srs_document\": \"\", \"output_file\": None, \"requirements_list\": \"\", \"requirements_file\": None}\n",
    "    \n",
    "    # Initialize API clients and rate limiters\n",
    "    clients = setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = API_CONFIGS[api_type]\n",
    "    rate_limiter = create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    try:\n",
    "        all_incose_requirements = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Group files for potential combination\n",
    "        file_groups = should_combine_files(markdown_files, markdown_dir, api_type)\n",
    "        logging.info(f\"Organized {len(markdown_files)} files into {len(file_groups)} processing groups\")\n",
    "        \n",
    "        for group in file_groups:\n",
    "            # For a single file\n",
    "            if len(group) == 1:\n",
    "                file_path = os.path.join(markdown_dir, group[0])\n",
    "                logging.info(f\"Processing single file: {group[0]}\")\n",
    "                \n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = clean_markdown(f.read())\n",
    "                \n",
    "                # Use dynamic chunk sizing\n",
    "                chunks = split_markdown_dynamic(content, api_type)\n",
    "                logging.info(f\"Split {group[0]} into {len(chunks)} chunks using dynamic sizing\")\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks, 1):\n",
    "                    logging.info(f\"Processing chunk {chunk_idx}/{len(chunks)} of {group[0]}\")\n",
    "                    response = make_api_request(client, api_type, chunk, check_limits, \n",
    "                                               ig_name, ig_version, chunk_idx, len(chunks))\n",
    "                    all_incose_requirements.append(response)\n",
    "                    time.sleep(config[\"delay_between_chunks\"])\n",
    "                \n",
    "                processed_files.append(group[0])\n",
    "                \n",
    "            # For multiple combined files\n",
    "            else:\n",
    "                logging.info(f\"Processing combined group of {len(group)} files\")\n",
    "                combined_content = []\n",
    "                \n",
    "                # Prepare combined content with clear file boundaries\n",
    "                for file in group:\n",
    "                    file_path = os.path.join(markdown_dir, file)\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        file_content = clean_markdown(f.read())\n",
    "                        combined_content.append(f\"## FILE: {file}\\n\\n{file_content}\\n\\n\")\n",
    "                \n",
    "                combined_text = \"\".join(combined_content)\n",
    "                chunks = split_markdown_dynamic(combined_text, api_type)\n",
    "                logging.info(f\"Split combined content into {len(chunks)} chunks\")\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks, 1):\n",
    "                    logging.info(f\"Processing chunk {chunk_idx}/{len(chunks)} of combined files\")\n",
    "                    response = make_api_request(client, api_type, chunk, check_limits,\n",
    "                                               ig_name, ig_version, chunk_idx, len(chunks))\n",
    "                    all_incose_requirements.append(response)\n",
    "                    time.sleep(config[\"delay_between_chunks\"])\n",
    "                \n",
    "                processed_files.extend(group)\n",
    "            \n",
    "            # Add delay between file groups\n",
    "            time.sleep(config[\"delay_between_batches\"])\n",
    "        \n",
    "        # Combine all requirements into a full INCOSE SRS document\n",
    "        srs_document = \"\"\n",
    "        for req_section in all_incose_requirements:\n",
    "            # Skip the intro text if it's present to avoid duplication\n",
    "            # Look for the first requirement section\n",
    "            if \"## REQ-\" in req_section:\n",
    "                # Find the index of the first requirement\n",
    "                req_start_idx = req_section.find(\"## REQ-\")\n",
    "                if req_start_idx > 0:\n",
    "                    # Only add the requirements part, not any introductory text\n",
    "                    srs_document += req_section[req_start_idx:]\n",
    "                else:\n",
    "                    srs_document += req_section\n",
    "            else:\n",
    "                # Add any content that doesn't contain requirements\n",
    "                # (this could be informational text related to requirements)\n",
    "                srs_document += req_section\n",
    "        \n",
    "        # Save INCOSE SRS document to markdown file\n",
    "        output_directory = os.path.join(PROJECT_ROOT, 'reqs_extraction', 'processed_output')\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Use API name in the filename\n",
    "        srs_output_file = os.path.join(output_directory, f\"{api_type}_reqs_list_v1_{timestamp}.md\")\n",
    "        \n",
    "        with open(srs_output_file, 'w') as f:\n",
    "            f.write(srs_document)\n",
    "        \n",
    "        logging.info(f\"Completed processing {len(processed_files)} files\")\n",
    "        logging.info(f\"Generated requirements document saved to {srs_output_file}\")\n",
    "        \n",
    "        # Now extract just the requirements list using the file-based approach\n",
    "        try:\n",
    "            requirements_list = extract_requirements_list_from_file(client, api_type, srs_output_file, check_limits)\n",
    "            \n",
    "            # Save the requirements list to a separate file\n",
    "            req_list_output_file = os.path.join(output_directory, f\"{ig_name.lower().replace(' ', '_')}_{api_type}_requirements_list_{timestamp}.md\")\n",
    "            \n",
    "            with open(req_list_output_file, 'w') as f:\n",
    "                f.write(requirements_list)\n",
    "                \n",
    "            logging.info(f\"Generated clean requirements list saved to {req_list_output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting requirements list: {str(e)}\")\n",
    "            requirements_list = \"Error extracting requirements list. See main SRS document instead.\"\n",
    "            req_list_output_file = None\n",
    "        \n",
    "        return {\n",
    "            \"processed_files\": processed_files,\n",
    "            \"srs_document\": srs_document,\n",
    "            \"output_file\": srs_output_file,\n",
    "            \"requirements_list\": requirements_list,\n",
    "            \"requirements_file\": req_list_output_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing content: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing with gemini...\n",
      "INFO:root:Starting processing with gemini on directory: /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned\n",
      "INFO:root:Found 7 markdown files:\n",
      "INFO:root:  - implementation.md\n",
      "INFO:root:  - examples.md\n",
      "INFO:root:  - profiles.md\n",
      "INFO:root:  - ChangeHistory.md\n",
      "INFO:root:  - artifacts.md\n",
      "INFO:root:  - index.md\n",
      "INFO:root:  - CapabilityStatement_plan_net.md\n",
      "INFO:root:Organized 7 files into 6 processing groups\n",
      "INFO:root:Processing combined group of 2 files\n",
      "INFO:root:Split combined content into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of combined files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Implementation Guide with Gemini...\n",
      "This may take several minutes depending on the size of the Implementation Guide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing single file: index.md\n",
      "INFO:root:Split index.md into 2 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/2 of index.md\n",
      "INFO:root:Processing chunk 2/2 of index.md\n",
      "INFO:root:Processing single file: examples.md\n",
      "INFO:root:Split examples.md into 3 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/3 of examples.md\n",
      "INFO:root:Processing chunk 2/3 of examples.md\n",
      "INFO:root:Processing chunk 3/3 of examples.md\n",
      "INFO:root:Processing single file: implementation.md\n",
      "INFO:root:Split implementation.md into 3 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/3 of implementation.md\n",
      "INFO:root:Processing chunk 2/3 of implementation.md\n",
      "INFO:root:Processing chunk 3/3 of implementation.md\n",
      "INFO:root:Processing single file: CapabilityStatement_plan_net.md\n",
      "INFO:root:Split CapabilityStatement_plan_net.md into 5 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/5 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Processing chunk 2/5 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Processing chunk 3/5 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Processing chunk 4/5 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Processing chunk 5/5 of CapabilityStatement_plan_net.md\n",
      "INFO:root:Processing single file: artifacts.md\n",
      "INFO:root:Split artifacts.md into 7 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/7 of artifacts.md\n",
      "INFO:root:Processing chunk 2/7 of artifacts.md\n",
      "INFO:root:Processing chunk 3/7 of artifacts.md\n",
      "INFO:root:Processing chunk 4/7 of artifacts.md\n",
      "INFO:root:Processing chunk 5/7 of artifacts.md\n",
      "INFO:root:Processing chunk 6/7 of artifacts.md\n",
      "INFO:root:Processing chunk 7/7 of artifacts.md\n",
      "INFO:root:Completed processing 7 files\n",
      "INFO:root:Generated requirements document saved to /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/processed_output/plan_net_gemini_20250402_145733.md\n",
      "INFO:root:Extracting clean requirements list from file using gemini...\n",
      "INFO:root:Found 1 requirements to process in chunks\n",
      "WARNING:root:Found extremely large requirement (135824 chars) that exceeds chunk size\n",
      "INFO:root:Processing large requirement with special handling\n",
      "INFO:root:Successfully processed large requirement REQ-1\n",
      "INFO:root:Successfully extracted all 1 requirements\n",
      "INFO:root:Generated clean requirements list saved to /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/processed_output/plan_net_gemini_requirements_list_20250402_145733.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing complete!\n",
      "Generated requirements document: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/processed_output/plan_net_gemini_20250402_145733.md\n",
      "Generated clean requirements list: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/processed_output/plan_net_gemini_requirements_list_20250402_145733.md\n",
      "Processed 7 files\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Main execution script\n",
    "if __name__ == \"__main__\":\n",
    "    # Define input and output directories using absolute paths\n",
    "    markdown_dir = MARKDOWN_DIR\n",
    "    output_directory = os.path.join(PROJECT_ROOT, 'reqs_extraction', 'processed_output')\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Get IG name and version from user input or use defaults\n",
    "    ig_name = input(\"Enter Implementation Guide name (default 'FHIR Implementation Guide'): \") or \"FHIR Implementation Guide\"\n",
    "    ig_version = input(\"Enter Implementation Guide version (default '1.0.0'): \") or \"1.0.0\"\n",
    "\n",
    "    # Choose which API to use\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 2): \") or \"2\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"gemini\")\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"Processing with {api_type}...\")\n",
    "        print(f\"\\nProcessing Implementation Guide with {api_type.capitalize()}...\")\n",
    "        print(f\"This may take several minutes depending on the size of the Implementation Guide.\")\n",
    "        \n",
    "        # Process the markdown files and generate direct INCOSE SRS document\n",
    "        api_results = process_markdown_content_for_incose_srs(\n",
    "            api_type=api_type, \n",
    "            markdown_dir=markdown_dir,\n",
    "            ig_name=ig_name,\n",
    "            ig_version=ig_version\n",
    "        )\n",
    "        \n",
    "        # Output the results to the user\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Processing complete!\")\n",
    "        print(f\"Generated requirements document: {api_results['output_file']}\")\n",
    "        print(f\"Generated clean requirements list: {api_results['requirements_file']}\")\n",
    "        print(f\"Processed {len(api_results['processed_files'])} files\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {api_type}: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log file for more details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
