{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Summarization of IG Documents: Requirements Extraction\n",
    "This script aims to develop a structure to send large amounts of text/content to LLM APIs through multiple calls. The current approach takes in all JSON files from the Plan Net IG and key narrative information in markdown form (formerly extracted from HTML files). The script then analyzes each type of information in batches, and creates a meta-list of all requirements extracted from those documents. The goal is to identify if this approach can produce all technical information at an appropriate level of deatil that an LLM would need to know to help design a test kit for a given IG.\n",
    "\n",
    "First attempts: We were able to run through the script fully using the Claude API with all JSONs and markdown content. The process took over 93 minutes. The output requirements list is saved in the file (processed_output/test_requirements_claude1.json). \n",
    "\n",
    "In progress: \n",
    "- Adding images back in, revising the prompting based on Inferno requirements extraction process documentation\n",
    "- Comparing LLM results\n",
    "- Reviewing LangChain capabilities to improve document loading and summary quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPORTS AND BASIC SETUP\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "from json_repair import repair_json\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from anthropic import RateLimitError\n",
    "from anthropic import Anthropic\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "\n",
    "# Basic setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "CERT_PATH = '/opt/homebrew/etc/openssl@3/cert.pem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_CONFIGS = {\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-1.5-pro-001\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"batch_size\": 1,  # Process one at a time\n",
    "        \"delay_between_chunks\": 5,\n",
    "        \"delay_between_batches\": 30,\n",
    "        \"requests_per_minute\": 30,  # More conservative rate limit\n",
    "        \"max_requests_per_day\": 60000,\n",
    "        \"delay_between_requests\": 3,\n",
    "        \"timeout\": 120  # Longer timeout for larger content\n",
    "    }\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"gemini\": \"\"\"Analyze FHIR Implementation Guide content to identify \n",
    "                 testable requirements as a Healthcare Integration Test Engineer.\"\"\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_json_files(source_folder='full-ig/site', destination_folder='full-ig/json_only'):\n",
    "    \"\"\"Copy and filter relevant JSON files\"\"\"\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    json_files = []\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        if (file_name.endswith('.json') and \n",
    "            not any(file_name.endswith(ext) for ext in [\n",
    "                '.ttl.json', '.jsonld.json', '.xml.json', '.change.history.json'\n",
    "            ])):\n",
    "            json_files.append(file_name)\n",
    "            shutil.copy(os.path.join(source_folder, file_name), destination_folder)\n",
    "            \n",
    "    logging.info(f\"Copied {len(json_files)} JSON files to {destination_folder}\")\n",
    "    return json_files\n",
    "\n",
    "def prepare_json_for_processing(json_file_path: str) -> Union[dict, list]:\n",
    "    \"\"\"Read and prepare JSON file for processing, handling UTF-8 BOM\"\"\"\n",
    "    try:\n",
    "        # First try reading with utf-8-sig encoding to handle BOM\n",
    "        with open(json_file_path, 'r', encoding='utf-8-sig') as f:\n",
    "            data = json.load(f)\n",
    "            return data['entry'] if isinstance(data, dict) and 'entry' in data else data\n",
    "    except (json.JSONDecodeError, UnicodeError) as e:\n",
    "        # If that fails, try with regular utf-8\n",
    "        try:\n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                return data['entry'] if isinstance(data, dict) and 'entry' in data else data\n",
    "        except (json.JSONDecodeError, UnicodeError) as e:\n",
    "            # If the file is corrupted, log it and skip\n",
    "            logging.error(f\"Error processing {json_file_path}: {str(e)}\")\n",
    "            return []  # Return empty list for corrupted files\n",
    "\n",
    "def split_json(json_data: Union[dict, list], max_size: int = 2000) -> List[list]:\n",
    "    \"\"\"Split JSON into chunks while maintaining object integrity\"\"\"\n",
    "    if isinstance(json_data, dict):\n",
    "        json_data = [json_data]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for item in json_data:\n",
    "        item_size = len(json.dumps(item))\n",
    "        if item_size > max_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            chunks.append([item])\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        elif current_size + item_size > max_size:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = [item]\n",
    "            current_size = item_size\n",
    "        else:\n",
    "            current_chunk.append(item)\n",
    "            current_size += item_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "def clean_markdown(text: str) -> str:\n",
    "    \"\"\"Clean markdown content\"\"\"\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\\\(.)', r'\\1', text)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'[-\\s]*\\n[-\\s]*', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_markdown(content: str, max_size: int = 2000) -> List[str]:\n",
    "    \"\"\"Split markdown into manageable chunks\"\"\"\n",
    "    chunks = []\n",
    "    lines = content.split('\\n')\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line_size = len(line)\n",
    "        if current_size + line_size > max_size:\n",
    "            if current_chunk:\n",
    "                chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [line]\n",
    "            current_size = line_size\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_size += line_size\n",
    "            \n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def consolidate_jsons(base_directory: str = 'full-ig/json_only'):\n",
    "    \"\"\"Consolidate related JSON files while maintaining integrity\"\"\"\n",
    "    subdirs = [d for d in os.listdir(base_directory) \n",
    "              if os.path.isdir(os.path.join(base_directory, d))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        folder_path = os.path.join(base_directory, subdir)\n",
    "        combined_data = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.json'):\n",
    "                try:\n",
    "                    with open(os.path.join(folder_path, filename), 'r') as f:\n",
    "                        json_content = json.load(f)\n",
    "                        if isinstance(json_content, dict) and 'entry' in json_content:\n",
    "                            combined_data.extend(json_content['entry'])\n",
    "                        else:\n",
    "                            combined_data.append(json_content)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.error(f\"Error decoding JSON from {filename}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if combined_data:\n",
    "            output_filename = f\"{subdir}_combined.json\"\n",
    "            output_path = os.path.join(base_directory, output_filename)\n",
    "            try:\n",
    "                with open(output_path, 'w') as outfile:\n",
    "                    json.dump({\n",
    "                        \"resourceType\": subdir,\n",
    "                        \"total\": len(combined_data),\n",
    "                        \"entry\": combined_data\n",
    "                    }, outfile, indent=2)\n",
    "                logging.info(f\"Created {output_filename} with {len(combined_data)} entries\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error writing {output_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rate_limiter():\n",
    "    \"\"\"Create a rate limiter state dictionary for all APIs\"\"\"\n",
    "    return {\n",
    "        api: {\n",
    "            'requests': [],\n",
    "            'daily_requests': 0,\n",
    "            'last_reset': time.time()\n",
    "        }\n",
    "        for api in API_CONFIGS.keys()\n",
    "    }\n",
    "\n",
    "def check_rate_limits(rate_limiter: dict, api: str):\n",
    "    \"\"\"Check and wait if rate limits would be exceeded\"\"\"\n",
    "    if api not in rate_limiter:\n",
    "        raise ValueError(f\"Unknown API: {api}\")\n",
    "        \n",
    "    now = time.time()\n",
    "    state = rate_limiter[api]\n",
    "    config = API_CONFIGS[api]\n",
    "    \n",
    "    # Reset daily counts if needed\n",
    "    day_seconds = 24 * 60 * 60\n",
    "    if now - state['last_reset'] >= day_seconds:\n",
    "        state['daily_requests'] = 0\n",
    "        state['last_reset'] = now\n",
    "    \n",
    "    # Check daily limit\n",
    "    if state['daily_requests'] >= config['max_requests_per_day']:\n",
    "        raise Exception(f\"{api} daily request limit exceeded\")\n",
    "    \n",
    "    # Remove old requests outside the current minute\n",
    "    state['requests'] = [\n",
    "        req_time for req_time in state['requests']\n",
    "        if now - req_time < 60\n",
    "    ]\n",
    "    \n",
    "    # Wait if at rate limit\n",
    "    if len(state['requests']) >= config['requests_per_minute']:\n",
    "        sleep_time = 60 - (now - state['requests'][0])\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        state['requests'] = state['requests'][1:]\n",
    "    \n",
    "    # Add minimum delay between requests\n",
    "    if state['requests'] and now - state['requests'][-1] < config['delay_between_requests']:\n",
    "        time.sleep(config['delay_between_requests'])\n",
    "    \n",
    "    # Record this request\n",
    "    state['requests'].append(now)\n",
    "    state['daily_requests'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=4, max=120),\n",
    "    stop=stop_after_attempt(8),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def make_api_request(client, api_type: str, content: Union[str, dict, list], rate_limit_func) -> str:\n",
    "    \"\"\"Make rate-limited API request with retries\"\"\"\n",
    "    rate_limit_func()\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    formatted_content = format_content_for_api(content, api_type)\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"gemini\":\n",
    "            try:\n",
    "                # Extract the text content for Gemini\n",
    "                prompt_text = formatted_content[\"contents\"][0][\"parts\"][0][\"text\"]\n",
    "                \n",
    "                response = client.generate_content(\n",
    "                    prompt_text,\n",
    "                    generation_config={\n",
    "                        \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                        \"temperature\": config[\"temperature\"]\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if hasattr(response, 'text'):\n",
    "                    return response.text\n",
    "                elif response.candidates:\n",
    "                    return response.candidates[0].content.parts[0].text\n",
    "                else:\n",
    "                    raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Gemini API error: {str(e)}\")\n",
    "                if \"429\" in str(e) or \"quota\" in str(e).lower():\n",
    "                    logging.warning(\"Gemini quota limit hit, backing off\")\n",
    "                    time.sleep(30)  # Additional backoff for quota limits\n",
    "                raise\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content_batch(api_type: str, contents: List[Union[str, dict]], \n",
    "                        config: dict, client, rate_limit_func) -> List[str]:\n",
    "    \"\"\"Process a batch of content with rate limiting\"\"\"\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        result = make_api_request(client, api_type, content, rate_limit_func)\n",
    "        results.append(result)\n",
    "        time.sleep(config[\"delay_between_chunks\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_requirements_extraction_prompt(content: Union[str, dict, list]) -> str:\n",
    "    \"\"\"Create a prompt that aligns with Inferno's requirements extraction process\"\"\"\n",
    "    \n",
    "    return f\"\"\"Analyze this FHIR Implementation Guide content to extract precise requirements following these guidelines:\n",
    "\n",
    "For each requirement you identify, provide:\n",
    "\n",
    "1. REQUIREMENT TEXT\n",
    "- Extract direct quotes from the source\n",
    "- For compound requirements, split into atomic requirements\n",
    "- Maintain context when splitting\n",
    "- Use [...] for added clarifications\n",
    "- Use ... for removed text\n",
    "- Format using markdown syntax for code blocks, italics, etc.\n",
    "\n",
    "2. REQUIREMENT METADATA\n",
    "- Conformance Level (SHALL, SHOULD, MAY, SHOULD NOT, SHALL NOT)\n",
    "- Actor(s) the requirement applies to\n",
    "- Whether the requirement is conditional (True/False)\n",
    "- Any sub-requirements or referenced requirements\n",
    "\n",
    "3. SOURCE TRACEABILITY\n",
    "- Note the specific section or location this requirement comes from\n",
    "- For JSON content, note the specific resource type and element\n",
    "\n",
    "When analyzing content, focus on:\n",
    "\n",
    "a) Making requirements atomic and testable\n",
    "b) Maintaining the original text while adding necessary context\n",
    "c) Identifying implicit requirements for each actor\n",
    "d) Distinguishing between conjunctive (\"and\") and disjunctive (\"or\") requirements\n",
    "e) Capturing terminology bindings and must-support elements\n",
    "f) Noting RESTful API conformance requirements\n",
    "g) Identifying conditional requirements\n",
    "\n",
    "Content to analyze:\n",
    "{json.dumps(content, indent=2) if isinstance(content, (dict, list)) else content}\n",
    "\n",
    "Format each requirement as:\n",
    "```\n",
    "Requirement Text: <quoted text with [...] for clarifications and ... for elisions>\n",
    "Conformance: <conformance level>\n",
    "Actor: <actor name(s)>\n",
    "Conditional: <True/False>\n",
    "Sub-Requirements: <list of referenced requirements if any>\n",
    "Source: <specific location in documentation>\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_requirements_output(output: str) -> List[Dict]:\n",
    "    \"\"\"Process LLM output into standardized requirements format\"\"\"\n",
    "    requirements = []\n",
    "    current_req = {}\n",
    "    \n",
    "    # Split output into individual requirements\n",
    "    req_blocks = output.split('\\n\\n')\n",
    "    \n",
    "    for block in req_blocks:\n",
    "        if block.strip().startswith('Requirement Text:'):\n",
    "            # Save previous requirement if it exists\n",
    "            if current_req:\n",
    "                requirements.append(current_req)\n",
    "                current_req = {}\n",
    "            \n",
    "            # Parse new requirement\n",
    "            lines = block.strip().split('\\n')\n",
    "            for line in lines:\n",
    "                if ': ' in line:\n",
    "                    key, value = line.split(': ', 1)\n",
    "                    key = key.lower().replace(' ', '_')\n",
    "                    current_req[key] = value.strip()\n",
    "    \n",
    "    # Add final requirement\n",
    "    if current_req:\n",
    "        requirements.append(current_req)\n",
    "        \n",
    "    return requirements\n",
    "\n",
    "def save_requirements_to_csv(requirements: List[Dict], output_file: str):\n",
    "    \"\"\"Save extracted requirements to CSV format matching Inferno's structure\"\"\"\n",
    "    df = pd.DataFrame(requirements)\n",
    "    \n",
    "    # Rename columns to match Inferno's format\n",
    "    column_mapping = {\n",
    "        'requirement_text': 'Requirement',\n",
    "        'conformance': 'Conformance',\n",
    "        'actor': 'Actor',\n",
    "        'conditional': 'Conditionality',\n",
    "        'source': 'URL',\n",
    "        'sub_requirements': 'Sub-Requirement(s)'\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Add required columns if missing\n",
    "    required_columns = ['Req Set', 'Id'] + list(column_mapping.values())\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "            \n",
    "    # Generate sequential IDs if not present\n",
    "    if 'Id' in df.columns and df['Id'].isna().all():\n",
    "        df['Id'] = range(1, len(df) + 1)\n",
    "        \n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_requirements_prompt(content_summaries: Dict[str, List[str]]) -> str:\n",
    "    \"\"\"Create prompt for generating test requirements\"\"\"\n",
    "    return f\"\"\"Synthesize these content summaries into comprehensive test requirements:\n",
    "\n",
    "    {json.dumps(content_summaries, indent=2)}\n",
    "\n",
    "    Extract specific requirements for:\n",
    "    1. Resource Profiles and Must Support Elements\n",
    "    2. Search Parameters and Operations\n",
    "    3. RESTful API Conformance\n",
    "    4. Value Sets and Terminology\n",
    "    5. Business Rules and Constraints\n",
    "    6. Security and Authorization\n",
    "\n",
    "    Focus on precise, testable criteria.\"\"\"\n",
    "\n",
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try: \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        gemini.configure(api_key=gemini_api_key)\n",
    "        gemini_client = gemini.GenerativeModel(\n",
    "            model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        return {\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_all_content(api_type: str, base_directory: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process all content and generate requirements in Inferno format\"\"\"\n",
    "    clients = setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = API_CONFIGS[api_type]\n",
    "    rate_limiter = create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    try:\n",
    "        # Process JSON files\n",
    "        json_files = copy_json_files()\n",
    "        all_requirements = []\n",
    "        total_files = len(json_files)\n",
    "        \n",
    "        logging.info(f\"Starting to process {total_files} JSON files\")\n",
    "        for idx, json_file in enumerate(json_files, 1):\n",
    "            logging.info(f\"Processing file {idx}/{total_files}: {json_file}\")\n",
    "            try:\n",
    "                json_data = prepare_json_for_processing(\n",
    "                    os.path.join(base_directory, 'json_only', json_file)\n",
    "                )\n",
    "                chunks = split_json(json_data)\n",
    "                logging.info(f\"Split {json_file} into {len(chunks)} chunks\")\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks, 1):\n",
    "                    logging.info(f\"Processing chunk {chunk_idx}/{len(chunks)} of {json_file}\")\n",
    "                    response = make_api_request(client, api_type, chunk, check_limits)\n",
    "                    chunk_requirements = process_llm_requirements_output(response)\n",
    "                    all_requirements.extend(chunk_requirements)\n",
    "                    logging.info(f\"Extracted {len(chunk_requirements)} requirements from chunk\")\n",
    "                    time.sleep(config[\"delay_between_chunks\"])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {json_file}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        # Process markdown files\n",
    "        markdown_dir = os.path.join(base_directory, 'markdown')\n",
    "        if os.path.exists(markdown_dir):\n",
    "            for md_file in os.listdir(markdown_dir):\n",
    "                if md_file.endswith('.md'):\n",
    "                    with open(os.path.join(markdown_dir, md_file), 'r') as f:\n",
    "                        content = clean_markdown(f.read())\n",
    "                    chunks = split_markdown(content)\n",
    "                    \n",
    "                    for chunk in chunks:\n",
    "                        response = make_api_request(client, api_type, chunk, check_limits)\n",
    "                        chunk_requirements = process_llm_requirements_output(response)\n",
    "                        all_requirements.extend(chunk_requirements)\n",
    "                        time.sleep(config[\"delay_between_chunks\"])\n",
    "        \n",
    "        # Save requirements to CSV\n",
    "        output_file = f\"requirements_{api_type}_extracted.csv\"\n",
    "        save_requirements_to_csv(all_requirements, output_file)\n",
    "        \n",
    "        return {\n",
    "            \"requirements\": all_requirements,\n",
    "            \"output_file\": output_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing content: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_content_for_api(content: Union[str, dict, list], api_type: str) -> Union[str, List[dict], dict]:\n",
    "    \"\"\"Format content appropriately for each API using Inferno's requirements structure\"\"\"\n",
    "    \n",
    "    # Create the base requirements extraction prompt\n",
    "    base_prompt = create_requirements_extraction_prompt(content)\n",
    "\n",
    "    if api_type == \"gemini\":\n",
    "        # Format specifically for Gemini's content structure\n",
    "        return {\n",
    "            \"contents\": [{\n",
    "                \"parts\": [{\n",
    "                    \"text\": base_prompt\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    # For other APIs, return just the text\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing with gemini...\n",
      "INFO:root:Copied 166 JSON files to full-ig/json_only\n",
      "INFO:root:Starting to process 166 JSON files\n",
      "INFO:root:Processing file 1/166: Location-PharmLoc1.json\n",
      "INFO:root:Split Location-PharmLoc1.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-PharmLoc1.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 2/166: StructureDefinition-plannet-Network.json\n",
      "INFO:root:Split StructureDefinition-plannet-Network.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-Network.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 3/166: ValueSet-EndpointPayloadTypeVS.json\n",
      "INFO:root:Split ValueSet-EndpointPayloadTypeVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-EndpointPayloadTypeVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 4/166: CodeSystem-DeliveryMethodCS.json\n",
      "INFO:root:Split CodeSystem-DeliveryMethodCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-DeliveryMethodCS.json\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing file 5/166: ValueSet-InsuranceProductTypeVS.json\n",
      "INFO:root:Split ValueSet-InsuranceProductTypeVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-InsuranceProductTypeVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 6/166: OrganizationAffiliation-BurrClinicAffil.json\n",
      "INFO:root:Split OrganizationAffiliation-BurrClinicAffil.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of OrganizationAffiliation-BurrClinicAffil.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 7/166: SearchParameter-organizationaffiliation-specialty.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-specialty.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-specialty.json\n",
      "INFO:root:Extracted 5 requirements from chunk\n",
      "INFO:root:Processing file 8/166: SearchParameter-insuranceplan-coverage-area.json\n",
      "INFO:root:Split SearchParameter-insuranceplan-coverage-area.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-insuranceplan-coverage-area.json\n",
      "INFO:root:Extracted 4 requirements from chunk\n",
      "INFO:root:Processing file 9/166: SearchParameter-organizationaffiliation-period.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-period.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-period.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 10/166: StructureDefinition-location-reference.json\n",
      "INFO:root:Split StructureDefinition-location-reference.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-location-reference.json\n",
      "INFO:root:Extracted 9 requirements from chunk\n",
      "INFO:root:Processing file 11/166: CodeSystem-QualificationStatusCS.json\n",
      "INFO:root:Split CodeSystem-QualificationStatusCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-QualificationStatusCS.json\n",
      "INFO:root:Extracted 1 requirements from chunk\n",
      "INFO:root:Processing file 12/166: SearchParameter-practitionerrole-specialty.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-specialty.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-specialty.json\n",
      "INFO:root:Extracted 5 requirements from chunk\n",
      "INFO:root:Processing file 13/166: SearchParameter-insuranceplan-plan-type.json\n",
      "INFO:root:Split SearchParameter-insuranceplan-plan-type.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-insuranceplan-plan-type.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 14/166: CodeSystem-InsurancePlanTypeCS.json\n",
      "INFO:root:Split CodeSystem-InsurancePlanTypeCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-InsurancePlanTypeCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 15/166: SearchParameter-practitionerrole-role.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-role.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-role.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 16/166: HealthcareService-HansSoloService.json\n",
      "INFO:root:Split HealthcareService-HansSoloService.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-HansSoloService.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 17/166: SearchParameter-organizationaffiliation-service.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-service.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-service.json\n",
      "INFO:root:Extracted 8 requirements from chunk\n",
      "INFO:root:Processing file 18/166: ValueSet-EndpointUsecaseVS.json\n",
      "INFO:root:Split ValueSet-EndpointUsecaseVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-EndpointUsecaseVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 19/166: usage-stats.json\n",
      "INFO:root:Split usage-stats.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of usage-stats.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 20/166: OrganizationAffiliation-HartfordOrthopedicAffil.json\n",
      "INFO:root:Split OrganizationAffiliation-HartfordOrthopedicAffil.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of OrganizationAffiliation-HartfordOrthopedicAffil.json\n",
      "INFO:root:Extracted 19 requirements from chunk\n",
      "INFO:root:Processing file 21/166: StructureDefinition-network-reference.json\n",
      "INFO:root:Split StructureDefinition-network-reference.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-network-reference.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 22/166: StructureDefinition-communication-proficiency.json\n",
      "INFO:root:Split StructureDefinition-communication-proficiency.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-communication-proficiency.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 23/166: OrganizationAffiliation-ConnHIEAffil.json\n",
      "INFO:root:Split OrganizationAffiliation-ConnHIEAffil.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of OrganizationAffiliation-ConnHIEAffil.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 24/166: SearchParameter-healthcareservice-location.json\n",
      "INFO:root:Split SearchParameter-healthcareservice-location.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-healthcareservice-location.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 25/166: Organization-HamiltonClinic.json\n",
      "INFO:root:Split Organization-HamiltonClinic.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-HamiltonClinic.json\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing file 26/166: CapabilityStatement-plan-net.json\n",
      "INFO:root:Split CapabilityStatement-plan-net.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CapabilityStatement-plan-net.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 27/166: ValueSet-AcceptingPatientsVS.json\n",
      "INFO:root:Split ValueSet-AcceptingPatientsVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-AcceptingPatientsVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 28/166: Organization-AcmeofCTPremNet.json\n",
      "INFO:root:Split Organization-AcmeofCTPremNet.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-AcmeofCTPremNet.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 29/166: SearchParameter-practitionerrole-period.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-period.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-period.json\n",
      "INFO:root:Extracted 3 requirements from chunk\n",
      "INFO:root:Processing file 30/166: ValueSet-AccessibilityVS.json\n",
      "INFO:root:Split ValueSet-AccessibilityVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-AccessibilityVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 31/166: ValueSet-IndividualAndGroupSpecialtiesVS.json\n",
      "INFO:root:Split ValueSet-IndividualAndGroupSpecialtiesVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-IndividualAndGroupSpecialtiesVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 32/166: PractitionerRole-JoeSmithRole1.json\n",
      "INFO:root:Split PractitionerRole-JoeSmithRole1.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of PractitionerRole-JoeSmithRole1.json\n",
      "INFO:root:Extracted 9 requirements from chunk\n",
      "INFO:root:Processing file 33/166: ValueSet-OrgTypeVS.json\n",
      "INFO:root:Split ValueSet-OrgTypeVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-OrgTypeVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 34/166: SearchParameter-location-address-state.json\n",
      "INFO:root:Split SearchParameter-location-address-state.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-location-address-state.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 35/166: OrganizationAffiliation-PharmChainAffil1.json\n",
      "INFO:root:Split OrganizationAffiliation-PharmChainAffil1.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of OrganizationAffiliation-PharmChainAffil1.json\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing file 36/166: SearchParameter-practitioner-given-name.json\n",
      "INFO:root:Split SearchParameter-practitioner-given-name.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitioner-given-name.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 37/166: SearchParameter-insuranceplan-identifier.json\n",
      "INFO:root:Split SearchParameter-insuranceplan-identifier.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-insuranceplan-identifier.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 38/166: ImplementationGuide-hl7.fhir.us.davinci-pdex-plan-net.json\n",
      "INFO:root:Split ImplementationGuide-hl7.fhir.us.davinci-pdex-plan-net.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ImplementationGuide-hl7.fhir.us.davinci-pdex-plan-net.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 39/166: OrganizationAffiliation-HamiltonClinicAffil.json\n",
      "INFO:root:Split OrganizationAffiliation-HamiltonClinicAffil.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of OrganizationAffiliation-HamiltonClinicAffil.json\n",
      "INFO:root:Extracted 7 requirements from chunk\n",
      "INFO:root:Processing file 40/166: SearchParameter-healthcareservice-endpoint.json\n",
      "INFO:root:Split SearchParameter-healthcareservice-endpoint.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-healthcareservice-endpoint.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 41/166: CodeSystem-HealthcareServiceCategoryCS.json\n",
      "INFO:root:Split CodeSystem-HealthcareServiceCategoryCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-HealthcareServiceCategoryCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 42/166: SearchParameter-endpoint-organization.json\n",
      "INFO:root:Split SearchParameter-endpoint-organization.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-endpoint-organization.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 43/166: SearchParameter-insuranceplan-administered-by.json\n",
      "INFO:root:Split SearchParameter-insuranceplan-administered-by.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-insuranceplan-administered-by.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 44/166: PractitionerRole-HansSoloRole1.json\n",
      "INFO:root:Split PractitionerRole-HansSoloRole1.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of PractitionerRole-HansSoloRole1.json\n",
      "INFO:root:Extracted 14 requirements from chunk\n",
      "INFO:root:Processing file 45/166: ValueSet-InsurancePlanTypeVS.json\n",
      "INFO:root:Split ValueSet-InsurancePlanTypeVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-InsurancePlanTypeVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 46/166: SearchParameter-organization-address.json\n",
      "INFO:root:Split SearchParameter-organization-address.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organization-address.json\n",
      "INFO:root:Extracted 5 requirements from chunk\n",
      "INFO:root:Processing file 47/166: StructureDefinition-plannet-Location.json\n",
      "INFO:root:Split StructureDefinition-plannet-Location.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-Location.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 48/166: Organization-Hospital.json\n",
      "INFO:root:Split Organization-Hospital.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-Hospital.json\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing file 49/166: HealthcareService-HamiltonClinicServices.json\n",
      "INFO:root:Split HealthcareService-HamiltonClinicServices.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-HamiltonClinicServices.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 50/166: InsurancePlan-AcmeQHPBronze.json\n",
      "INFO:root:Split InsurancePlan-AcmeQHPBronze.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of InsurancePlan-AcmeQHPBronze.json\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing file 51/166: Location-StateOfCTLocation.json\n",
      "INFO:root:Split Location-StateOfCTLocation.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-StateOfCTLocation.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 52/166: Location-HospLoc1.json\n",
      "INFO:root:Split Location-HospLoc1.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-HospLoc1.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 53/166: SearchParameter-insuranceplan-name.json\n",
      "INFO:root:Split SearchParameter-insuranceplan-name.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-insuranceplan-name.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 54/166: StructureDefinition-plannet-Endpoint.json\n",
      "INFO:root:Split StructureDefinition-plannet-Endpoint.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-Endpoint.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 55/166: ValueSet-MinEndpointConnectionTypeVS.json\n",
      "INFO:root:Split ValueSet-MinEndpointConnectionTypeVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-MinEndpointConnectionTypeVS.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 56/166: ValueSet-NonIndividualSpecialtyAndDegreeLicenseCertificateVS.json\n",
      "INFO:root:Split ValueSet-NonIndividualSpecialtyAndDegreeLicenseCertificateVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-NonIndividualSpecialtyAndDegreeLicenseCertificateVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 57/166: StructureDefinition-delivery-method.json\n",
      "INFO:root:Split StructureDefinition-delivery-method.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-delivery-method.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 58/166: PractitionerRole-CounselorRole1.json\n",
      "INFO:root:Split PractitionerRole-CounselorRole1.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of PractitionerRole-CounselorRole1.json\n",
      "INFO:root:Extracted 8 requirements from chunk\n",
      "INFO:root:Processing file 59/166: StructureDefinition-accessibility.json\n",
      "INFO:root:Split StructureDefinition-accessibility.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-accessibility.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 60/166: StructureDefinition-org-description.json\n",
      "INFO:root:Split StructureDefinition-org-description.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-org-description.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 61/166: SearchParameter-healthcareservice-coverage-area.json\n",
      "INFO:root:Split SearchParameter-healthcareservice-coverage-area.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-healthcareservice-coverage-area.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 62/166: SearchParameter-practitionerrole-network.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-network.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-network.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 63/166: CodeSystem-OrganizationAffiliationRoleCS.json\n",
      "INFO:root:Split CodeSystem-OrganizationAffiliationRoleCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-OrganizationAffiliationRoleCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 64/166: StructureDefinition-endpoint-usecase.json\n",
      "INFO:root:Split StructureDefinition-endpoint-usecase.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-endpoint-usecase.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 65/166: SearchParameter-healthcareservice-specialty.json\n",
      "INFO:root:Split SearchParameter-healthcareservice-specialty.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-healthcareservice-specialty.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 66/166: CodeSystem-OrgTypeCS.json\n",
      "INFO:root:Split CodeSystem-OrgTypeCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-OrgTypeCS.json\n",
      "INFO:root:Extracted 12 requirements from chunk\n",
      "INFO:root:Processing file 67/166: SearchParameter-healthcareservice-service-type.json\n",
      "INFO:root:Split SearchParameter-healthcareservice-service-type.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-healthcareservice-service-type.json\n",
      "INFO:root:Extracted 4 requirements from chunk\n",
      "INFO:root:Processing file 68/166: PractitionerRole-AnonRole.json\n",
      "INFO:root:Split PractitionerRole-AnonRole.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of PractitionerRole-AnonRole.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 69/166: HealthcareService-PharmChainCompService.json\n",
      "INFO:root:Split HealthcareService-PharmChainCompService.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-PharmChainCompService.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 70/166: SearchParameter-location-type.json\n",
      "INFO:root:Split SearchParameter-location-type.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-location-type.json\n",
      "INFO:root:Extracted 4 requirements from chunk\n",
      "INFO:root:Processing file 71/166: SearchParameter-practitionerrole-organization.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-organization.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-organization.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 72/166: Location-HansSoloClinic.json\n",
      "INFO:root:Split Location-HansSoloClinic.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-HansSoloClinic.json\n",
      "INFO:root:Extracted 16 requirements from chunk\n",
      "INFO:root:Processing file 73/166: ValueSet-HealthcareServiceTypeVS.json\n",
      "INFO:root:Split ValueSet-HealthcareServiceTypeVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-HealthcareServiceTypeVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 74/166: StructureDefinition-plannet-Practitioner.json\n",
      "INFO:root:Split StructureDefinition-plannet-Practitioner.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-Practitioner.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 75/166: CodeSystem-AcceptingPatientsCS.json\n",
      "INFO:root:Split CodeSystem-AcceptingPatientsCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-AcceptingPatientsCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 76/166: Practitioner-Counselor.json\n",
      "INFO:root:Split Practitioner-Counselor.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Practitioner-Counselor.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 77/166: SearchParameter-location-organization.json\n",
      "INFO:root:Split SearchParameter-location-organization.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-location-organization.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 78/166: SearchParameter-organization-type.json\n",
      "INFO:root:Split SearchParameter-organization-type.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organization-type.json\n",
      "INFO:root:Extracted 3 requirements from chunk\n",
      "INFO:root:Processing file 79/166: CodeSystem-LanguageProficiencyCS.json\n",
      "INFO:root:Split CodeSystem-LanguageProficiencyCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-LanguageProficiencyCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 80/166: HealthcareService-PharmChainMailService.json\n",
      "INFO:root:Split HealthcareService-PharmChainMailService.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-PharmChainMailService.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 81/166: Organization-BigBox.json\n",
      "INFO:root:Split Organization-BigBox.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-BigBox.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 82/166: ValueSet-PractitionerRoleVS.json\n",
      "INFO:root:Split ValueSet-PractitionerRoleVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-PractitionerRoleVS.json\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing file 83/166: HealthcareService-CancerClinicService.json\n",
      "INFO:root:Split HealthcareService-CancerClinicService.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-CancerClinicService.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 84/166: StructureDefinition-contactpoint-availabletime.json\n",
      "INFO:root:Split StructureDefinition-contactpoint-availabletime.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-contactpoint-availabletime.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 85/166: HealthcareService-BurrClinicServices.json\n",
      "INFO:root:Split HealthcareService-BurrClinicServices.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-BurrClinicServices.json\n",
      "INFO:root:Extracted 5 requirements from chunk\n",
      "INFO:root:Processing file 86/166: CodeSystem-AccessibilityCS.json\n",
      "INFO:root:Split CodeSystem-AccessibilityCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-AccessibilityCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 87/166: SearchParameter-organizationaffiliation-primary-organization.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-primary-organization.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-primary-organization.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 88/166: SearchParameter-practitionerrole-service.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-service.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-service.json\n",
      "INFO:root:Extracted 7 requirements from chunk\n",
      "INFO:root:Processing file 89/166: StructureDefinition-qualification.json\n",
      "INFO:root:Split StructureDefinition-qualification.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-qualification.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 90/166: ValueSet-SpecialtyAndDegreeLicenseCertificateVS.json\n",
      "INFO:root:Split ValueSet-SpecialtyAndDegreeLicenseCertificateVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-SpecialtyAndDegreeLicenseCertificateVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 91/166: SearchParameter-organizationaffiliation-participating-organization.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-participating-organization.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-participating-organization.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 92/166: expansions.json\n",
      "INFO:root:Split expansions.json into 24 chunks\n",
      "INFO:root:Processing chunk 1/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 2/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 3/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 4/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 5/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 6/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 7/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 8/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 9/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 10/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 11/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 12/24 of expansions.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 13/24 of expansions.json\n",
      "ERROR:root:Gemini API error: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error in gemini API request: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error processing file expansions.json: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "INFO:root:Processing file 93/166: SearchParameter-practitioner-family-name.json\n",
      "INFO:root:Split SearchParameter-practitioner-family-name.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitioner-family-name.json\n",
      "ERROR:root:Gemini API error: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error in gemini API request: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error processing file SearchParameter-practitioner-family-name.json: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "INFO:root:Processing file 94/166: SearchParameter-healthcareservice-organization.json\n",
      "INFO:root:Split SearchParameter-healthcareservice-organization.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-healthcareservice-organization.json\n",
      "ERROR:root:Gemini API error: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error in gemini API request: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error processing file SearchParameter-healthcareservice-organization.json: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "INFO:root:Processing file 95/166: Organization-PharmChain.json\n",
      "INFO:root:Split Organization-PharmChain.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-PharmChain.json\n",
      "ERROR:root:Gemini API error: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error in gemini API request: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error processing file Organization-PharmChain.json: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "INFO:root:Processing file 96/166: ValueSet-HealthcareServiceCategoryVS.json\n",
      "INFO:root:Split ValueSet-HealthcareServiceCategoryVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-HealthcareServiceCategoryVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 97/166: ValueSet-VirtualModalitiesVS.json\n",
      "INFO:root:Split ValueSet-VirtualModalitiesVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-VirtualModalitiesVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 98/166: Location-PharmLoc4.json\n",
      "INFO:root:Split Location-PharmLoc4.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-PharmLoc4.json\n",
      "INFO:root:Extracted 13 requirements from chunk\n",
      "INFO:root:Processing file 99/166: Organization-AcmeofCTStdNet.json\n",
      "INFO:root:Split Organization-AcmeofCTStdNet.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-AcmeofCTStdNet.json\n",
      "INFO:root:Extracted 5 requirements from chunk\n",
      "INFO:root:Processing file 100/166: Location-HospLoc2.json\n",
      "INFO:root:Split Location-HospLoc2.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-HospLoc2.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 101/166: CodeSystem-InsuranceProductTypeCS.json\n",
      "INFO:root:Split CodeSystem-InsuranceProductTypeCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-InsuranceProductTypeCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 102/166: SearchParameter-organization-partof.json\n",
      "INFO:root:Split SearchParameter-organization-partof.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organization-partof.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 103/166: SearchParameter-organizationaffiliation-role.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-role.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-role.json\n",
      "INFO:root:Extracted 5 requirements from chunk\n",
      "INFO:root:Processing file 104/166: ValueSet-EndpointConnectionTypeVS.json\n",
      "INFO:root:Split ValueSet-EndpointConnectionTypeVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-EndpointConnectionTypeVS.json\n",
      "INFO:root:Extracted 2 requirements from chunk\n",
      "INFO:root:Processing file 105/166: HealthcareService-HartfordOrthopedicServices.json\n",
      "INFO:root:Split HealthcareService-HartfordOrthopedicServices.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-HartfordOrthopedicServices.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 106/166: StructureDefinition-plannet-InsurancePlan.json\n",
      "INFO:root:Split StructureDefinition-plannet-InsurancePlan.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-InsurancePlan.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 107/166: ValueSet-SpecialtiesVS.json\n",
      "INFO:root:Split ValueSet-SpecialtiesVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-SpecialtiesVS.json\n",
      "ERROR:root:Gemini API error: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error in gemini API request: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "ERROR:root:Error processing file ValueSet-SpecialtiesVS.json: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "INFO:root:Processing file 108/166: CodeSystem-ProviderRoleCS.json\n",
      "INFO:root:Split CodeSystem-ProviderRoleCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-ProviderRoleCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 109/166: SearchParameter-practitionerrole-endpoint.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-endpoint.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-endpoint.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 110/166: SearchParameter-practitioner-name.json\n",
      "INFO:root:Split SearchParameter-practitioner-name.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitioner-name.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 111/166: canonicals.json\n",
      "INFO:root:Split canonicals.json into 13 chunks\n",
      "INFO:root:Processing chunk 1/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 2/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 3/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 4/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 5/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 6/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 7/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 8/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 9/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 10/13 of canonicals.json\n",
      "INFO:root:Extracted 1 requirements from chunk\n",
      "INFO:root:Processing chunk 11/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 12/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing chunk 13/13 of canonicals.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 112/166: CodeSystem-EndpointConnectionTypeCS.json\n",
      "INFO:root:Split CodeSystem-EndpointConnectionTypeCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-EndpointConnectionTypeCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 113/166: SearchParameter-location-address-postalcode.json\n",
      "INFO:root:Split SearchParameter-location-address-postalcode.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-location-address-postalcode.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 114/166: SearchParameter-healthcareservice-name.json\n",
      "INFO:root:Split SearchParameter-healthcareservice-name.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-healthcareservice-name.json\n",
      "INFO:root:Extracted 4 requirements from chunk\n",
      "INFO:root:Processing file 115/166: ValueSet-LanguageProficiencyVS.json\n",
      "INFO:root:Split ValueSet-LanguageProficiencyVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-LanguageProficiencyVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 116/166: SearchParameter-organization-endpoint.json\n",
      "INFO:root:Split SearchParameter-organization-endpoint.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organization-endpoint.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 117/166: Organization-Acme.json\n",
      "INFO:root:Split Organization-Acme.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-Acme.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 118/166: SearchParameter-practitionerrole-location.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-location.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-location.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 119/166: Organization-ConnHIE.json\n",
      "INFO:root:Split Organization-ConnHIE.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-ConnHIE.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 120/166: SearchParameter-organization-coverage-area.json\n",
      "INFO:root:Split SearchParameter-organization-coverage-area.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organization-coverage-area.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 121/166: package.manifest.json\n",
      "INFO:root:Split package.manifest.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of package.manifest.json\n",
      "INFO:root:Extracted 3 requirements from chunk\n",
      "INFO:root:Processing file 122/166: CodeSystem-VirtualModalitiesCS.json\n",
      "INFO:root:Split CodeSystem-VirtualModalitiesCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-VirtualModalitiesCS.json\n",
      "INFO:root:Extracted 1 requirements from chunk\n",
      "INFO:root:Processing file 123/166: OrganizationAffiliation-PharmChainAffil2.json\n",
      "INFO:root:Split OrganizationAffiliation-PharmChainAffil2.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of OrganizationAffiliation-PharmChainAffil2.json\n",
      "INFO:root:Extracted 7 requirements from chunk\n",
      "INFO:root:Processing file 124/166: CodeSystem-EndpointPayloadTypeCS.json\n",
      "INFO:root:Split CodeSystem-EndpointPayloadTypeCS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of CodeSystem-EndpointPayloadTypeCS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 125/166: SearchParameter-insuranceplan-type.json\n",
      "INFO:root:Split SearchParameter-insuranceplan-type.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-insuranceplan-type.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 126/166: Practitioner-HansSolo.json\n",
      "INFO:root:Split Practitioner-HansSolo.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Practitioner-HansSolo.json\n",
      "INFO:root:Extracted 9 requirements from chunk\n",
      "INFO:root:Processing file 127/166: SearchParameter-location-address.json\n",
      "INFO:root:Split SearchParameter-location-address.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-location-address.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 128/166: StructureDefinition-plannet-Organization.json\n",
      "INFO:root:Split StructureDefinition-plannet-Organization.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-Organization.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 129/166: SearchParameter-organizationaffiliation-location.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-location.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-location.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 130/166: Practitioner-JoeSmith.json\n",
      "INFO:root:Split Practitioner-JoeSmith.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Practitioner-JoeSmith.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 131/166: ValueSet-QualificationStatusVS.json\n",
      "INFO:root:Split ValueSet-QualificationStatusVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-QualificationStatusVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 132/166: Organization-BurrClinic.json\n",
      "INFO:root:Split Organization-BurrClinic.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-BurrClinic.json\n",
      "INFO:root:Extracted 5 requirements from chunk\n",
      "INFO:root:Processing file 133/166: Organization-CancerClinic.json\n",
      "INFO:root:Split Organization-CancerClinic.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-CancerClinic.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 134/166: StructureDefinition-via-intermediary.json\n",
      "INFO:root:Split StructureDefinition-via-intermediary.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-via-intermediary.json\n",
      "INFO:root:Extracted 11 requirements from chunk\n",
      "INFO:root:Processing file 135/166: SearchParameter-practitionerrole-practitioner.json\n",
      "INFO:root:Split SearchParameter-practitionerrole-practitioner.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-practitionerrole-practitioner.json\n",
      "INFO:root:Extracted 9 requirements from chunk\n",
      "INFO:root:Processing file 136/166: StructureDefinition-plannet-PractitionerRole.json\n",
      "INFO:root:Split StructureDefinition-plannet-PractitionerRole.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-PractitionerRole.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 137/166: Location-PharmLoc3.json\n",
      "INFO:root:Split Location-PharmLoc3.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-PharmLoc3.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 138/166: StructureDefinition-practitioner-qualification.json\n",
      "INFO:root:Split StructureDefinition-practitioner-qualification.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-practitioner-qualification.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 139/166: SearchParameter-location-partof.json\n",
      "INFO:root:Split SearchParameter-location-partof.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-location-partof.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 140/166: SearchParameter-healthcareservice-service-category.json\n",
      "INFO:root:Split SearchParameter-healthcareservice-service-category.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-healthcareservice-service-category.json\n",
      "INFO:root:Extracted 4 requirements from chunk\n",
      "INFO:root:Processing file 141/166: HealthcareService-HospERService.json\n",
      "INFO:root:Split HealthcareService-HospERService.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-HospERService.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 142/166: InsurancePlan-AcmeQHPGold.json\n",
      "INFO:root:Split InsurancePlan-AcmeQHPGold.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of InsurancePlan-AcmeQHPGold.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 143/166: SearchParameter-organizationaffiliation-endpoint.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-endpoint.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-endpoint.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 144/166: PractitionerRole-JoeSmithRole2.json\n",
      "INFO:root:Split PractitionerRole-JoeSmithRole2.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of PractitionerRole-JoeSmithRole2.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 145/166: ValueSet-DeliveryMethodVS.json\n",
      "INFO:root:Split ValueSet-DeliveryMethodVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-DeliveryMethodVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 146/166: PractitionerRole-JoeSmithRole3.json\n",
      "INFO:root:Split PractitionerRole-JoeSmithRole3.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of PractitionerRole-JoeSmithRole3.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 147/166: SearchParameter-location-endpoint.json\n",
      "INFO:root:Split SearchParameter-location-endpoint.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-location-endpoint.json\n",
      "INFO:root:Extracted 5 requirements from chunk\n",
      "INFO:root:Processing file 148/166: ValueSet-NonIndividualSpecialtiesVS.json\n",
      "INFO:root:Split ValueSet-NonIndividualSpecialtiesVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-NonIndividualSpecialtiesVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 149/166: StructureDefinition-newpatients.json\n",
      "INFO:root:Split StructureDefinition-newpatients.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-newpatients.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 150/166: Location-CancerClinicLoc.json\n",
      "INFO:root:Split Location-CancerClinicLoc.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-CancerClinicLoc.json\n",
      "INFO:root:Extracted 4 requirements from chunk\n",
      "INFO:root:Processing file 151/166: SearchParameter-organization-name.json\n",
      "INFO:root:Split SearchParameter-organization-name.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organization-name.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 152/166: HealthcareService-VirtualCounselService.json\n",
      "INFO:root:Split HealthcareService-VirtualCounselService.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-VirtualCounselService.json\n",
      "INFO:root:Extracted 14 requirements from chunk\n",
      "INFO:root:Processing file 153/166: Organization-HartfordOrthopedics.json\n",
      "INFO:root:Split Organization-HartfordOrthopedics.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Organization-HartfordOrthopedics.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 154/166: ValueSet-OrganizationAffiliationRoleVS.json\n",
      "INFO:root:Split ValueSet-OrganizationAffiliationRoleVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-OrganizationAffiliationRoleVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 155/166: Endpoint-AcmeOfCTPortalEndpoint.json\n",
      "INFO:root:Split Endpoint-AcmeOfCTPortalEndpoint.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Endpoint-AcmeOfCTPortalEndpoint.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 156/166: Location-PharmLoc2.json\n",
      "INFO:root:Split Location-PharmLoc2.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of Location-PharmLoc2.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 157/166: plan-net.openapi.json\n",
      "INFO:root:Split plan-net.openapi.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of plan-net.openapi.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 158/166: SearchParameter-organizationaffiliation-network.json\n",
      "INFO:root:Split SearchParameter-organizationaffiliation-network.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-organizationaffiliation-network.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 159/166: HealthcareService-PharmChainRetailService.json\n",
      "INFO:root:Split HealthcareService-PharmChainRetailService.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of HealthcareService-PharmChainRetailService.json\n",
      "INFO:root:Extracted 13 requirements from chunk\n",
      "INFO:root:Processing file 160/166: ValueSet-NetworkTypeVS.json\n",
      "INFO:root:Split ValueSet-NetworkTypeVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-NetworkTypeVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 161/166: StructureDefinition-plannet-HealthcareService.json\n",
      "INFO:root:Split StructureDefinition-plannet-HealthcareService.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-HealthcareService.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 162/166: SearchParameter-insuranceplan-owned-by.json\n",
      "INFO:root:Split SearchParameter-insuranceplan-owned-by.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-insuranceplan-owned-by.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 163/166: OrganizationAffiliation-PharmChainAffil3.json\n",
      "INFO:root:Split OrganizationAffiliation-PharmChainAffil3.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of OrganizationAffiliation-PharmChainAffil3.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 164/166: SearchParameter-location-address-city.json\n",
      "INFO:root:Split SearchParameter-location-address-city.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of SearchParameter-location-address-city.json\n",
      "INFO:root:Extracted 6 requirements from chunk\n",
      "INFO:root:Processing file 165/166: ValueSet-IndividualSpecialtyAndDegreeLicenseCertificateVS.json\n",
      "INFO:root:Split ValueSet-IndividualSpecialtyAndDegreeLicenseCertificateVS.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of ValueSet-IndividualSpecialtyAndDegreeLicenseCertificateVS.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Processing file 166/166: StructureDefinition-plannet-OrganizationAffiliation.json\n",
      "INFO:root:Split StructureDefinition-plannet-OrganizationAffiliation.json into 1 chunks\n",
      "INFO:root:Processing chunk 1/1 of StructureDefinition-plannet-OrganizationAffiliation.json\n",
      "INFO:root:Extracted 0 requirements from chunk\n",
      "INFO:root:Saved gemini results to processed_output/test_requirements_gemini.json\n"
     ]
    }
   ],
   "source": [
    "# Define input and output directories\n",
    "base_directory = 'full-ig'\n",
    "output_directory = 'processed_output'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process with each API\n",
    "apis = [\"gemini\"]\n",
    "results = {}\n",
    "\n",
    "for api_type in apis:\n",
    "    logging.info(f\"Processing with {api_type}...\")\n",
    "    results[api_type] = process_all_content(api_type, base_directory)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(output_directory, f\"test_requirements_{api_type}.json\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results[api_type], f, indent=2)\n",
    "    logging.info(f\"Saved {api_type} results to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
