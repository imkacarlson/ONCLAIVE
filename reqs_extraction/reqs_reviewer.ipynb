{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Requirements Refinement Tool\n",
    "\n",
    "This tool processes a raw list of FHIR Implementation Guide requirements and uses an LLM to produce a refined, concise list of only the testable requirements.\n",
    "\n",
    "#### What It Does\n",
    "\n",
    "- Takes a markdown file containing FHIR requirements (generated from an IG)\n",
    "- Applies filtering to identify only testable requirements\n",
    "- Consolidates duplicate requirements and merges related ones\n",
    "- Formats each requirement with consistent structure\n",
    "- Outputs a clean, testable requirements list\n",
    "\n",
    "#### How to Use\n",
    "\n",
    "1. Individual cert setup may need to be modified in `setup_clients()` function\n",
    "2. Run interactive mode in notebook: `result = run_refinement()` \n",
    "   - Or process directly: `result = refine_requirements(\"path/to/requirements.md\", \"claude\")`\n",
    "3. Direct notebook to filepath of requirements list of interest\n",
    "4. The refined requirements will be saved as `revised_reqs_output/{api}_reqs_list_v2_{timestamp}.md`\n",
    "\n",
    "Notes:\n",
    "- Supports Claude, Gemini, or GPT-4o\n",
    "- API keys should be in .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Import required libraries (ensure these are installed)\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 11:45:48,302 - root - INFO - Current working directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction\n",
      "2025-04-16 11:45:48,303 - root - INFO - Project root: /Users/ceadams/Documents/onclaive/onclaive\n",
      "2025-04-16 11:45:48,303 - root - INFO - Default input directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/processed_output\n",
      "2025-04-16 11:45:48,304 - root - INFO - Default output directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = Path.cwd().parent  # Parent directory (one level above cwd)\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory\n",
    "DEFAULT_INPUT_DIR = CURRENT_DIR / \"initial_reqs_output\"  # Default input directory\n",
    "DEFAULT_OUTPUT_DIR = CURRENT_DIR / \"revised_reqs_output\"  # Default output directory\n",
    "\n",
    "# Create output directory\n",
    "DEFAULT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Log the directories\n",
    "logging.info(f\"Current working directory: {CURRENT_DIR}\")\n",
    "logging.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "logging.info(f\"Default input directory: {DEFAULT_INPUT_DIR}\")\n",
    "logging.info(f\"Default output directory: {DEFAULT_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API Configuration\n",
    "API_CONFIGS = {\n",
    "    \"claude\": {\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"delay_between_requests\": 1\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-1.5-pro-001\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"delay_between_requests\": 2,\n",
    "        \"timeout\": 60\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"delay_between_requests\": 2\n",
    "    }\n",
    "}\n",
    "\n",
    "# System prompts\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"claude\": \"You are a Healthcare Standards Expert tasked with analyzing and refining FHIR Implementation Guide requirements.\",\n",
    "    \"gemini\": \"Your role is to analyze and refine FHIR Implementation Guide requirements, focusing on making them concise, testable, and conformance-oriented.\",\n",
    "    \"gpt\": \"As a Healthcare Standards Expert, analyze and refine FHIR Implementation Guide requirements to produce a concise, testable requirements list.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        verify_path = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "        http_client = httpx.Client(\n",
    "            verify=verify_path if os.path.exists(verify_path) else True,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            logger.warning(\"GEMINI_API_KEY not found\")\n",
    "            gemini_client = None\n",
    "        else:\n",
    "            gemini.configure(api_key=gemini_api_key)\n",
    "            gemini_client = gemini.GenerativeModel(\n",
    "                model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                    \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            logger.warning(\"OPENAI_API_KEY not found\")\n",
    "            openai_client = None\n",
    "        else:\n",
    "            openai_client = OpenAI(\n",
    "                api_key=openai_api_key,\n",
    "                timeout=60.0\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gpt\": openai_client,\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_requirements_refinement_prompt(requirements_list: str) -> str:\n",
    "    \"\"\"\n",
    "    Create the prompt for refining requirements list\n",
    "    \n",
    "    Args:\n",
    "        requirements_list: The original list of requirements\n",
    "        \n",
    "    Returns:\n",
    "        str: The prompt for the LLM\n",
    "    \"\"\"\n",
    "    return f\"\"\"Your task is to review this list of FHIR Implementation Guide requirements and create a refined, concise list of only the testable requirements. Follow these guidelines carefully:\n",
    "\n",
    "1. Produce a list of (maximum 50) clear, testable requirements that a conformance testing tool could verify.\n",
    "\n",
    "2. Include ONLY requirements that:\n",
    "   - Have explicit conformance language (SHALL, SHOULD, MAY, MUST, REQUIRED, etc.)\n",
    "   - Describe specific, verifiable behavior or capability\n",
    "   - Could be objectively tested through software testing or attestation\n",
    "\n",
    "3. EXCLUDE the following types of content:\n",
    "   - General introductory or conclusive/summarization comments\n",
    "   - Implementation guidance or explanatory text\n",
    "   - Examples or sample queries\n",
    "   - Duplicate requirements (consolidate similar requirements)\n",
    "   - Information about resource relationships without conformance statements\n",
    "   - General structural information about profiles or resources\n",
    "   - Requirements fragments that should be part of a single testable requirement\n",
    "\n",
    "4. For each requirement, include:\n",
    "   - A clear, concise statement of what MUST, SHOULD, MAY, SHALL, etc. be implemented\n",
    "   - The actor responsible (Server, Client, Application, etc.)\n",
    "   - The conformance level (SHALL, SHOULD, MAY, MUST, REQUIRED, etc.)\n",
    "\n",
    "5. Format each requirement consistently:\n",
    "   - Use active voice\n",
    "   - Begin with the actor (e.g., \"Server SHALL...\")\n",
    "   - Make each requirement atomic and independently testable\n",
    "   - Ensure requirements are implementation-neutral\n",
    "\n",
    "After filtering, verify that each requirement in your final list represents a discrete, testable capability or constraint that would be appropriate for conformance testing.\n",
    "\n",
    "Keep the formatting of each requirement as follows- renumber requirement IDs as you keep requirements in a list, starting with 01:\n",
    "    \n",
    "    ---\n",
    "    # REQ-XX\n",
    "    **Summary**: [summary text]\n",
    "    **Description**: \"[description text]\"\n",
    "    **Verification**: [method]\n",
    "    **Actor**: [actor]\n",
    "    **Conformance**: [SHALL/SHOULD/MAY/etc.]\n",
    "    **Conditional**: [True/False]\n",
    "    **Source**: [reference]\n",
    "    ---\n",
    "\n",
    "Do not include any other text in the response output, besides the requirements list. \n",
    "\n",
    "Here is the list of requirements to refine:\n",
    "\n",
    "{requirements_list}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=4, max=360),\n",
    "    stop=stop_after_attempt(8),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def make_api_request(client, api_type: str, content: str) -> str:\n",
    "    \"\"\"Make API request with retries\"\"\"\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    prompt = get_requirements_refinement_prompt(content)\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }],\n",
    "                system=SYSTEM_PROMPTS[api_type]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            response = client.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            elif response.candidates:\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPTS[api_type]},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_requirements(input_file: str, api_type: str = \"claude\", \n",
    "                       output_dir: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Refine requirements using the specified API\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input requirements list markdown file\n",
    "        api_type: The API to use (\"claude\", \"gemini\", or \"gpt\")\n",
    "        output_dir: Directory to save the output (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing processing results and path to refined requirements\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting requirements refinement with {api_type}\")\n",
    "    \n",
    "    # Use default output directory if none provided\n",
    "    if output_dir is None:\n",
    "        output_dir = DEFAULT_OUTPUT_DIR\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Validate input file\n",
    "    input_path = Path(input_file)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "    # Read input requirements\n",
    "    with open(input_path, 'r') as f:\n",
    "        requirements_content = f.read()\n",
    "    \n",
    "    # Initialize API clients\n",
    "    clients = setup_clients()\n",
    "    if api_type not in clients or clients[api_type] is None:\n",
    "        raise ValueError(f\"API client for {api_type} is not available\")\n",
    "    \n",
    "    client = clients[api_type]\n",
    "    \n",
    "    try:\n",
    "        # Process the requirements\n",
    "        logger.info(f\"Sending requirements to {api_type} for refinement...\")\n",
    "        refined_requirements = make_api_request(client, api_type, requirements_content)\n",
    "        \n",
    "        # Generate output filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"{api_type}_reqs_list_v2_{timestamp}.md\"\n",
    "        output_file_path = output_dir / output_filename\n",
    "        \n",
    "        # Save refined requirements\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(refined_requirements)\n",
    "        \n",
    "        logger.info(f\"Requirements refinement complete. Output saved to: {output_file_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"input_file\": str(input_path),\n",
    "            \"output_file\": str(output_file_path),\n",
    "            \"api_used\": api_type,\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error refining requirements: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_refinement():\n",
    "    \"\"\"Run the refinement process with user input\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FHIR Requirements Refinement Tool\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get input directory or use default\n",
    "    input_dir = input(f\"Enter input directory path (default '{DEFAULT_INPUT_DIR}'): \") or str(DEFAULT_INPUT_DIR)\n",
    "    input_dir_path = Path(input_dir)\n",
    "    \n",
    "    if not input_dir_path.exists():\n",
    "        print(f\"Warning: Input directory {input_dir} does not exist.\")\n",
    "        input_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    else:\n",
    "        # List all markdown files in the input directory\n",
    "        md_files = list(input_dir_path.glob(\"*.md\"))\n",
    "        \n",
    "        if md_files:\n",
    "            # Sort files by modification time (newest first)\n",
    "            md_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "            \n",
    "            # Show only the 10 most recent files\n",
    "            recent_files = md_files[:10]\n",
    "            \n",
    "            print(\"\\nMost recent files:\")\n",
    "            for idx, file in enumerate(recent_files, 1):\n",
    "                # Format the modification time as part of the display\n",
    "                mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "            \n",
    "            # Let user select from the list, see more files, or enter a custom path\n",
    "            print(\"\\nOptions:\")\n",
    "            print(\"- Select a number (1-10) to choose a file\")\n",
    "            print(\"- Enter 'all' to see all files\")\n",
    "            print(\"- Enter a full path to use a specific file\")\n",
    "            \n",
    "            selection = input(\"\\nYour selection: \")\n",
    "            \n",
    "            if selection.lower() == 'all':\n",
    "                # Show all files with pagination\n",
    "                all_files = md_files\n",
    "                page_size = 20\n",
    "                total_pages = (len(all_files) + page_size - 1) // page_size\n",
    "                \n",
    "                current_page = 1\n",
    "                while current_page <= total_pages:\n",
    "                    start_idx = (current_page - 1) * page_size\n",
    "                    end_idx = min(start_idx + page_size, len(all_files))\n",
    "                    \n",
    "                    print(f\"\\nAll files (page {current_page}/{total_pages}):\")\n",
    "                    for idx, file in enumerate(all_files[start_idx:end_idx], start_idx + 1):\n",
    "                        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                        print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "                    \n",
    "                    if current_page < total_pages:\n",
    "                        next_action = input(\"\\nPress Enter for next page, 'q' to select, or enter a number to choose a file: \")\n",
    "                        if next_action.lower() == 'q':\n",
    "                            break\n",
    "                        elif next_action.isdigit() and 1 <= int(next_action) <= len(all_files):\n",
    "                            input_file = str(all_files[int(next_action) - 1])\n",
    "                            break\n",
    "                        else:\n",
    "                            current_page += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if 'input_file' not in locals():\n",
    "                    # If we went through all pages without selection\n",
    "                    file_number = input(\"\\nEnter the file number to process: \")\n",
    "                    if file_number.isdigit() and 1 <= int(file_number) <= len(all_files):\n",
    "                        input_file = str(all_files[int(file_number) - 1])\n",
    "                    else:\n",
    "                        input_file = file_number  # Treat as a custom path\n",
    "            \n",
    "            elif selection.isdigit() and 1 <= int(selection) <= len(recent_files):\n",
    "                input_file = str(recent_files[int(selection) - 1])\n",
    "            else:\n",
    "                input_file = selection  # Treat as a custom path\n",
    "        else:\n",
    "            print(f\"No markdown files found in {input_dir}\")\n",
    "            input_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    \n",
    "    # Get output directory or use default\n",
    "    output_dir = input(f\"Enter output directory path (default '{DEFAULT_OUTPUT_DIR}'): \") or str(DEFAULT_OUTPUT_DIR)\n",
    "    output_dir_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Select the API to use\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    try:\n",
    "        # Run the refinement\n",
    "        print(f\"\\nProcessing requirements with {api_type.capitalize()}...\")\n",
    "        result = refine_requirements(input_file, api_type, output_dir_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Requirements Refinement Complete!\")\n",
    "        print(f\"Input file: {result['input_file']}\")\n",
    "        print(f\"Refined requirements saved to: {result['output_file']}\")\n",
    "        print(f\"API used: {result['api_used']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during refinement: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FHIR Requirements Refinement Tool\n",
      "================================================================================\n",
      "\n",
      "Most recent files:\n",
      "1. claude_reqs_list_v1_20250416_113606.md (2025-04-16 11:36)\n",
      "2. claude_reqs_list_v1_20250416_112958.md (2025-04-16 11:29)\n",
      "3. claude_reqs_list_v1_20250416_112422.md (2025-04-16 11:24)\n",
      "4. claude_reqs_list_v1_20250416_111547.md (2025-04-16 11:15)\n",
      "5. claude_reqs_list_v1_20250416_103702.md (2025-04-16 10:37)\n",
      "6. plan_net_gemini_requirements_list_20250402_145733.md (2025-04-02 14:57)\n",
      "7. plan_net_gemini_20250402_145733.md (2025-04-02 14:57)\n",
      "8. plan_net_claude_requirements_list_20250402_144346.md (2025-04-02 14:43)\n",
      "9. plan_net_claude_20250402_144346.md (2025-04-02 14:43)\n",
      "10. plan_net_gpt_requirements_list_20250402_135527.md (2025-04-02 13:55)\n",
      "\n",
      "Options:\n",
      "- Select a number (1-10) to choose a file\n",
      "- Enter 'all' to see all files\n",
      "- Enter a full path to use a specific file\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 11:45:59,918 - __main__ - INFO - Starting requirements refinement with claude\n",
      "2025-04-16 11:45:59,944 - __main__ - INFO - Sending requirements to claude for refinement...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing requirements with Claude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 11:46:10,242 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 11:46:10,245 - __main__ - INFO - Requirements refinement complete. Output saved to: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs/claude_reqs_list_v2_20250416_114610.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Requirements Refinement Complete!\n",
      "Input file: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/processed_output/claude_reqs_list_v1_20250416_113606.md\n",
      "Refined requirements saved to: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs/claude_reqs_list_v2_20250416_114610.md\n",
      "API used: claude\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the interactive version\n",
    "result = run_refinement()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
