{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Requirements Refinement Tool\n",
    "\n",
    "This tool processes a raw list of FHIR Implementation Guide requirements and uses an LLM to produce a refined, concise list of only the testable requirements.\n",
    "\n",
    "#### What It Does\n",
    "\n",
    "- Takes a markdown file containing FHIR requirements (generated from an IG)\n",
    "- Applies filtering to identify only testable requirements\n",
    "- Consolidates duplicate requirements and merges related ones\n",
    "- Formats each requirement with consistent structure\n",
    "- Outputs a clean, testable requirements list\n",
    "\n",
    "#### How to Use\n",
    "\n",
    "1. Run interactive mode in notebook: `run_refinement()` or `result = run_refinement()`\n",
    "2. Direct notebook to filepath of requirements list of interest\n",
    "3. The refined requirements will be saved as `revised_reqs_output/{api}_reqs_list_v2_{timestamp}.md`\n",
    "\n",
    "Notes:\n",
    "- Supports Claude, Gemini, or GPT-4o\n",
    "- API keys should be in .env file\n",
    "- API configurations are set in llm_utils.py- changes to configurations should be made there\n",
    "- Individual cert setup may need to be modified in `setup_clients()` function in the llm_utils.py file before running this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 14:25:22,005 - root - INFO - Current working directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction\n",
      "2025-07-23 14:25:22,006 - root - INFO - Project root: /Users/ceadams/Documents/onclaive/onclaive\n",
      "2025-07-23 14:25:22,006 - root - INFO - Default input directory: /Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/requirements_extraction/markdown\n",
      "2025-07-23 14:25:22,006 - root - INFO - Default output directory: /Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/revised_reqs_extraction\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = Path.cwd().parent  # Parent directory (one level above cwd)\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory\n",
    "#DEFAULT_INPUT_DIR = CURRENT_DIR / \"initial_reqs_output\"  # Default input directory\n",
    "#DEFAULT_OUTPUT_DIR = CURRENT_DIR / \"revised_reqs_output\"  # Default output directory\n",
    "\n",
    "DEFAULT_INPUT_DIR = \"/Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/requirements_extraction/markdown\"\n",
    "DEFAULT_OUTPUT_DIR = \"/Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/revised_reqs_extraction\"\n",
    "\n",
    "# Create output directory\n",
    "#DEFAULT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Log the directories\n",
    "logging.info(f\"Current working directory: {CURRENT_DIR}\")\n",
    "logging.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "logging.info(f\"Default input directory: {DEFAULT_INPUT_DIR}\")\n",
    "logging.info(f\"Default output directory: {DEFAULT_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "module_path = os.path.join(PROJECT_ROOT, 'llm_utils.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"llm_utils\", module_path)\n",
    "llm_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(llm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 14:25:22,019 - root - INFO - Prompt environment set up at: /Users/ceadams/Documents/onclaive/onclaive/prompts\n",
      "2025-07-23 14:25:22,020 - root - INFO - Using prompts directory: /Users/ceadams/Documents/onclaive/onclaive/prompts\n",
      "2025-07-23 14:25:22,020 - root - INFO - Requirements refinement prompt: /Users/ceadams/Documents/onclaive/onclaive/prompts/requirements_refinement.md\n"
     ]
    }
   ],
   "source": [
    "# Import prompt utilities\n",
    "prompt_utils_path = os.path.join(PROJECT_ROOT, 'prompt_utils.py')\n",
    "spec = importlib.util.spec_from_file_location(\"prompt_utils\", prompt_utils_path)\n",
    "prompt_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(prompt_utils)\n",
    "\n",
    "# Setup the prompt environment\n",
    "prompt_env = prompt_utils.setup_prompt_environment(PROJECT_ROOT)\n",
    "PROMPT_DIR = prompt_env[\"prompt_dir\"]\n",
    "REQUIREMENTS_REFINEMENT_PATH = prompt_env[\"requirements_refinement_path\"]\n",
    "\n",
    "logging.info(f\"Using prompts directory: {PROMPT_DIR}\")\n",
    "logging.info(f\"Requirements refinement prompt: {REQUIREMENTS_REFINEMENT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"claude\": \"You are a Healthcare Standards Expert tasked with analyzing and refining FHIR Implementation Guide requirements.\",\n",
    "    \"gemini\": \"Your role is to analyze and refine FHIR Implementation Guide requirements, focusing on making them concise, testable, and conformance-oriented.\",\n",
    "    \"gpt\": \"As a Healthcare Standards Expert, analyze and refine FHIR Implementation Guide requirements to produce a concise, testable requirements list.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requirements_refinement_prompt(requirements_list: str) -> str:\n",
    "    \"\"\"\n",
    "    Create the prompt for refining requirements list using external prompt file\n",
    "    \n",
    "    Args:\n",
    "        requirements_list: The original list of requirements\n",
    "        \n",
    "    Returns:\n",
    "        str: The prompt for the LLM loaded from external file\n",
    "    \"\"\"\n",
    "    return prompt_utils.load_prompt(\n",
    "        REQUIREMENTS_REFINEMENT_PATH,\n",
    "        requirements_list=requirements_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_tokens(text: str, api_type: str = \"claude\") -> int:\n",
    "    \"\"\"\n",
    "    Estimate token count for different models using your existing count_tokens method\n",
    "    \"\"\"\n",
    "    # Create a temporary client instance to use the count_tokens method\n",
    "    try:\n",
    "        temp_client = llm_utils.LLMApiClient()\n",
    "        return temp_client.count_tokens(text, api_type)\n",
    "    except:\n",
    "        # Fallback estimation if client creation fails\n",
    "        if api_type == \"gpt\":\n",
    "            try:\n",
    "                import tiktoken\n",
    "                encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "                return len(encoding.encode(text))\n",
    "            except:\n",
    "                return len(text) // 4\n",
    "        else:\n",
    "            # Claude and Gemini fallback\n",
    "            return len(text) // 4\n",
    "\n",
    "def get_token_limits(api_type: str) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Get input and output token limits for different APIs\n",
    "    Uses your existing API_CONFIGS\n",
    "    \"\"\"\n",
    "    # Input limits (conservative estimates for context windows)\n",
    "    input_limits = {\n",
    "        \"claude\": 180000,    # Claude 3.5 Sonnet has 200K, leave buffer\n",
    "        \"gemini\": 900000,    # Gemini 2.5 Pro has 1M, leave buffer  \n",
    "        \"gpt\": 120000        # GPT-4o has 128K, leave buffer\n",
    "    }\n",
    "    \n",
    "    # Output limits from your config\n",
    "    output_limits = {\n",
    "        \"claude\": llm_utils.API_CONFIGS[\"claude\"][\"max_tokens\"],\n",
    "        \"gemini\": llm_utils.API_CONFIGS[\"gemini\"][\"max_tokens\"], \n",
    "        \"gpt\": llm_utils.API_CONFIGS[\"gpt\"][\"max_tokens\"]\n",
    "    }\n",
    "    \n",
    "    return input_limits.get(api_type, 100000), output_limits.get(api_type, 4000)\n",
    "\n",
    "def split_requirements_by_tokens(requirements_text: str, max_input_tokens: int, \n",
    "                                prompt_template: str, api_type: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split requirements into chunks that fit within token limits\n",
    "    \"\"\"\n",
    "    # For prompt overhead calculation, use your existing prompt loading function\n",
    "    sample_prompt = prompt_utils.load_prompt(REQUIREMENTS_REFINEMENT_PATH, requirements_list=\"PLACEHOLDER\")\n",
    "    prompt_overhead = estimate_tokens(sample_prompt, api_type) - estimate_tokens(\"PLACEHOLDER\", api_type)\n",
    "    \n",
    "    # Available tokens for requirements (with safety buffer)\n",
    "    available_tokens = max_input_tokens - prompt_overhead - 2000  # Larger safety buffer\n",
    "    \n",
    "    logger.info(f\"Prompt overhead: {prompt_overhead} tokens\")\n",
    "    logger.info(f\"Available tokens for requirements: {available_tokens}\")\n",
    "    \n",
    "    # Split by individual requirements first\n",
    "    req_pattern = r'(?=^---\\s*\\n#\\s*REQ-\\d+)'\n",
    "    requirements = re.split(req_pattern, requirements_text, flags=re.MULTILINE)\n",
    "    requirements = [req.strip() for req in requirements if req.strip()]\n",
    "    \n",
    "    logger.info(f\"Found {len(requirements)} individual requirements to process\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for i, req in enumerate(requirements):\n",
    "        req_tokens = estimate_tokens(req, api_type)\n",
    "        \n",
    "        # If single requirement exceeds limit, it needs special handling\n",
    "        if req_tokens > available_tokens:\n",
    "            logger.warning(f\"Requirement {i+1} exceeds token limit ({req_tokens} > {available_tokens})\")\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "                current_tokens = 0\n",
    "            \n",
    "            # Try to split the large requirement (this is tricky and may not work perfectly)\n",
    "            chunks.append(req)  # Add it anyway, let API handle it\n",
    "            continue\n",
    "        \n",
    "        # Check if adding this requirement would exceed limit\n",
    "        if current_tokens + req_tokens > available_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = req\n",
    "            current_tokens = req_tokens\n",
    "        else:\n",
    "            current_chunk += \"\\n\\n\" + req if current_chunk else req\n",
    "            current_tokens += req_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    logger.info(f\"Split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "def merge_and_renumber_requirements(results: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Merge multiple result chunks and renumber requirements sequentially\n",
    "    \"\"\"\n",
    "    all_requirements = []\n",
    "    \n",
    "    for result in results:\n",
    "        # Skip safety filter blocked content\n",
    "        if \"[CONTENT BLOCKED BY SAFETY FILTER - SKIPPED]\" in result:\n",
    "            logger.warning(\"Skipping chunk blocked by safety filter\")\n",
    "            continue\n",
    "            \n",
    "        # Extract individual requirements from each result\n",
    "        req_pattern = r'---\\s*\\n(#\\s*REQ-\\d+.*?)(?=---|\\Z)'\n",
    "        matches = re.findall(req_pattern, result, re.DOTALL)\n",
    "        all_requirements.extend(matches)\n",
    "    \n",
    "    # Renumber requirements\n",
    "    final_output = \"\"\n",
    "    for i, req in enumerate(all_requirements, 1):\n",
    "        # Replace the REQ-XX number\n",
    "        req_renumbered = re.sub(r'#\\s*REQ-\\d+', f'# REQ-{i:02d}', req)\n",
    "        final_output += f\"---\\n{req_renumbered}\\n\"\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "\n",
    "def validate_output_completeness(input_count: int, output_count: int, \n",
    "                               estimated_output_tokens: int, max_output_tokens: int):\n",
    "    \"\"\"\n",
    "    Validate that output wasn't truncated\n",
    "    \"\"\"\n",
    "    warnings = []\n",
    "    \n",
    "    # Check if output might be truncated\n",
    "    if estimated_output_tokens > max_output_tokens * 0.95:\n",
    "        warnings.append(f\"Output may be truncated. Estimated tokens: {estimated_output_tokens}, Limit: {max_output_tokens}\")\n",
    "    \n",
    "    # Check if significant requirements were lost (more than 70% reduction might indicate issues)\n",
    "    if output_count < input_count * 0.3:\n",
    "        warnings.append(f\"Large reduction in requirements count (from {input_count} to {output_count}). Verify this is expected.\")\n",
    "    \n",
    "    return warnings\n",
    "\n",
    "def make_api_request_with_limits(client_instance, api_type: str, content: str) -> str:\n",
    "    \"\"\"Enhanced API request with token limit checking using your LLMApiClient\"\"\"\n",
    "    \n",
    "    # Get token limits for this API\n",
    "    max_input_tokens, max_output_tokens = get_token_limits(api_type)\n",
    "    \n",
    "    # Get the full prompt using your existing function\n",
    "    full_prompt = get_requirements_refinement_prompt(content)\n",
    "    estimated_tokens = estimate_tokens(full_prompt, api_type)\n",
    "    \n",
    "    logger.info(f\"Estimated input tokens: {estimated_tokens}\")\n",
    "    logger.info(f\"API input limit: {max_input_tokens}\")\n",
    "    logger.info(f\"API output limit: {max_output_tokens}\")\n",
    "    \n",
    "    if estimated_tokens > max_input_tokens * 0.9:  # 90% threshold\n",
    "        logger.warning(\"Input may exceed token limits. Splitting into chunks...\")\n",
    "        \n",
    "        # Load the prompt template from your external file instead of hardcoding\n",
    "        with open(REQUIREMENTS_REFINEMENT_PATH, 'r') as f:\n",
    "            prompt_template = f.read()\n",
    "        \n",
    "        # Split requirements into manageable chunks\n",
    "        chunks = split_requirements_by_tokens(content, max_input_tokens, prompt_template, api_type)\n",
    "        logger.info(f\"Split into {len(chunks)} chunks\")\n",
    "        \n",
    "        all_results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            logger.info(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "            \n",
    "            # Use your existing prompt loading function for each chunk\n",
    "            chunk_prompt = get_requirements_refinement_prompt(chunk)\n",
    "            \n",
    "            # Use your existing LLMApiClient method\n",
    "            try:\n",
    "                response = client_instance.make_llm_request(\n",
    "                    api_type=api_type,\n",
    "                    prompt=chunk_prompt,\n",
    "                    sys_prompt=SYSTEM_PROMPTS[api_type],\n",
    "                    reformat=False  # We're already providing the formatted prompt\n",
    "                )\n",
    "                all_results.append(response)\n",
    "                \n",
    "                # Add small delay between chunks\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing chunk {i+1}: {str(e)}\")\n",
    "                all_results.append(f\"[ERROR PROCESSING CHUNK {i+1}: {str(e)}]\")\n",
    "        \n",
    "        # Merge results and renumber requirements\n",
    "        return merge_and_renumber_requirements(all_results)\n",
    "    \n",
    "    else:\n",
    "        # Single API call - use your existing method\n",
    "        return client_instance.make_llm_request(\n",
    "            api_type=api_type,\n",
    "            prompt=full_prompt,\n",
    "            sys_prompt=SYSTEM_PROMPTS[api_type],\n",
    "            reformat=False  # We're already providing the formatted prompt\n",
    "        )\n",
    "\n",
    "# Updated make_api_request function \n",
    "def make_api_request(client_instance, api_type: str, content: str) -> str:\n",
    "    \"\"\"Make API request with token limit checking using your LLMApiClient\"\"\"\n",
    "    return make_api_request_with_limits(client_instance, api_type, content)\n",
    "\n",
    "\n",
    "\n",
    "# Updated refine_requirements function\n",
    "def refine_requirements(input_file: str, api_type: str = \"claude\", \n",
    "                       output_dir: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Refine requirements using the specified API with token limit handling\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting requirements refinement with {api_type}\")\n",
    "    \n",
    "    # Use default output directory if none provided\n",
    "    if output_dir is None:\n",
    "        output_dir = DEFAULT_OUTPUT_DIR\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Validate input file\n",
    "    input_path = Path(input_file)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "    # Read input requirements\n",
    "    with open(input_path, 'r') as f:\n",
    "        requirements_content = f.read()\n",
    "    \n",
    "    # Count original requirements\n",
    "    original_req_count = count_requirements_in_markdown(requirements_content)\n",
    "    logger.info(f\"Original requirements count: {original_req_count}\")\n",
    "    \n",
    "    # Check input size\n",
    "    input_tokens = estimate_tokens(requirements_content, api_type)\n",
    "    logger.info(f\"Input size: {len(requirements_content)} characters, ~{input_tokens} tokens\")\n",
    "    \n",
    "    # Initialize your LLMApiClient\n",
    "    try:\n",
    "        client_instance = llm_utils.LLMApiClient()\n",
    "        if api_type not in client_instance.clients or client_instance.clients[api_type] is None:\n",
    "            raise ValueError(f\"API client for {api_type} is not available\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing LLMApiClient: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        # Process the requirements with enhanced token handling\n",
    "        logger.info(f\"Sending requirements to {api_type} for refinement...\")\n",
    "        refined_requirements = make_api_request(client_instance, api_type, requirements_content)\n",
    "        \n",
    "        # Validate output\n",
    "        refined_req_count = count_requirements_in_markdown(refined_requirements)\n",
    "        output_tokens = estimate_tokens(refined_requirements, api_type)\n",
    "        max_input_tokens, max_output_tokens = get_token_limits(api_type)\n",
    "        \n",
    "        # Check for potential issues\n",
    "        warnings = validate_output_completeness(\n",
    "            original_req_count, refined_req_count, output_tokens, max_output_tokens\n",
    "        )\n",
    "        \n",
    "        if warnings:\n",
    "            for warning in warnings:\n",
    "                logger.warning(warning)\n",
    "                print(f\"WARNING: {warning}\")\n",
    "        \n",
    "        # Generate output filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"{api_type}_reqs_list_v2_{timestamp}.md\"\n",
    "        output_file_path = output_dir / output_filename\n",
    "        \n",
    "        # Save refined requirements\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(refined_requirements)\n",
    "        \n",
    "        logger.info(f\"Requirements refinement complete. Output saved to: {output_file_path}\")\n",
    "        logger.info(f\"Refined {original_req_count} -> {refined_req_count} requirements\")\n",
    "        logger.info(f\"Output size: {len(refined_requirements)} characters, ~{output_tokens} tokens\")\n",
    "        \n",
    "        return {\n",
    "            \"input_file\": str(input_path),\n",
    "            \"output_file\": str(output_file_path),\n",
    "            \"api_used\": api_type,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"original_requirements_count\": original_req_count,\n",
    "            \"requirements_count\": refined_req_count,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"warnings\": warnings\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error refining requirements: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_api_request(client, api_type: str, content: str) -> str:\n",
    "#     \"\"\"Make API request with retries\"\"\"\n",
    "\n",
    "#     prompt = get_requirements_refinement_prompt(content)\n",
    "    \n",
    "#     # Create a rate limiter for this request\n",
    "#     rate_limiter = llm_utils.create_rate_limiter()\n",
    "#     rate_limit_func = llm_utils.create_rate_limit_function(rate_limiter, api_type)\n",
    "    \n",
    "#     return llm_utils.make_llm_request(\n",
    "#         client=client,\n",
    "#         api_type=api_type,\n",
    "#         prompt=prompt,\n",
    "#         system_prompt=SYSTEM_PROMPTS[api_type],\n",
    "#         rate_limit_func=rate_limit_func\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def refine_requirements(input_file: str, api_type: str = \"claude\", \n",
    "#                        output_dir: str = None) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Refine requirements using the specified API\n",
    "    \n",
    "#     Args:\n",
    "#         input_file: Path to the input requirements list markdown file\n",
    "#         api_type: The API to use (\"claude\", \"gemini\", or \"gpt\")\n",
    "#         output_dir: Directory to save the output (optional)\n",
    "        \n",
    "#     Returns:\n",
    "#         Dict containing processing results and path to refined requirements\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Starting requirements refinement with {api_type}\")\n",
    "    \n",
    "#     # Use default output directory if none provided\n",
    "#     if output_dir is None:\n",
    "#         output_dir = DEFAULT_OUTPUT_DIR\n",
    "#     else:\n",
    "#         output_dir = Path(output_dir)\n",
    "#         output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     # Validate input file\n",
    "#     input_path = Path(input_file)\n",
    "#     if not input_path.exists():\n",
    "#         raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "#     # Read input requirements\n",
    "#     with open(input_path, 'r') as f:\n",
    "#         requirements_content = f.read()\n",
    "    \n",
    "#     # Initialize API clients\n",
    "#     clients = llm_utils.setup_clients()\n",
    "#     if api_type not in clients or clients[api_type] is None:\n",
    "#         raise ValueError(f\"API client for {api_type} is not available\")\n",
    "    \n",
    "#     client = clients[api_type]\n",
    "    \n",
    "#     try:\n",
    "#         # Process the requirements\n",
    "#         logger.info(f\"Sending requirements to {api_type} for refinement...\")\n",
    "#         refined_requirements = make_api_request(client, api_type, requirements_content)\n",
    "        \n",
    "#         # Generate output filename\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         output_filename = f\"{api_type}_reqs_list_v2_{timestamp}.md\"\n",
    "#         output_file_path = output_dir / output_filename\n",
    "        \n",
    "#         # Save refined requirements\n",
    "#         with open(output_file_path, 'w') as f:\n",
    "#             f.write(refined_requirements)\n",
    "        \n",
    "#         # Count refined requirements\n",
    "#         refined_req_count = count_requirements_in_markdown(refined_requirements)\n",
    "        \n",
    "#         logger.info(f\"Requirements refinement complete. Output saved to: {output_file_path}\")\n",
    "#         logger.info(f\"Identified {refined_req_count} requirements\")\n",
    "        \n",
    "#         return {\n",
    "#             \"input_file\": str(input_path),\n",
    "#             \"output_file\": str(output_file_path),\n",
    "#             \"api_used\": api_type,\n",
    "#             \"timestamp\": timestamp,\n",
    "#             \"requirements_count\": refined_req_count\n",
    "#         }\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error refining requirements: {str(e)}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_requirements_in_markdown(markdown_text):\n",
    "    \"\"\"\n",
    "    Count the number of requirements in a markdown file that follow the REQ-XX format.\n",
    "    \n",
    "    Handles both formats:\n",
    "    # REQ-01\n",
    "    or\n",
    "    ## REQ-01\n",
    "    \n",
    "    Example of the expected formats:\n",
    "    # REQ-01\n",
    "    **Summary**: Some requirement summary\n",
    "    \n",
    "    ## REQ-02\n",
    "    **Summary**: Another requirement summary\n",
    "    \"\"\"\n",
    "    # Pattern for both formats: either # REQ-XX or ## REQ-XX\n",
    "    req_pattern = r\"^\\s*(#|##)\\s+REQ-\\d+\"\n",
    "    \n",
    "    # Count the occurrences\n",
    "    lines = markdown_text.split('\\n')\n",
    "    count = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if re.match(req_pattern, line):\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_refinement():\n",
    "    \"\"\"Run the refinement process with user input\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FHIR Requirements Refinement Tool\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Start timing the entire function execution\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get input directory or use default\n",
    "    input_dir = input(f\"Enter input directory path or accept default (default '{DEFAULT_INPUT_DIR}'): \") or str(DEFAULT_INPUT_DIR)\n",
    "    input_dir_path = Path(input_dir)\n",
    "    \n",
    "    if not input_dir_path.exists():\n",
    "        print(f\"Warning: Input directory {input_dir} does not exist.\")\n",
    "        input_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    else:\n",
    "        # List all markdown files in the input directory\n",
    "        md_files = list(input_dir_path.glob(\"*.md\"))\n",
    "        \n",
    "        if md_files:\n",
    "            # Sort files by modification time (newest first)\n",
    "            md_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "            \n",
    "            # Show only the 10 most recent files\n",
    "            recent_files = md_files[:10]\n",
    "            \n",
    "            print(\"\\nMost recent files:\")\n",
    "            for idx, file in enumerate(recent_files, 1):\n",
    "                # Format the modification time as part of the display\n",
    "                mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "            \n",
    "            # Let user select from the list, see more files, or enter a custom path\n",
    "            print(\"\\nOptions:\")\n",
    "            print(\"- Select a number (1-10) to choose one of the following most recently generated files\")\n",
    "            print(\"- Enter 'all' to see all files\")\n",
    "            print(\"- Enter a full path to use a specific file\")\n",
    "            \n",
    "            selection = input(\"\\nReview the printed options for choosing a requirements file and enter applicable selection: \")\n",
    "            \n",
    "            if selection.lower() == 'all':\n",
    "                # Show all files with pagination\n",
    "                all_files = md_files\n",
    "                page_size = 20\n",
    "                total_pages = (len(all_files) + page_size - 1) // page_size\n",
    "                \n",
    "                current_page = 1\n",
    "                while current_page <= total_pages:\n",
    "                    start_idx = (current_page - 1) * page_size\n",
    "                    end_idx = min(start_idx + page_size, len(all_files))\n",
    "                    \n",
    "                    print(f\"\\nAll files (page {current_page}/{total_pages}):\")\n",
    "                    for idx, file in enumerate(all_files[start_idx:end_idx], start_idx + 1):\n",
    "                        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                        print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "                    \n",
    "                    if current_page < total_pages:\n",
    "                        next_action = input(\"\\nPress Enter for next page, 'q' to select, or enter a number to choose a file: \")\n",
    "                        if next_action.lower() == 'q':\n",
    "                            break\n",
    "                        elif next_action.isdigit() and 1 <= int(next_action) <= len(all_files):\n",
    "                            input_file = str(all_files[int(next_action) - 1])\n",
    "                            break\n",
    "                        else:\n",
    "                            current_page += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if 'input_file' not in locals():\n",
    "                    # If we went through all pages without selection\n",
    "                    file_number = input(\"\\nEnter the file number to process: \")\n",
    "                    if file_number.isdigit() and 1 <= int(file_number) <= len(all_files):\n",
    "                        input_file = str(all_files[int(file_number) - 1])\n",
    "                    else:\n",
    "                        input_file = file_number  # Treat as a custom path\n",
    "            \n",
    "            elif selection.isdigit() and 1 <= int(selection) <= len(recent_files):\n",
    "                input_file = str(recent_files[int(selection) - 1])\n",
    "            else:\n",
    "                input_file = selection  # Treat as a custom path\n",
    "        else:\n",
    "            print(f\"No markdown files found in {input_dir}\")\n",
    "            input_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    \n",
    "    # Get output directory or use default\n",
    "    output_dir = input(f\"Enter output directory path or accept default (default '{DEFAULT_OUTPUT_DIR}'): \") or str(DEFAULT_OUTPUT_DIR)\n",
    "    output_dir_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Select the API to use\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice of API to use, based on the printed listing (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    try:\n",
    "        # Run the refinement\n",
    "        print(f\"\\nProcessing requirements with {api_type.capitalize()}...\")\n",
    "        result = refine_requirements(input_file, api_type, output_dir_path)\n",
    "        \n",
    "        # Calculate total execution time\n",
    "        total_elapsed_time = time.time() - start_time\n",
    "        total_elapsed_formatted = str(timedelta(seconds=int(total_elapsed_time)))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Requirements Refinement Complete!\")\n",
    "        print(f\"Input file: {result['input_file']}\")\n",
    "        print(f\"Refined requirements saved to: {result['output_file']}\")\n",
    "        print(f\"API used: {result['api_used']}\")\n",
    "        print(f\"Number of requirements identified: {result['requirements_count']}\")\n",
    "        print(f\"Total execution time: {total_elapsed_formatted}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during refinement: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FHIR Requirements Refinement Tool\n",
      "================================================================================\n",
      "\n",
      "Most recent files:\n",
      "1. claude_reqs_list_v1_20250723_134708.md (2025-07-23 13:47)\n",
      "2. claude_reqs_list_v1_20250723_132400.md (2025-07-23 13:24)\n",
      "3. claude_reqs_list_v1_20250722_083504.md (2025-07-22 08:35)\n",
      "4. gemini_reqs_list_v1_20250722_082855.md (2025-07-22 08:28)\n",
      "5. gpt_reqs_list_v1_20250722_081950.md (2025-07-22 08:19)\n",
      "6. claude_reqs_list_v1_20250620_115908.md (2025-06-20 11:59)\n",
      "\n",
      "Options:\n",
      "- Select a number (1-10) to choose one of the following most recently generated files\n",
      "- Enter 'all' to see all files\n",
      "- Enter a full path to use a specific file\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 14:25:30,711 - __main__ - INFO - Starting requirements refinement with claude\n",
      "2025-07-23 14:25:30,736 - __main__ - INFO - Sending requirements to claude for refinement...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing requirements with Claude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 14:26:31,259 - anthropic._base_client - INFO - Retrying request to /v1/messages in 0.435537 seconds\n",
      "2025-07-23 14:27:01,112 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-07-23 14:27:01,117 - __main__ - INFO - Requirements refinement complete. Output saved to: /Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/revised_reqs_extraction/claude_reqs_list_v2_20250723_142701.md\n",
      "2025-07-23 14:27:01,118 - __main__ - INFO - Identified 15 requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Requirements Refinement Complete!\n",
      "Input file: /Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/requirements_extraction/markdown/claude_reqs_list_v1_20250723_134708.md\n",
      "Refined requirements saved to: /Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/revised_reqs_extraction/claude_reqs_list_v2_20250723_142701.md\n",
      "API used: claude\n",
      "Number of requirements identified: 15\n",
      "Total execution time: 0:01:39\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_file': '/Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/requirements_extraction/markdown/claude_reqs_list_v1_20250723_134708.md',\n",
       " 'output_file': '/Users/ceadams/Documents/onclaive/onclaive/pipeline/checkpoints/revised_reqs_extraction/claude_reqs_list_v2_20250723_142701.md',\n",
       " 'api_used': 'claude',\n",
       " 'timestamp': '20250723_142701',\n",
       " 'requirements_count': 15}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the interactive version\n",
    "run_refinement()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
