{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Requirements Refinement Tool\n",
    "\n",
    "This tool processes a raw list of FHIR Implementation Guide requirements and uses an LLM to produce a refined, concise list of only the testable requirements.\n",
    "\n",
    "#### What It Does\n",
    "\n",
    "- Takes a markdown file containing FHIR requirements (generated from an IG)\n",
    "- Applies filtering to identify only testable requirements\n",
    "- Consolidates duplicate requirements and merges related ones\n",
    "- Formats each requirement with consistent structure\n",
    "- Outputs a clean, testable requirements list\n",
    "\n",
    "#### How to Use\n",
    "\n",
    "1. Run interactive mode in notebook: `result = run_refinement()` \n",
    "   - Or process directly: `result = refine_requirements(\"path/to/requirements.md\", \"claude\")`\n",
    "2. Direct notebook to filepath of requirements list of interest\n",
    "3. The refined requirements will be saved as `revised_reqs_output/{api}_reqs_list_v2_{timestamp}.md`\n",
    "\n",
    "Notes:\n",
    "- Supports Claude, Gemini, or GPT-4o\n",
    "- API keys should be in .env file\n",
    "- API configurations are set in llm_utils.py- changes to configurations should be made there\n",
    "- Individual cert setup may need to be modified in `setup_clients()` function in the llm_utils.py file before running this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:11:10,666 - root - INFO - Current working directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction\n",
      "2025-04-30 12:11:10,667 - root - INFO - Project root: /Users/ceadams/Documents/onclaive/onclaive\n",
      "2025-04-30 12:11:10,668 - root - INFO - Default input directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/initial_reqs_output\n",
      "2025-04-30 12:11:10,668 - root - INFO - Default output directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs_output\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = Path.cwd().parent  # Parent directory (one level above cwd)\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory\n",
    "DEFAULT_INPUT_DIR = CURRENT_DIR / \"initial_reqs_output\"  # Default input directory\n",
    "DEFAULT_OUTPUT_DIR = CURRENT_DIR / \"revised_reqs_output\"  # Default output directory\n",
    "\n",
    "# Create output directory\n",
    "DEFAULT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Log the directories\n",
    "logging.info(f\"Current working directory: {CURRENT_DIR}\")\n",
    "logging.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "logging.info(f\"Default input directory: {DEFAULT_INPUT_DIR}\")\n",
    "logging.info(f\"Default output directory: {DEFAULT_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "module_path = os.path.join(PROJECT_ROOT, 'llm_utils.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"llm_utils\", module_path)\n",
    "llm_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(llm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt utilities\n",
    "prompt_utils_path = os.path.join(PROJECT_ROOT, 'prompt_utils.py')\n",
    "spec = importlib.util.spec_from_file_location(\"prompt_utils\", prompt_utils_path)\n",
    "prompt_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(prompt_utils)\n",
    "\n",
    "# Setup the prompt environment\n",
    "prompt_env = prompt_utils.setup_prompt_environment(PROJECT_ROOT)\n",
    "PROMPT_DIR = prompt_env[\"prompt_dir\"]\n",
    "REQUIREMENTS_REFINEMENT_PATH = prompt_env[\"requirements_refinement_path\"]\n",
    "\n",
    "logging.info(f\"Using prompts directory: {PROMPT_DIR}\")\n",
    "logging.info(f\"Requirements refinement prompt: {REQUIREMENTS_REFINEMENT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"claude\": \"You are a Healthcare Standards Expert tasked with analyzing and refining FHIR Implementation Guide requirements.\",\n",
    "    \"gemini\": \"Your role is to analyze and refine FHIR Implementation Guide requirements, focusing on making them concise, testable, and conformance-oriented.\",\n",
    "    \"gpt\": \"As a Healthcare Standards Expert, analyze and refine FHIR Implementation Guide requirements to produce a concise, testable requirements list.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requirements_refinement_prompt(requirements_list: str) -> str:\n",
    "    \"\"\"\n",
    "    Create the prompt for refining requirements list using external prompt file\n",
    "    \n",
    "    Args:\n",
    "        requirements_list: The original list of requirements\n",
    "        \n",
    "    Returns:\n",
    "        str: The prompt for the LLM loaded from external file\n",
    "    \"\"\"\n",
    "    return prompt_utils.load_prompt(\n",
    "        REQUIREMENTS_REFINEMENT_PATH,\n",
    "        requirements_list=requirements_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(client, api_type: str, content: str) -> str:\n",
    "    \"\"\"Make API request with retries\"\"\"\n",
    "\n",
    "    prompt = get_requirements_refinement_prompt(content)\n",
    "    \n",
    "    # Create a rate limiter for this request\n",
    "    rate_limiter = llm_utils.create_rate_limiter()\n",
    "    rate_limit_func = llm_utils.create_rate_limit_function(rate_limiter, api_type)\n",
    "    \n",
    "    return llm_utils.make_llm_request(\n",
    "        client=client,\n",
    "        api_type=api_type,\n",
    "        prompt=prompt,\n",
    "        system_prompt=SYSTEM_PROMPTS[api_type],\n",
    "        rate_limit_func=rate_limit_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_requirements(input_file: str, api_type: str = \"claude\", \n",
    "                       output_dir: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Refine requirements using the specified API\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input requirements list markdown file\n",
    "        api_type: The API to use (\"claude\", \"gemini\", or \"gpt\")\n",
    "        output_dir: Directory to save the output (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing processing results and path to refined requirements\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting requirements refinement with {api_type}\")\n",
    "    \n",
    "    # Use default output directory if none provided\n",
    "    if output_dir is None:\n",
    "        output_dir = DEFAULT_OUTPUT_DIR\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Validate input file\n",
    "    input_path = Path(input_file)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "    # Read input requirements\n",
    "    with open(input_path, 'r') as f:\n",
    "        requirements_content = f.read()\n",
    "    \n",
    "    # Initialize API clients\n",
    "    clients = llm_utils.setup_clients()\n",
    "    if api_type not in clients or clients[api_type] is None:\n",
    "        raise ValueError(f\"API client for {api_type} is not available\")\n",
    "    \n",
    "    client = clients[api_type]\n",
    "    \n",
    "    try:\n",
    "        # Process the requirements\n",
    "        logger.info(f\"Sending requirements to {api_type} for refinement...\")\n",
    "        refined_requirements = make_api_request(client, api_type, requirements_content)\n",
    "        \n",
    "        # Generate output filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"{api_type}_reqs_list_v2_{timestamp}.md\"\n",
    "        output_file_path = output_dir / output_filename\n",
    "        \n",
    "        # Save refined requirements\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(refined_requirements)\n",
    "        \n",
    "        logger.info(f\"Requirements refinement complete. Output saved to: {output_file_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"input_file\": str(input_path),\n",
    "            \"output_file\": str(output_file_path),\n",
    "            \"api_used\": api_type,\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error refining requirements: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_refinement():\n",
    "    \"\"\"Run the refinement process with user input\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FHIR Requirements Refinement Tool\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get input directory or use default\n",
    "    input_dir = input(f\"Enter input directory path (default '{DEFAULT_INPUT_DIR}'): \") or str(DEFAULT_INPUT_DIR)\n",
    "    input_dir_path = Path(input_dir)\n",
    "    \n",
    "    if not input_dir_path.exists():\n",
    "        print(f\"Warning: Input directory {input_dir} does not exist.\")\n",
    "        input_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    else:\n",
    "        # List all markdown files in the input directory\n",
    "        md_files = list(input_dir_path.glob(\"*.md\"))\n",
    "        \n",
    "        if md_files:\n",
    "            # Sort files by modification time (newest first)\n",
    "            md_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "            \n",
    "            # Show only the 10 most recent files\n",
    "            recent_files = md_files[:10]\n",
    "            \n",
    "            print(\"\\nMost recent files:\")\n",
    "            for idx, file in enumerate(recent_files, 1):\n",
    "                # Format the modification time as part of the display\n",
    "                mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "            \n",
    "            # Let user select from the list, see more files, or enter a custom path\n",
    "            print(\"\\nOptions:\")\n",
    "            print(\"- Select a number (1-10) to choose a file\")\n",
    "            print(\"- Enter 'all' to see all files\")\n",
    "            print(\"- Enter a full path to use a specific file\")\n",
    "            \n",
    "            selection = input(\"\\nYour selection: \")\n",
    "            \n",
    "            if selection.lower() == 'all':\n",
    "                # Show all files with pagination\n",
    "                all_files = md_files\n",
    "                page_size = 20\n",
    "                total_pages = (len(all_files) + page_size - 1) // page_size\n",
    "                \n",
    "                current_page = 1\n",
    "                while current_page <= total_pages:\n",
    "                    start_idx = (current_page - 1) * page_size\n",
    "                    end_idx = min(start_idx + page_size, len(all_files))\n",
    "                    \n",
    "                    print(f\"\\nAll files (page {current_page}/{total_pages}):\")\n",
    "                    for idx, file in enumerate(all_files[start_idx:end_idx], start_idx + 1):\n",
    "                        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                        print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "                    \n",
    "                    if current_page < total_pages:\n",
    "                        next_action = input(\"\\nPress Enter for next page, 'q' to select, or enter a number to choose a file: \")\n",
    "                        if next_action.lower() == 'q':\n",
    "                            break\n",
    "                        elif next_action.isdigit() and 1 <= int(next_action) <= len(all_files):\n",
    "                            input_file = str(all_files[int(next_action) - 1])\n",
    "                            break\n",
    "                        else:\n",
    "                            current_page += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if 'input_file' not in locals():\n",
    "                    # If we went through all pages without selection\n",
    "                    file_number = input(\"\\nEnter the file number to process: \")\n",
    "                    if file_number.isdigit() and 1 <= int(file_number) <= len(all_files):\n",
    "                        input_file = str(all_files[int(file_number) - 1])\n",
    "                    else:\n",
    "                        input_file = file_number  # Treat as a custom path\n",
    "            \n",
    "            elif selection.isdigit() and 1 <= int(selection) <= len(recent_files):\n",
    "                input_file = str(recent_files[int(selection) - 1])\n",
    "            else:\n",
    "                input_file = selection  # Treat as a custom path\n",
    "        else:\n",
    "            print(f\"No markdown files found in {input_dir}\")\n",
    "            input_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    \n",
    "    # Get output directory or use default\n",
    "    output_dir = input(f\"Enter output directory path (default '{DEFAULT_OUTPUT_DIR}'): \") or str(DEFAULT_OUTPUT_DIR)\n",
    "    output_dir_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Select the API to use\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    try:\n",
    "        # Run the refinement\n",
    "        print(f\"\\nProcessing requirements with {api_type.capitalize()}...\")\n",
    "        result = refine_requirements(input_file, api_type, output_dir_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Requirements Refinement Complete!\")\n",
    "        print(f\"Input file: {result['input_file']}\")\n",
    "        print(f\"Refined requirements saved to: {result['output_file']}\")\n",
    "        print(f\"API used: {result['api_used']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during refinement: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FHIR Requirements Refinement Tool\n",
      "================================================================================\n",
      "\n",
      "Most recent files:\n",
      "1. gemini_reqs_list_v1_20250423_140750.md (2025-04-23 14:07)\n",
      "2. example_claude_reqs_list_v1_20250416_141301.md (2025-04-23 10:46)\n",
      "\n",
      "Options:\n",
      "- Select a number (1-10) to choose a file\n",
      "- Enter 'all' to see all files\n",
      "- Enter a full path to use a specific file\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:11:25,133 - __main__ - INFO - Starting requirements refinement with claude\n",
      "2025-04-30 12:11:25,160 - __main__ - INFO - Sending requirements to claude for refinement...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing requirements with Claude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:11:37,035 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-04-30 12:11:37,038 - __main__ - INFO - Requirements refinement complete. Output saved to: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs_output/claude_reqs_list_v2_20250430_121137.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Requirements Refinement Complete!\n",
      "Input file: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/initial_reqs_output/example_claude_reqs_list_v1_20250416_141301.md\n",
      "Refined requirements saved to: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs_output/claude_reqs_list_v2_20250430_121137.md\n",
      "API used: claude\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the interactive version\n",
    "result = run_refinement()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
