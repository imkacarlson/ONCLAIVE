{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Requirements Refinement Tool\n",
    "\n",
    "This tool processes a raw list of FHIR Implementation Guide requirements and uses an LLM to produce a refined, concise list of only the testable requirements.\n",
    "\n",
    "#### What It Does\n",
    "\n",
    "- Takes a markdown file containing FHIR requirements (generated from an IG)\n",
    "- Applies filtering to identify only testable requirements\n",
    "- Consolidates duplicate requirements and merges related ones\n",
    "- Formats each requirement with consistent structure\n",
    "- Outputs a clean, testable requirements list (15-50 requirements)\n",
    "\n",
    "#### How to Use\n",
    "\n",
    "1. Individual cert setup may need to be modified in `setup_clients()` function\n",
    "2. Run interactive mode in notebook: `result = run_refinement()` \n",
    "   - Or process directly: `result = refine_requirements(\"path/to/requirements.md\", \"claude\")`\n",
    "3. Import the module to another notebook: `from refine_requirements import refine_requirements, run_refinement`\n",
    "4. Direct notebook to filepath of requirements list of interest\n",
    "5. The refined requirements will be saved to `revised_reqs/refined_requirements_{api}_{timestamp}.md`\n",
    "\n",
    "Notes:\n",
    "- Supports Claude, Gemini, or GPT-4\n",
    "- API keys should be in .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Import required libraries (ensure these are installed)\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define paths\n",
    "PROJECT_ROOT = Path.cwd().parent  # Parent directory (one level above cwd)\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory\n",
    "OUTPUT_DIR = CURRENT_DIR / \"revised_reqs\"\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API Configuration\n",
    "API_CONFIGS = {\n",
    "    \"claude\": {\n",
    "        \"model_name\": \"claude-3-5-sonnet-20241022\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"delay_between_requests\": 1\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"model\": \"models/gemini-1.5-pro-001\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"delay_between_requests\": 2,\n",
    "        \"timeout\": 60\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"delay_between_requests\": 2\n",
    "    }\n",
    "}\n",
    "\n",
    "# System prompts\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"claude\": \"You are a Healthcare Standards Expert tasked with analyzing and refining FHIR Implementation Guide requirements.\",\n",
    "    \"gemini\": \"Your role is to analyze and refine FHIR Implementation Guide requirements, focusing on making them concise, testable, and conformance-oriented.\",\n",
    "    \"gpt\": \"As a Healthcare Standards Expert, analyze and refine FHIR Implementation Guide requirements to produce a concise, testable requirements list.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_clients():\n",
    "    \"\"\"Initialize clients for each LLM service\"\"\"\n",
    "    try:\n",
    "        # Claude setup\n",
    "        verify_path = '/opt/homebrew/etc/openssl@3/cert.pem'\n",
    "        http_client = httpx.Client(\n",
    "            verify=verify_path if os.path.exists(verify_path) else True,\n",
    "            timeout=60.0\n",
    "        )\n",
    "        claude_client = Anthropic(\n",
    "            api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
    "            http_client=http_client\n",
    "        )\n",
    "        \n",
    "        # Gemini setup\n",
    "        gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "        if not gemini_api_key:\n",
    "            logger.warning(\"GEMINI_API_KEY not found\")\n",
    "            gemini_client = None\n",
    "        else:\n",
    "            gemini.configure(api_key=gemini_api_key)\n",
    "            gemini_client = gemini.GenerativeModel(\n",
    "                model_name=API_CONFIGS[\"gemini\"][\"model\"],\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": API_CONFIGS[\"gemini\"][\"max_tokens\"],\n",
    "                    \"temperature\": API_CONFIGS[\"gemini\"][\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # OpenAI setup\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not openai_api_key:\n",
    "            logger.warning(\"OPENAI_API_KEY not found\")\n",
    "            openai_client = None\n",
    "        else:\n",
    "            openai_client = OpenAI(\n",
    "                api_key=openai_api_key,\n",
    "                timeout=60.0\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"claude\": claude_client,\n",
    "            \"gpt\": openai_client,\n",
    "            \"gemini\": gemini_client\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up clients: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_requirements_refinement_prompt(requirements_list: str) -> str:\n",
    "    \"\"\"\n",
    "    Create the prompt for refining requirements list\n",
    "    \n",
    "    Args:\n",
    "        requirements_list: The original list of requirements\n",
    "        \n",
    "    Returns:\n",
    "        str: The prompt for the LLM\n",
    "    \"\"\"\n",
    "    return f\"\"\"Your task is to review this list of FHIR Implementation Guide requirements and create a refined, concise list of only the testable requirements. Follow these guidelines carefully:\n",
    "\n",
    "1. Produce a list of 15-50 clear, testable requirements that a conformance testing tool could verify.\n",
    "\n",
    "2. Include ONLY requirements that:\n",
    "   - Have explicit conformance language (SHALL, SHOULD, MAY, MUST, REQUIRED, etc.)\n",
    "   - Describe specific, verifiable behavior or capability\n",
    "   - Could be objectively tested through software testing or attestation\n",
    "\n",
    "3. EXCLUDE the following types of content:\n",
    "   - General introductory or conclusive/summarization comments\n",
    "   - Implementation guidance or explanatory text\n",
    "   - Examples or sample queries\n",
    "   - Duplicate requirements (consolidate similar requirements)\n",
    "   - Information about resource relationships without conformance statements\n",
    "   - General structural information about profiles or resources\n",
    "   - Requirements fragments that should be part of a single testable requirement\n",
    "\n",
    "4. For each requirement, include:\n",
    "   - A clear, concise statement of what MUST, SHOULD, MAY, SHALL, etc. be implemented\n",
    "   - The actor responsible (Server, Client, Application, etc.)\n",
    "   - The conformance level (SHALL, SHOULD, MAY, MUST, REQUIRED, etc.)\n",
    "\n",
    "5. Format each requirement consistently:\n",
    "   - Use active voice\n",
    "   - Begin with the actor (e.g., \"Server SHALL...\")\n",
    "   - Make each requirement atomic and independently testable\n",
    "   - Ensure requirements are implementation-neutral\n",
    "\n",
    "After filtering, verify that each requirement in your final list represents a discrete, testable capability or constraint that would be appropriate for conformance testing.\n",
    "\n",
    "Keep the formatting of each requirement as follows- renumber requirement IDs as you keep requirements in a list, starting with 01:\n",
    "    \n",
    "    ---\n",
    "    # REQ-XX\n",
    "    **Summary**: [summary text]\n",
    "    **Description**: \"[description text]\"\n",
    "    **Verification**: [method]\n",
    "    **Actor**: [actor]\n",
    "    **Conformance**: [SHALL/SHOULD/MAY/etc.]\n",
    "    **Conditional**: [True/False]\n",
    "    **Source**: [reference]\n",
    "    ---\n",
    "\n",
    "Do not include any other text in the response output, besides the requirements list. \n",
    "\n",
    "Here is the list of requirements to refine:\n",
    "\n",
    "{requirements_list}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=2, min=4, max=360),\n",
    "    stop=stop_after_attempt(8),\n",
    "    retry=retry_if_exception_type((RateLimitError, TimeoutError))\n",
    ")\n",
    "def make_api_request(client, api_type: str, content: str) -> str:\n",
    "    \"\"\"Make API request with retries\"\"\"\n",
    "    \n",
    "    config = API_CONFIGS[api_type]\n",
    "    prompt = get_requirements_refinement_prompt(content)\n",
    "    \n",
    "    try:\n",
    "        if api_type == \"claude\":\n",
    "            response = client.messages.create(\n",
    "                model=config[\"model_name\"],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }],\n",
    "                system=SYSTEM_PROMPTS[api_type]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "            \n",
    "        elif api_type == \"gemini\":\n",
    "            response = client.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"max_output_tokens\": config[\"max_tokens\"],\n",
    "                    \"temperature\": config[\"temperature\"]\n",
    "                }\n",
    "            )\n",
    "            if hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            elif response.candidates:\n",
    "                return response.candidates[0].content.parts[0].text\n",
    "            else:\n",
    "                raise ValueError(\"No response generated from Gemini API\")\n",
    "                    \n",
    "        elif api_type == \"gpt\":\n",
    "            response = client.chat.completions.create(\n",
    "                model=config[\"model\"],\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPTS[api_type]},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=config[\"max_tokens\"],\n",
    "                temperature=config[\"temperature\"]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in {api_type} API request: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def refine_requirements(input_file: str, api_type: str = \"claude\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Refine requirements using the specified API\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input requirements list markdown file\n",
    "        api_type: The API to use (\"claude\", \"gemini\", or \"gpt\")\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing processing results and path to refined requirements\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting requirements refinement with {api_type}\")\n",
    "    \n",
    "    # Validate input file\n",
    "    input_path = Path(input_file)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "    # Read input requirements\n",
    "    with open(input_path, 'r') as f:\n",
    "        requirements_content = f.read()\n",
    "    \n",
    "    # Initialize API clients\n",
    "    clients = setup_clients()\n",
    "    if api_type not in clients or clients[api_type] is None:\n",
    "        raise ValueError(f\"API client for {api_type} is not available\")\n",
    "    \n",
    "    client = clients[api_type]\n",
    "    \n",
    "    try:\n",
    "        # Process the requirements\n",
    "        logger.info(f\"Sending requirements to {api_type} for refinement...\")\n",
    "        refined_requirements = make_api_request(client, api_type, requirements_content)\n",
    "        \n",
    "        # Generate output filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"refined_requirements_{api_type}_{timestamp}.md\"\n",
    "        output_file_path = OUTPUT_DIR / output_filename\n",
    "        \n",
    "        # Save refined requirements\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(refined_requirements)\n",
    "        \n",
    "        logger.info(f\"Requirements refinement complete. Output saved to: {output_file_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"input_file\": str(input_path),\n",
    "            \"output_file\": str(output_file_path),\n",
    "            \"api_used\": api_type,\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error refining requirements: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import this module and call run_refinement() to start the process\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_refinement():\n",
    "    \"\"\"Run the refinement process with user input\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FHIR Requirements Refinement Tool\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get input file path\n",
    "    default_input_dir = CURRENT_DIR / \"processed_output\"\n",
    "    if default_input_dir.exists():\n",
    "        # List markdown files in the default directory\n",
    "        md_files = list(default_input_dir.glob(\"*requirements_list*.md\"))\n",
    "        if md_files:\n",
    "            print(\"\\nAvailable requirements files:\")\n",
    "            for idx, file in enumerate(md_files, 1):\n",
    "                print(f\"{idx}. {file.name}\")\n",
    "            \n",
    "            # Let user select from the list or enter a custom path\n",
    "            selection = input(\"\\nSelect a file number or enter a full path to a requirements file: \")\n",
    "            \n",
    "            if selection.isdigit() and 1 <= int(selection) <= len(md_files):\n",
    "                input_file = str(md_files[int(selection)-1])\n",
    "            else:\n",
    "                input_file = selection\n",
    "        else:\n",
    "            input_file = input(\"\\nEnter path to requirements markdown file: \")\n",
    "    else:\n",
    "        input_file = input(\"\\nEnter path to requirements markdown file: \")\n",
    "    \n",
    "    # Select the API to use\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    try:\n",
    "        # Run the refinement\n",
    "        print(f\"\\nProcessing requirements with {api_type.capitalize()}...\")\n",
    "        result = refine_requirements(input_file, api_type)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Requirements Refinement Complete!\")\n",
    "        print(f\"Input file: {result['input_file']}\")\n",
    "        print(f\"Refined requirements saved to: {result['output_file']}\")\n",
    "        print(f\"API used: {result['api_used']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during refinement: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # The script can be imported and used without automatically executing\n",
    "    print(\"Import this module and call run_refinement() to start the process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FHIR Requirements Refinement Tool\n",
      "================================================================================\n",
      "\n",
      "Available requirements files:\n",
      "1. plan_net_gemini_requirements_list_20250402_132718.md\n",
      "2. plan_net_gemini_requirements_list_20250312_141125.md\n",
      "3. plan_net_gemini_requirements_list_20250402_124101.md\n",
      "4. plan_net_claude_requirements_list_20250402_113844.md\n",
      "5. plan_net_gpt_requirements_list_20250402_135527.md\n",
      "6. plan_net_gemini_requirements_list_20250402_115052.md\n",
      "7. plan_net_claude_requirements_list_20250402_144346.md\n",
      "8. plan_net_gpt_requirements_list_20250402_130154.md\n",
      "9. plan_net_claude_requirements_list_20250312_142943.md\n",
      "10. da_vinci_pdex_plan_net_gemini_requirements_list_20250312_135622.md\n",
      "11. plan_net_claude_requirements_list_20250319_115141.md\n",
      "12. plan_net_gpt_requirements_list_20250402_123150.md\n",
      "13. plan_net_gemini_requirements_list_20250402_145733.md\n",
      "14. plan_net_gpt_requirements_list_20250319_113850.md\n",
      "15. plan_net_gemini_requirements_list_20250402_133857.md\n",
      "16. plan_net_claude_requirements_list_20250319_111650.md\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 14:58:00,755 - __main__ - INFO - Starting requirements refinement with gemini\n",
      "2025-04-02 14:58:00,784 - __main__ - INFO - Sending requirements to gemini for refinement...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing requirements with Gemini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 14:58:27,474 - __main__ - INFO - Requirements refinement complete. Output saved to: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs/refined_requirements_gemini_20250402_145827.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Requirements Refinement Complete!\n",
      "Input file: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/processed_output/plan_net_gemini_20250402_145733.md\n",
      "Refined requirements saved to: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs/refined_requirements_gemini_20250402_145827.md\n",
      "API used: gemini\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the interactive version\n",
    "result = run_refinement()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
